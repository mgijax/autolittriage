[DEFAULT]
HOMEDIR: /Users/jak/work/autolittriage
#GROUPDIR: %(HOMEDIR)s/%(CURATION_GROUP)s
MLTEXTTOOLSDIR: /Users/jak/work/MLtextTools

# Delimited text files from getTrainingData.py and input to predict
FIELDSEP = '|'
RECORDSEP = ';;'

TRAINING_DATA: %(HOMEDIR)s/Data/1sttest
# data directory in format that sklearn.datasets.load_files() wants
# Directory where training data lives - split into class name subdirectories
#    as desired by sklearn.datasets.load_files()

DATA_TO_PREDICT: %(HOMEDIR)s/Data/predict/predict_data.txt
# tab delimited file of data to predict

PREPROCESSORS = ['removeRefSection', 'rejectIfNoMice', 'removeURLsCleanStem']
# name of preprocessor functions to call to help prepare documents

[CLASS_NAMES]
# Class names and y_value mappings.
# This is confusing & has taken a long time to understand.
# See sklearn.metrics:   confusion_matrix,  classification_report
#   make_scorer, fbeta_score, precision_score, recall_score

y_class_names: ['discard', 'keep']
# The labels matching y_values from the training set: y_class_names[y_val]= name
# These match training set directory names in alpha order.

y_class_to_score: 0
# the index in y_class_names of the class to score, i.e., compute precision,
#  recall, f-score, etc.
# This class is used in the grid search scoring to select the best model.

rpt_class_names: ['keep', 'discard']
# Order + labels we want to report in confusion matrix and other rpts.
# Will be really confusing if these names don't match y_class_names!

rpt_class_mapping: [ 1, 0 ]
# List of y_values to rpt in confusion matrix and other reports.
# rpt_class_mapping[y_val] maps to rpt_class_names[]

rpt_classification_report_num: 2
# How many class_names to show in classification_report.
# These classes will be in rpt_class_mapping order


[MODEL_TUNING]
TUNING_INDEX_FILE: index.out
# Where to write index file during tuning runs

PRED_OUTPUT_FILE_PREFIX: predictions
# filename prefix to add "_test.tsv", "_training.tsv" to when outputing files
# of predictions for the test and training sets while running a model tuning
#  script

# Fscore beta to use for comparing Pipelines/models during tuning
GRIDSEARCH_BETA = 4	; default Fscore beta for comparing params in GridSearch
COMPARE_BETA    = 4	; use when comparing different models (outside GS)

TEST_SPLIT      = 0.20	; fraction of sample set to use for test set
GRIDSEARCH_CV   = 5	; number of GridSearch fits (folds) to use
