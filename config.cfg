[DEFAULT]
HOMEDIR: /Users/jak/work/autolittriage
#GROUPDIR: %(HOMEDIR)s/%(CURATION_GROUP)s
MLTEXTTOOLSDIR: /Users/jak/work/MLtextTools

# Delimited text files from getTrainingData.py and input to predict
FIELDSEP = '|'
RECORDSEP = ';;'

TRAINING_DATA: %(HOMEDIR)s/Data/dec5/Proc1/trainingSetFig.txt
# data directory in format that sklearn.datasets.load_files() wants
# Directory where training data lives - split into class name subdirectories
#    as desired by sklearn.datasets.load_files()

DATA_TO_PREDICT: %(HOMEDIR)s/Data/predict/predict_data.txt
# tab delimited file of data to predict

#PREPROCESSORS = ['removeRefSection', 'rejectIfNoMice', 'removeURLsCleanStem']
# name of preprocessor functions to call to help prepare documents
# (not currently used)

FIG_CONVERSION: paragraph
FIG_CONVERSION_NWORDS: 50
# How should figure text be extracted
# Options are 'legend', 'paragraph', 'close words'. see figureText.py

[CLASS_NAMES]
# Class names and y_value mappings.
# This is confusing & has taken a long time to understand.
# See sklearn.metrics:   confusion_matrix,  classification_report
#   make_scorer, fbeta_score, precision_score, recall_score

y_class_names: ['discard', 'keep']
# The labels matching y_values from the training set: y_class_names[y_val]= name
# These match training set directory names in alpha order.

y_class_to_score: 1
# the index in y_class_names of the class to score, i.e., compute precision,
#  recall, f-score, etc.
# This is also the (numeric) value in y_ that we are treating as the "positive"
#   class.
# This class is used in the grid search scoring to select the best model.
# AND in reporting scores in output

rpt_class_names: ['keep', 'discard']
# Order + labels we want to report in confusion matrix and other rpts.
# Will be really confusing if these names don't match y_class_names!

rpt_class_mapping: [ 1, 0 ]
# List of y_values to rpt in confusion matrix and other reports.
# rpt_class_mapping[y_val] maps to rpt_class_names[]

rpt_classification_report_num: 2
# How many class_names to show in classification_report.
# These classes will be in rpt_class_mapping order


[MODEL_TUNING]
TUNING_INDEX_FILE: index.out
# Where to write index file during tuning runs

# Fscore beta to use for comparing Pipelines/models during tuning
GRIDSEARCH_BETA = 2	; default Fscore beta for comparing params in GridSearch
COMPARE_BETA    = 2	; use when comparing different models (outside GS)

TEST_SPLIT      = 0.20	; fraction of sample set to use for test set
NUM_CV          = 5	; num of GridSearch cross validation fits (folds) to use
