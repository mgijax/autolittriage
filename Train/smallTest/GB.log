Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.06s
         2           0.9507            0.04s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.08s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-08-26-42  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=388   randForSplit=497   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=388,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	faster
+0.0618	cre
+0.0500	metabol
+0.0453	ani chang
+0.0406	cell_lin indic
+0.0365	cyclin bound
+0.0330	bm
+0.0295	analys shown
+0.0239	strong accumul
+0.0195	panel figur
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-08-26-45. Total      2.85 seconds

Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-08-50-26  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=904   randForSplit=555   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=904,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	histogram
+0.0618	govern
+0.0500	deliveri
+0.0453	analyz ponceau
+0.0406	puls
+0.0365	t487 figur
+0.0330	cell exampl
+0.0295	use synchron
+0.0239	roscovitin confirm
+0.0195	lysat untreat
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-08-50-29. Total      2.85 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 08:50:36 2019
Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:00:17 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.06s
         2           0.9507            0.04s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-09-01-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=926   randForSplit=869   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=926,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	cre
+0.0618	cage
+0.0500	figur signific
+0.0453	ad short
+0.0406	decreas number
+0.0365	myc plk1
+0.0330	restor
+0.0295	phosphoryl endogen
+0.0239	abolish plk1
+0.0195	endogen bmyb
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-09-01-14. Total      1.64 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:06:31 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-09-18-40  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=816   randForSplit=48   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=816,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	histogram
+0.0618	deliveri
+0.0500	restor
+0.0453	experi gain
+0.0406	exampl
+0.0365	t487 control
+0.0330	mice signific
+0.0295	wb cdk2
+0.0239	advantag substrat
+0.0195	proteolysi experi
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-09-18-42. Total      1.87 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:19:09 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-09-19-12  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=351   randForSplit=247   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=351,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	control figur
+0.0618	signific differ
+0.0500	figur differ
+0.0453	phosphoryl surpris
+0.0406	metabol
+0.0365	transfect myb
+0.0330	figur indic
+0.0295	resist permit
+0.0239	protein complex
+0.0195	terminus figur
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-09-19-15. Total      3.44 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:19:59 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.04s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-09-20-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=392   randForSplit=744   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=392,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	mice mice
+0.0618	cre
+0.0500	dendrit
+0.0453	abil pin1
+0.0406	environment
+0.0365	extrem terminus
+0.0330	mice did
+0.0295	experi antibodi
+0.0239	tad follow
+0.0195	subdivid activ
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-09-20-05. Total      1.78 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:20:23 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.04s
         4           0.7011            0.02s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.04s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-09-22  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=357   randForSplit=443   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=357,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]

### Feature weights: highest 20
+0.6176	mice figur
+0.3231	yellow
+0.0593	figur indic
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427
+0.0000	a427 h358
+0.0000	a68
+0.0000	a68 mm
+0.0000	a7c11
+0.0000	a7c11 breast
+0.0000	a7c11 brpkp110
+0.0000	a7c11 cell
+0.0000	a7c11 cell_lin
+0.0000	a7c11 growth

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 39600
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427', u'a427 h358', u'a68']

Middle 10 features: [u'lack plk1', u'lack secondari', u'lack t476', u'lack termin', u'lack therefor', u'lactat', u'lactat bnip3', u'lactat output', u'lactat raav', u'lactat triglycerid']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/16-15-09-24. Total      2.10 seconds

Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.06s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.06s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-14-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=780   randForSplit=902   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=780,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]

### Feature weights: highest 20
+0.6176	mice figur
+0.2224	yellow
+0.1599	figur indic
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427
+0.0000	a427 h358
+0.0000	a68
+0.0000	a68 mm
+0.0000	a7c11
+0.0000	a7c11 breast
+0.0000	a7c11 brpkp110
+0.0000	a7c11 cell
+0.0000	a7c11 cell_lin
+0.0000	a7c11 growth

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 39600
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427', u'a427 h358', u'a68']

Middle 10 features: [u'lack plk1', u'lack secondari', u'lack t476', u'lack termin', u'lack therefor', u'lactat', u'lactat bnip3', u'lactat output', u'lactat raav', u'lactat triglycerid']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/16-15-14-15. Total      1.99 seconds

Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-16-37  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=423   randForSplit=500   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=423,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	exampl
+0.0641	faster
+0.0519	densiti
+0.0466	pretreat
+0.0421	bm
+0.0376	employ
+0.0343	term
+0.0304	wce
+0.0247	besid
+0.0201	coprecipit
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/16-15-16-38. Total      0.77 seconds

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-18-10  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=209   randForSplit=178   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=209,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	restor
+0.0770	ro
+0.0641	modul
+0.0519	densiti
+0.0421	metabol
+0.0376	star
+0.0343	insert
+0.0247	aliquot
+0.0201	coloni
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/16-15-18-11. Total      0.94 seconds

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.02s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-21-46  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=303   randForSplit=564   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=303,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.1137	govern
+0.0641	bm
+0.0519	dendrit
+0.0466	wb
+0.0421	helper
+0.0376	prove
+0.0304	pbd
+0.0247	e2f5
+0.0201	glutathionesepharos
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/16-15-21-47. Total      0.86 seconds

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.02s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/17-11-43-31  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=409   randForSplit=265   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=409, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	govern
+0.0641	faster
+0.0519	insert
+0.0466	crystal
+0.0421	environment
+0.0376	alkalin
+0.0343	bm
+0.0304	pretreat
+0.0247	t476
+0.0201	judg
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/17-11-43-32. Total      0.87 seconds

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
*** After GS Fit

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
*** After trainset Fit

### Start Time 2019/10/17-11-49-03  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=886   randForSplit=889   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=886, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.1137	term
+0.0641	modul
+0.0519	dendrit
+0.0466	mimosin
+0.0421	densiti
+0.0376	nonmitot
+0.0304	r695
+0.0247	preferenti
+0.0201	multiploid
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/17-11-49-04. Total      0.82 seconds

in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.00s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.0524            0.00s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.00s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.0320            0.01s
         2           0.8873            0.00s
         3           0.7712            0.00s
         4           0.6754            0.00s
         5           0.5947            0.00s
*** After GS Fit

in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.1918            0.02s
         2           1.0365            0.01s
         3           0.9071            0.01s
         4           0.7977            0.00s
         5           0.7044            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1928            0.01s
         2           1.0374            0.00s
         3           0.9079            0.00s
         4           0.7985            0.00s
         5           0.7051            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1863            0.01s
         2           1.0313            0.01s
         3           0.9023            0.00s
         4           0.7934            0.00s
         5           0.7005            0.00s
*** After GS Fit

()
<type 'numpy.ndarray'>
<type 'numpy.ndarray'>
()
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.01s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
*** After GS Fit


<type 'numpy.ndarray'>
1
<type 'numpy.ndarray'>
1

in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
*** After GS Fit


<type 'numpy.ndarray'>
1
<type 'numpy.ndarray'>
1

y
<type 'numpy.ndarray'>
1
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-08-19-03  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=872   randForSplit=356   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.75      0.86         4
Valid discard       0.67      1.00      0.80         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.83      0.88      0.83         6
 weighted avg       0.89      0.83      0.84         6

Valid F2: 0.78947 (keep)

['yes', 'no']
[[3 1]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=872, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6087	gene
+0.0593	clinic
+0.0550	cre
+0.0478	gstplk1
+0.0444	repeat
+0.0387	pin1transfect
+0.0360	fos
+0.0315	pin1medi
+0.0292	dysfunct
+0.0256	trityl
+0.0237	infus
+0.0000	a10
+0.0000	a1978
+0.0000	a427
+0.0000	a68
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xl
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4054
First 10 features: [u'a10', u'a1978', u'a427', u'a68', u'aa', u'aad', u'aav', u'aav5', u'aav6', u'aav8']

Middle 10 features: [u'lack', u'lactat', u'ladder', u'laden', u'laid', u'lamp1', u'lamp2', u'land', u'lane', u'langdon']

Last 10 features: [u'yield', u'young', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 1
25533336

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-08-19-05. Total      1.33 seconds

{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=225, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=225, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 225, 'classifier__criterion': 'friedman_mse'}
{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=233, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=233, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 233, 'classifier__criterion': 'friedman_mse'}
{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=131, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=131, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 131, 'classifier__criterion': 'friedman_mse'}
******
{'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
******
{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=527, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=527, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 527, 'classifier__criterion': 'friedman_mse'}
******
{'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
******
{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=517, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=517, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 517, 'classifier__criterion': 'friedman_mse'}
******
{'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
******
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-11-32-57  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=517   randForSplit=113   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.80      1.00      0.89         4
Valid discard       1.00      0.50      0.67         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.90      0.75      0.78         6
 weighted avg       0.87      0.83      0.81         6

Valid F2: 0.95238 (keep)

['yes', 'no']
[[4 0]
 [1 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=517, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.1697	section
+0.1622	gene
+0.1370	ratio
+0.0963	week
+0.0777	recipi
+0.0630	modul
+0.0589	idu
+0.0511	anim
+0.0475	cvd
+0.0416	recapitul
+0.0385	mimosin
+0.0312	cytokinesi
+0.0254	oxygenas
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4058
First 10 features: [u'a10', u'a1express', u'a427', u'a7c11', u'aa', u'aad', u'aav', u'aav5', u'aav9', u'ab']

Middle 10 features: [u'laden', u'lambda', u'lambert', u'laminin', u'lamp2', u'land', u'lane', u'langdon', u'larg', u'larger']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 1
29247188

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-11-32-58. Total      0.55 seconds

in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.00s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-11-34-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=169   randForSplit=464   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.67      0.50      0.57         4
Valid discard       0.33      0.50      0.40         2

    micro avg       0.50      0.50      0.50         6
    macro avg       0.50      0.50      0.49         6
 weighted avg       0.56      0.50      0.51         6

Valid F2: 0.52632 (keep)

['yes', 'no']
[[2 2]
 [1 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=169, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+1.0000	gene
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav8
+0.0000	aav8hntcp
+0.0000	aavscr
+0.0000	aavshano2
+0.0000	abbrevi
+0.0000	abca1
+0.0000	abd
+0.0000	abdomin
+0.0000	aberr
+0.0000	abil
+0.0000	abl
+0.0000	ablat

### Feature weights: lowest 20
+0.0000	xf96
+0.0000	xmg1
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 3820
First 10 features: [u'a1978', u'a1express', u'a427', u'a68', u'aa', u'aad', u'aav', u'aav8', u'aav8hntcp', u'aavscr']

Middle 10 features: [u'kb', u'kd', u'kda', u'keap', u'keap1', u'keap1medi', u'kelch', u'kept', u'keratin', u'keratinocyt']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 1
30321399

### False negatives for Validation set: 2
25533336
27411738

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-11-34-04. Total      0.53 seconds

in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-11-35-22  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=598   randForSplit=524   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.75      0.86         4
Valid discard       0.67      1.00      0.80         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.83      0.88      0.83         6
 weighted avg       0.89      0.83      0.84         6

Valid F2: 0.78947 (keep)

['yes', 'no']
[[3 1]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=598, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6087	gene
+0.0593	boundari
+0.0550	latent
+0.0478	readout
+0.0444	pad
+0.0387	wb
+0.0360	ly6g
+0.0315	fastap
+0.0292	supernat
+0.0256	myb
+0.0237	recombinas
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	wwp2
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone

### Vectorizer:   Number of Features: 3992
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aavscr']

Middle 10 features: [u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e', u'l697']

Last 10 features: [u'yellow', u'yield', u'young', u'youngest', u'zaytseva', u'zero', u'zn', u'znf822', u'znpp', u'zone']

### False positives for Validation set: 0

### False negatives for Validation set: 1
27411738

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-11-35-23. Total      0.55 seconds

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
### Start Time 2019/10/18-11-37-24  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=180   randForSplit=428   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=180, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	cage
+0.0641	ventral
+0.0519	cre
+0.0466	stlc
+0.0421	term
+0.0376	mimosin
+0.0343	modul
+0.0304	tp
+0.0247	autoradiographi
+0.0201	pbm
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-11-37-25. Total      0.59 seconds

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.03s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
### Start Time 2019/10/18-12-40-46  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=23   randForSplit=943   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=23, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0941	insert
+0.0794	faster
+0.0641	modul
+0.0466	glutathionesepharos
+0.0376	wce
+0.0343	deliveri
+0.0304	dbd
+0.0247	dic
+0.0201	poorer
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-12-40-47. Total      0.65 seconds

in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-12-42-42  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=84   randForSplit=820   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        12
Train discard       1.00      1.00      1.00         8

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[12  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.60      0.75         5
Valid discard       0.33      1.00      0.50         1

    micro avg       0.67      0.67      0.67         6
    macro avg       0.67      0.80      0.62         6
 weighted avg       0.89      0.67      0.71         6

Valid F2: 0.65217 (keep)

['yes', 'no']
[[3 2]
 [0 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=84, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.4095	reduct
+0.2172	ratio
+0.0556	nonlinear
+0.0531	leupeptin
+0.0450	intraven
+0.0430	autophag
+0.0365	authent
+0.0349	male
+0.0297	proteolysi
+0.0283	firm
+0.0241	adjuv
+0.0230	glycogen
+0.0000	a10
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	wwp2
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xl
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	zaytseva
+0.0000	zero
+0.0000	zn
+0.0000	znf822
+0.0000	znpp

### Vectorizer:   Number of Features: 4060
First 10 features: [u'a10', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5', u'aav6', u'aav8']

Middle 10 features: [u'laid', u'lamp1', u'lamp2', u'land', u'lane', u'langdon', u'larg', u'larger', u'largest', u'laser']

Last 10 features: [u'year', u'yeast', u'yellow', u'yield', u'young', u'zaytseva', u'zero', u'zn', u'znf822', u'znpp']

### False positives for Validation set: 0

### False negatives for Validation set: 2
28732055
25882312

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           12            8          60%
Validation Set      :            6            5            1          83%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-12-42-43. Total      0.56 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.01s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-12-50-54  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=815   randForSplit=679   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.80      1.00      0.89         4
Valid discard       1.00      0.50      0.67         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.90      0.75      0.78         6
 weighted avg       0.87      0.83      0.81         6

Valid F2: 0.95238 (keep)

['yes', 'no']
[[4 0]
 [1 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=815, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.3583	gene
+0.1106	scale
+0.0963	term
+0.0777	s2c
+0.0630	defin
+0.0589	pretreat
+0.0511	inject
+0.0475	thymidin
+0.0416	faster
+0.0385	kinesin
+0.0312	cy
+0.0254	coprecipit
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aav

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xl
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4119
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aav', u'aav5', u'aav6']

Middle 10 features: [u'ldlr', u'lead', u'learn', u'learnt', u'lectin', u'led', u'left', u'legend', u'length', u'lentivir']

Last 10 features: [u'yellow', u'yield', u'young', u'youngest', u'zero', u'zhang', u'zip', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 1
29247188

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-12-50-56. Total      2.25 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-12-55-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=764   randForSplit=180   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=764, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	environment
+0.0641	cage
+0.0519	helper
+0.0466	eearly_embryo
+0.0421	puls
+0.0376	mobil
+0.0343	faster
+0.0304	unphosphoryl
+0.0247	bmyb
+0.0201	alanin
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-12-55-14. Total      1.07 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-15-29  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=692   randForSplit=167   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=692,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	bm
+0.0641	predomin
+0.0519	helper
+0.0505	coomassi
+0.0466	exemplarili
+0.0421	remark
+0.0376	cytokinesi
+0.0343	govern
+0.0247	readout
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-13-15-30. Total      1.06 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-52-24  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=738   randForSplit=163   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=738,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	densiti
+0.0641	ventral
+0.0519	faster
+0.0466	mybspecif
+0.0421	metabol
+0.0376	electrophoret
+0.0343	govern
+0.0304	l697
+0.0247	assum
+0.0201	foxm1
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-13-52-25. Total      1.06 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-54-22  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=886   randForSplit=618   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=886,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.1137	term
+0.0641	modul
+0.0519	dendrit
+0.0466	mimosin
+0.0421	densiti
+0.0376	nonmitot
+0.0304	r695
+0.0247	preferenti
+0.0201	multiploid
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-13-54-23. Total      1.06 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-56-41  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=451   randForSplit=165   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=451,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	modul
+0.0641	versus
+0.0519	metabol
+0.0466	polyploidi
+0.0421	insert
+0.0376	binucl
+0.0343	faster
+0.0304	replat
+0.0247	gstplk1
+0.0201	spectral
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-13-56-42. Total      1.06 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.00s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.01s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.01s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.01s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.00s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.01s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.01s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.01s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0320            0.01s
         2           0.8873            0.01s
         3           0.7712            0.00s
         4           0.6754            0.00s
         5           0.5947            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-58-45  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=38   randForSplit=547   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        14
Train discard       1.00      1.00      1.00         6

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[14  0]
 [ 0  6]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         3
Valid discard       1.00      1.00      1.00         3

    micro avg       1.00      1.00      1.00         6
    macro avg       1.00      1.00      1.00         6
 weighted avg       1.00      1.00      1.00         6

Valid F2: 1.00000 (keep)

['yes', 'no']
[[3 0]
 [0 3]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=38,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.3721	gene
+0.1451	histogram
+0.0998	dendrit
+0.0680	ratio
+0.0645	violet
+0.0526	despit
+0.0518	consensus
+0.0427	faster
+0.0419	aliquot
+0.0340	address
+0.0276	foxm1
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone

### Vectorizer:   Number of Features: 4184
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kinet', u'kit', u'kl', u'klh', u'klrg', u'klrg1', u'klrg1hi', u'klrg1lo', u'knock', u'knock_in']

Last 10 features: [u'yellow', u'yield', u'young', u'youngest', u'zaytseva', u'zero', u'zn', u'znf822', u'znpp', u'zone']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           14            6          70%
Validation Set      :            6            3            3          50%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-13-58-48. Total      2.26 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.01s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.01s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-00-25  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=933   randForSplit=150   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        15
Train discard       1.00      1.00      1.00         5

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[15  0]
 [ 0  5]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.50      1.00      0.67         2
Valid discard       1.00      0.50      0.67         4

    micro avg       0.67      0.67      0.67         6
    macro avg       0.75      0.75      0.67         6
 weighted avg       0.83      0.67      0.67         6

Valid F2: 0.83333 (keep)

['yes', 'no']
[[2 0]
 [2 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=933,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6190	ratio
+0.1310	comparison
+0.1125	fraction
+0.0899	bone
+0.0475	despit
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5
+0.0000	aav6
+0.0000	aav8
+0.0000	aav8hntcp
+0.0000	aav9
+0.0000	aavscr

### Feature weights: lowest 20
+0.0000	xl
+0.0000	xmg1
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4196
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'ki', u'ki67', u'kidney', u'kill', u'kinas', u'kinesin', u'kinet', u'kit', u'kl', u'klh']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 2
30683694
28515318

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           15            5          75%
Validation Set      :            6            2            4          33%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-00-27. Total      2.23 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.00s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.00s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-01-11  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=99   randForSplit=467   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.75      0.75      0.75         4
Valid discard       0.50      0.50      0.50         2

    micro avg       0.67      0.67      0.67         6
    macro avg       0.62      0.62      0.62         6
 weighted avg       0.67      0.67      0.67         6

Valid F2: 0.75000 (keep)

['yes', 'no']
[[3 1]
 [1 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=99,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.8077	gene
+0.0562	govern
+0.0453	gh
+0.0367	s7a
+0.0298	vehicl
+0.0242	marrow
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav8
+0.0000	aav8hntcp
+0.0000	ab
+0.0000	abbrevi
+0.0000	abca1
+0.0000	abd

### Feature weights: lowest 20
+0.0000	xf96
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 3888
First 10 features: [u'a10', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav8', u'aav8hntcp']

Middle 10 features: [u'kl', u'klh', u'klrg', u'klrg1', u'klrg1hi', u'klrg1lo', u'knock_in', u'knock_out', u'knockdown', u'knowledg']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 1
30321399

### False negatives for Validation set: 1
27411738

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-01-13. Total      2.03 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.00s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.00s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.01s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.00s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0320            0.01s
         2           0.8873            0.01s
         3           0.7712            0.00s
         4           0.6754            0.00s
         5           0.5947            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-07-11  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=619   randForSplit=73   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)	NPV: 1.00000

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.75      0.86         4
Valid discard       0.67      1.00      0.80         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.83      0.88      0.83         6
 weighted avg       0.89      0.83      0.84         6

Valid F2: 0.78947 (keep)	NPV: 0.66667

['yes', 'no']
[[3 1]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)	NPV: 1.00000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=619,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6087	gene
+0.0593	eg5
+0.0550	memory27
+0.0478	spectral
+0.0444	ampa
+0.0387	nonmitot
+0.0360	fepsp
+0.0315	pretreat
+0.0292	anticamkii
+0.0256	thymidin
+0.0237	camkiia
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5
+0.0000	aav8

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4080
First 10 features: [u'a1978', u'a1express', u'a427', u'a68', u'aa', u'aad', u'aav', u'aav5', u'aav8', u'aav8hntcp']

Middle 10 features: [u'klrg1hi', u'klrg1lo', u'knock', u'knock_in', u'knock_out', u'knockdown', u'knowledg', u'known', u'kodak', u'kras']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 1
25533336

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-07-13. Total      2.07 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.1918            0.01s
         2           1.0365            0.00s
         3           0.9071            0.00s
         4           0.7977            0.00s
         5           0.7044            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.00s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1928            0.01s
         2           1.0374            0.00s
         3           0.9079            0.00s
         4           0.7985            0.00s
         5           0.7051            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1918            0.01s
         2           1.0365            0.00s
         3           0.9071            0.00s
         4           0.7977            0.00s
         5           0.7044            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.00s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.00s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.00s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1928            0.01s
         2           1.0374            0.00s
         3           0.9079            0.00s
         4           0.7985            0.00s
         5           0.7051            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1863            0.01s
         2           1.0313            0.01s
         3           0.9023            0.00s
         4           0.7934            0.00s
         5           0.7005            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-11-19  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=153   randForSplit=831   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        11
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.0000 (keep)    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[11  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.67      0.80         6
Valid discard       0.00      0.00      0.00         0

    micro avg       0.67      0.67      0.67         6
    macro avg       0.50      0.33      0.40         6
 weighted avg       1.00      0.67      0.80         6

Valid F2: 0.7143 (keep)    P: 1.0000    R: 0.6667    NPV: 0.0000

['yes', 'no']
[[4 2]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.0000 (keep)    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=153,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6368	ratio
+0.0665	furthermor
+0.0534	span
+0.0522	santa
+0.0423	necrosi
+0.0351	share
+0.0344	apci
+0.0286	xds
+0.0279	posttranscript
+0.0227	chemotact
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	writh
+0.0000	wrote
+0.0000	ww
+0.0000	wwp1
+0.0000	wwp2
+0.0000	x710
+0.0000	xf96
+0.0000	xl
+0.0000	xscale
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zn
+0.0000	znf822
+0.0000	znpp

### Vectorizer:   Number of Features: 4114
First 10 features: [u'a10', u'a1978', u'a1express', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5', u'aav6']

Middle 10 features: [u'l211a', u'l3', u'l477h', u'l552e', u'l697', u'la', u'label', u'laboratori', u'lack', u'lactat']

Last 10 features: [u'yeast', u'yellow', u'yield', u'young', u'youngest', u'zaytseva', u'zero', u'zn', u'znf822', u'znpp']

### False positives for Validation set: 0

### False negatives for Validation set: 2
30097518
28732055

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           11            9          55%
Validation Set      :            6            6            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-11-21. Total      2.18 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-12-40  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=234   randForSplit=950   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        12
Train discard       1.00      1.00      1.00         8

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.0000 (keep)    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[12  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.40      0.57         5
Valid discard       0.25      1.00      0.40         1

    micro avg       0.50      0.50      0.50         6
    macro avg       0.62      0.70      0.49         6
 weighted avg       0.88      0.50      0.54         6

Valid F2: 0.4545 (keep)    P: 1.0000    R: 0.4000    NPV: 0.2500

['yes', 'no']
[[2 3]
 [0 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.0000 (keep)    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=234,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6429	imag
+0.1882	scheme
+0.1689	caspas
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	aa
+0.0000	aav
+0.0000	aav5
+0.0000	aav6
+0.0000	aav8
+0.0000	aav8hntcp
+0.0000	aav9
+0.0000	aavscr
+0.0000	aavshano2
+0.0000	abbrevi
+0.0000	abca1
+0.0000	abd
+0.0000	abdomin

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xl
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4074
First 10 features: [u'a10', u'a1express', u'a427', u'a68', u'aa', u'aav', u'aav5', u'aav6', u'aav8', u'aav8hntcp']

Middle 10 features: [u'late', u'latenc', u'later', u'layer', u'lc', u'lc3', u'lc3b', u'lc3posit', u'ldl', u'ldlr']

Last 10 features: [u'yellow', u'yield', u'young', u'youngest', u'zero', u'zhang', u'zip', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 3
27298446
25882312
28558013

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           12            8          60%
Validation Set      :            6            5            1          83%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-12-42. Total      2.10 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-17-16  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=694   randForSplit=743   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        12
Train discard       1.00      1.00      1.00         8

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train (keep) P: 1.0000    R: 1.0000    f2: 1.0000    NPV: 1.0000

['yes', 'no']
[[12  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.80      0.80      0.80         5
Valid discard       0.00      0.00      0.00         1

    micro avg       0.67      0.67      0.67         6
    macro avg       0.40      0.40      0.40         6
 weighted avg       0.67      0.67      0.67         6

Valid (keep) P: 0.8000    R: 0.8000    f2: 0.8000    NPV: 0.0000

['yes', 'no']
[[4 1]
 [1 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  (keep) P: 1.0000    R: 1.0000    f2: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=694,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.3592	imag
+0.2676	ratio
+0.0556	mice
+0.0531	fusion
+0.0450	r192g
+0.0430	femor
+0.0365	sigmoid
+0.0349	callus
+0.0297	impli
+0.0283	suppress
+0.0241	ip
+0.0230	recombinas
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	xds
+0.0000	xf96
+0.0000	xl
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4151
First 10 features: [u'a10', u'a1express', u'a427', u'a7c11', u'aa', u'aad', u'aav', u'aav5', u'aav6', u'aav8']

Middle 10 features: [u'laboratori', u'lack', u'lactat', u'lactobacillus', u'ladder', u'laden', u'laminin', u'land', u'lane', u'langdon']

Last 10 features: [u'yield', u'young', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'zone', u'zygos']

### False positives for Validation set: 1
28274624

### False negatives for Validation set: 1
25882312

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           12            8          60%
Validation Set      :            6            5            1          83%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-17-18. Total      2.14 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.01s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.01s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.01s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.01s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0320            0.01s
         2           0.8873            0.01s
         3           0.7712            0.00s
         4           0.6754            0.00s
         5           0.5947            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-21-36  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=63   randForSplit=979   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        14
Train discard       1.00      1.00      1.00         6

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train (keep) P: 1.0000    R: 1.0000    F2: 1.0000    NPV: 1.0000

['yes', 'no']
[[14  0]
 [ 0  6]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         3
Valid discard       1.00      1.00      1.00         3

    micro avg       1.00      1.00      1.00         6
    macro avg       1.00      1.00      1.00         6
 weighted avg       1.00      1.00      1.00         6

Valid (keep) P: 1.0000    R: 1.0000    F2: 1.0000    NPV: 1.0000

['yes', 'no']
[[3 0]
 [0 3]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  (keep) P: 1.0000    R: 1.0000    F2: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=63,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.4401	gene
+0.0998	helper
+0.0802	restor
+0.0648	govern
+0.0645	conform
+0.0526	predomin
+0.0518	l697
+0.0427	cage
+0.0419	glutathionesepharos
+0.0340	pretreat
+0.0276	roscovitin
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4082
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aav', u'aav5', u'aav9']

Middle 10 features: [u'la', u'label', u'lack', u'lactat', u'lactobacillus', u'laden', u'laid', u'laminin', u'lamp1', u'lamp2']

Last 10 features: [u'yeast', u'yellow', u'yield', u'young', u'zero', u'zhang', u'zip', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           14            6          70%
Validation Set      :            6            3            3          50%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-21-38. Total      2.18 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.00s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.01s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.01s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-23-14  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=556   randForSplit=667   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        12
Train discard       1.00      1.00      1.00         8

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[12  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.80      0.89         5
Valid discard       0.50      1.00      0.67         1

    micro avg       0.83      0.83      0.83         6
    macro avg       0.75      0.90      0.78         6
 weighted avg       0.92      0.83      0.85         6

Valid (keep) F2: 0.8333    P: 1.0000    R: 0.8000    NPV: 0.5000

['yes', 'no']
[[4 1]
 [0 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=556,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.4277	gene
+0.1991	ratio
+0.0853	ask
+0.0531	innat
+0.0450	altogeth
+0.0430	chimera
+0.0365	deimmun
+0.0349	halt
+0.0283	pge2
+0.0241	splenocyt
+0.0230	nk1
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav6
+0.0000	aav8

### Feature weights: lowest 20
+0.0000	xl
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zygos

### Vectorizer:   Number of Features: 3939
First 10 features: [u'a10', u'a1express', u'a427', u'a7c11', u'aa', u'aad', u'aav', u'aav6', u'aav8', u'aav8hntcp']

Middle 10 features: [u'l697', u'label', u'laboratori', u'lack', u'lactat', u'lactobacillus', u'ladder', u'lambda', u'lambert', u'laminin']

Last 10 features: [u'young', u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 1
28732055

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           12            8          60%
Validation Set      :            6            5            1          83%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-23-16. Total      2.11 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.02s
         2           0.8350            0.01s
         3           0.7241            0.01s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/24-14-22-17  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=961   randForSplit=408   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=961,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	ventral
+0.0717	predomin
+0.0579	deliveri
+0.0469	metabol
+0.0380	densiti
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/24-14-22-18. Total      1.17 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.02s
         2           0.8350            0.01s
         3           0.7241            0.01s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-15-21-22  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=490   randForSplit=979   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=490,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	deliveri
+0.0717	histogram
+0.0579	modul
+0.0469	environment
+0.0380	govern
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/28-15-21-24. Total      1.23 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-15-28-18  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=594   randForSplit=944   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=594,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	dendrit
+0.0717	exampl
+0.0579	densiti
+0.0469	remark
+0.0380	modul
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2019/10/28-15-28-19. Total      1.18 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-15-43-08  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=204   randForSplit=168   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=204,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	cage
+0.0717	deliveri
+0.0579	govern
+0.0469	faster
+0.0380	densiti
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2019/10/28-15-43-09. Total      1.18 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-15-50-15  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=19   randForSplit=813   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=19,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	bm
+0.0717	term
+0.0579	insert
+0.0469	govern
+0.0380	helper
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2019/10/28-15-50-16. Total      1.18 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-16-18-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=875   randForSplit=344   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=875,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	restor
+0.0849	insert
+0.0717	govern
+0.0579	cre
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2019/10/28-16-18-05. Total      1.25 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.03s
         2           0.8350            0.01s
         3           0.7241            0.01s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/29-09-01-55  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=498   randForSplit=762   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=498,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	dendrit
+0.0717	restor
+0.0579	histogram
+0.0469	faster
+0.0380	densiti
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/29-09-01-56. Total      1.01 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0550            0.01s
         2           0.9182            0.01s
         3           0.8044            0.00s
         4           0.7082            0.00s
         5           0.6261            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0550            0.01s
         2           0.9182            0.01s
         3           0.8044            0.00s
         4           0.7082            0.00s
         5           0.6261            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0817            0.01s
         2           0.9394            0.01s
         3           0.8217            0.00s
         4           0.7226            0.00s
         5           0.6382            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0550            0.01s
         2           0.9182            0.01s
         3           0.8044            0.00s
         4           0.7082            0.00s
         5           0.6261            0.00s
### Start Time 2019/10/29-09-08-36  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=407   randForSplit=405   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Clf instance at 0x1a1cf41908>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=407,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.1053	bm
+0.0886	term
+0.0717	histogram
+0.0384	metabol
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/29-09-08-37. Total      1.06 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0597            0.01s
         2           0.9224            0.01s
         3           0.8083            0.00s
         4           0.7117            0.00s
         5           0.6292            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0597            0.01s
         2           0.9224            0.01s
         3           0.8083            0.00s
         4           0.7117            0.00s
         5           0.6292            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0857            0.01s
         2           0.9434            0.01s
         3           0.8254            0.00s
         4           0.7261            0.00s
         5           0.6413            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0597            0.01s
         2           0.9224            0.01s
         3           0.8083            0.00s
         4           0.7117            0.00s
         5           0.6292            0.00s
### Start Time 2019/10/29-17-00-39  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=454   randForSplit=429   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Clf instance at 0x1a14935d40>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=454,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.1054	remark
+0.0886	ventral
+0.0717	densiti
+0.0384	term
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/29-17-00-40. Total      0.96 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0593            0.01s
         2           0.9221            0.01s
         3           0.8080            0.00s
         4           0.7115            0.00s
         5           0.6290            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0593            0.01s
         2           0.9221            0.01s
         3           0.8080            0.00s
         4           0.7115            0.00s
         5           0.6290            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0893            0.01s
         2           0.9468            0.01s
         3           0.8286            0.00s
         4           0.7290            0.00s
         5           0.6440            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0593            0.01s
         2           0.9221            0.01s
         3           0.8080            0.00s
         4           0.7115            0.00s
         5           0.6290            0.00s
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9157            0.01s
         3           0.8021            0.00s
         4           0.7062            0.00s
         5           0.6242            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9157            0.01s
         3           0.8021            0.00s
         4           0.7062            0.00s
         5           0.6242            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0847            0.01s
         2           0.9423            0.01s
         3           0.8244            0.00s
         4           0.7252            0.00s
         5           0.6405            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9157            0.01s
         3           0.8021            0.00s
         4           0.7062            0.00s
         5           0.6242            0.00s
### Start Time 2019/10/30-10-49-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=56   randForSplit=547   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Clf instance at 0x1a22c02cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=56,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0887	predomin
+0.0717	histogram
+0.0581	govern
+0.0472	puls
+0.0384	term
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/30-10-49-05. Total      0.95 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.01s
         2           0.9271            0.01s
         3           0.8124            0.00s
         4           0.7155            0.00s
         5           0.6325            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.01s
         2           0.9271            0.01s
         3           0.8124            0.00s
         4           0.7155            0.00s
         5           0.6325            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0878            0.01s
         2           0.9453            0.01s
         3           0.8273            0.00s
         4           0.7278            0.00s
         5           0.6429            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.01s
         2           0.9271            0.01s
         3           0.8124            0.00s
         4           0.7155            0.00s
         5           0.6325            0.00s
### Start Time 2019/10/30-10-50-06  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=993   randForSplit=289   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.50      1.00      0.67         1
Valid discard       1.00      0.50      0.67         2

    micro avg       0.67      0.67      0.67         3
    macro avg       0.75      0.75      0.67         3
 weighted avg       0.83      0.67      0.67         3

Valid (keep) F2: 0.8333    P: 0.5000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [1 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Clf instance at 0x1a1da70908>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=993,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.1298	suffici
+0.0886	cage
+0.0472	cre
+0.0384	remark
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 1
29378950

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/30-10-50-07. Total      1.05 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0248            0.01s
         2           0.8893            0.01s
         3           0.7777            0.00s
         4           0.6840            0.00s
         5           0.6041            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0248            0.01s
         2           0.8893            0.01s
         3           0.7777            0.00s
         4           0.6840            0.00s
         5           0.6041            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0720            0.01s
         2           0.9296            0.01s
         3           0.8124            0.00s
         4           0.7140            0.00s
         5           0.6303            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0248            0.01s
         2           0.8893            0.01s
         3           0.7777            0.00s
         4           0.6840            0.00s
         5           0.6041            0.00s
### Start Time 2019/11/04-12-17-38  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=329   randForSplit=410   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Note: this is a note
### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a15b59cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=329,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0888	restor
+0.0855	term
+0.0717	deliveri
+0.0581	exampl
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/04-12-17-39. Total      1.08 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0496            0.01s
         2           0.9131            0.01s
         3           0.7997            0.00s
         4           0.7041            0.00s
         5           0.6223            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0496            0.01s
         2           0.9131            0.01s
         3           0.7997            0.00s
         4           0.7041            0.00s
         5           0.6223            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0835            0.01s
         2           0.9412            0.01s
         3           0.8234            0.00s
         4           0.7242            0.00s
         5           0.6396            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0496            0.01s
         2           0.9131            0.01s
         3           0.7997            0.00s
         4           0.7041            0.00s
         5           0.6223            0.00s
### Start Time 2019/11/04-12-18-14  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=431   randForSplit=393   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a13edd908>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=431,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.1468	histogram
+0.0717	modul
+0.0472	insert
+0.0384	deliveri
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/04-12-18-15. Total      1.14 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0611            0.01s
         2           0.9238            0.01s
         3           0.8095            0.00s
         4           0.7128            0.00s
         5           0.6302            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0611            0.01s
         2           0.9238            0.01s
         3           0.8095            0.00s
         4           0.7128            0.00s
         5           0.6302            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0903            0.01s
         2           0.9478            0.01s
         3           0.8295            0.00s
         4           0.7298            0.00s
         5           0.6448            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0611            0.01s
         2           0.9238            0.01s
         3           0.8095            0.00s
         4           0.7128            0.00s
         5           0.6302            0.00s
### Start Time 2019/11/06-10-53-21  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=55   randForSplit=364   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.50      1.00      0.67         1
Valid discard       1.00      0.50      0.67         2

    micro avg       0.67      0.67      0.67         3
    macro avg       0.75      0.75      0.67         3
 weighted avg       0.83      0.67      0.67         3

Valid (keep) F2: 0.8333    P: 0.5000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [1 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a18ab4c68>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=55,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Feature weights: highest 20
+0.6959	gene
+0.0966	suffici
+0.0886	exampl
+0.0717	remark
+0.0472	deliveri
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 1
29378950

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-10-53-22. Total      1.13 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0440            0.01s
         2           0.9078            0.01s
         3           0.7949            0.00s
         4           0.6997            0.00s
         5           0.6184            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0440            0.01s
         2           0.9078            0.01s
         3           0.7949            0.00s
         4           0.6997            0.00s
         5           0.6184            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0801            0.01s
         2           0.9378            0.01s
         3           0.8202            0.00s
         4           0.7213            0.00s
         5           0.6370            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0440            0.01s
         2           0.9078            0.01s
         3           0.7949            0.00s
         4           0.6997            0.00s
         5           0.6184            0.00s
### Start Time 2019/11/06-10-56-33  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=395   randForSplit=954   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a15dcbb00>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=395,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
{'n_jobs': 1, 'verbose': True, 'estimator__vectorizer__dtype': <type 'numpy.int64'>, 'estimator__vectorizer__ngram_range': (1, 2), 'estimator__vectorizer__min_df': 0.02, 'estimator__memory': None, 'estimator__classifier__min_samples_leaf': 1, 'estimator__classifier__validation_fraction': 0.1, 'estimator__steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a15c04cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=395,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False))], 'estimator__vectorizer__input': u'content', 'estimator__classifier__loss': 'deviance', 'param_grid': {'vectorizer__min_df': [0.01, 0.02], 'vectorizer__ngram_range': [(1, 1)], 'classifier__n_estimators': [5], 'vectorizer__max_df': [0.75]}, 'cv': [([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 27, 28])], 'estimator__classifier__warm_start': False, 'scoring': make_scorer(fbeta_score, beta=2, pos_label=1), 'estimator__classifier__min_impurity_decrease': 0.0, 'estimator__vectorizer__preprocessor': None, 'estimator__vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'fit_params': None, 'estimator__vectorizer__vocabulary': None, 'estimator': Pipeline(memory=None,
     steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='englis...    subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False))]), 'estimator__vectorizer__lowercase': False, 'estimator__classifier__max_features': None, 'pre_dispatch': '2*n_jobs', 'estimator__vectorizer__tokenizer': None, 'estimator__classifier__tol': 0.0001, 'estimator__classifier__min_weight_fraction_leaf': 0.0, 'estimator__classifier__verbose': 1, 'estimator__classifier__init': <__main__.Working_Init_Classifier instance at 0x1a15c04cb0>, 'estimator__vectorizer__decode_error': 'strict', 'estimator__classifier': GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a15c04cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=395,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False), 'refit': True, 'estimator__vectorizer__max_features': None, 'estimator__classifier__presort': 'auto', 'estimator__classifier__criterion': 'friedman_mse', 'estimator__vectorizer__max_df': 0.75, 'estimator__vectorizer__strip_accents': None, 'estimator__vectorizer__encoding': u'utf-8', 'estimator__vectorizer__stop_words': 'english', 'estimator__classifier__min_samples_split': 2, 'iid': True, 'estimator__vectorizer__analyzer': u'word', 'estimator__classifier__n_estimators': 100, 'return_train_score': 'warn', 'estimator__classifier__subsample': 1.0, 'estimator__classifier__min_impurity_split': None, 'estimator__classifier__n_iter_no_change': None, 'estimator__vectorizer__binary': True, 'error_score': 'raise-deprecating', 'estimator__classifier__learning_rate': 0.1, 'estimator__classifier__random_state': 395, 'estimator__classifier__max_leaf_nodes': None, 'estimator__vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'estimator__classifier__max_depth': 3}
### Feature weights: highest 20
+0.6959	gene
+0.0887	versus
+0.0717	environment
+0.0581	densiti
+0.0472	suffici
+0.0384	restor
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-10-56-34. Total      1.10 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0492            0.01s
         2           0.9128            0.01s
         3           0.7994            0.00s
         4           0.7038            0.00s
         5           0.6221            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0492            0.01s
         2           0.9128            0.01s
         3           0.7994            0.00s
         4           0.7038            0.00s
         5           0.6221            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0834            0.01s
         2           0.9410            0.01s
         3           0.8233            0.00s
         4           0.7241            0.00s
         5           0.6395            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0492            0.01s
         2           0.9128            0.01s
         3           0.7994            0.00s
         4           0.7038            0.00s
         5           0.6221            0.00s
### Start Time 2019/11/06-11-01-34  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=330   randForSplit=264   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a22f2bc68>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=330,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]
cv:[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 27, 28])]
error_score:raise-deprecating
estimator:Pipeline(memory=None,
     steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='englis...    subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False))])
estimator__classifier:GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a22d1ccf8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=330,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)
estimator__classifier__criterion:friedman_mse
estimator__classifier__init:<__main__.Working_Init_Classifier instance at 0x1a22d1ccf8>
estimator__classifier__learning_rate:0.1
estimator__classifier__loss:deviance
estimator__classifier__max_depth:3
estimator__classifier__max_features:None
estimator__classifier__max_leaf_nodes:None
estimator__classifier__min_impurity_decrease:0.0
estimator__classifier__min_impurity_split:None
estimator__classifier__min_samples_leaf:1
estimator__classifier__min_samples_split:2
estimator__classifier__min_weight_fraction_leaf:0.0
estimator__classifier__n_estimators:100
estimator__classifier__n_iter_no_change:None
estimator__classifier__presort:auto
estimator__classifier__random_state:330
estimator__classifier__subsample:1.0
estimator__classifier__tol:0.0001
estimator__classifier__validation_fraction:0.1
estimator__classifier__verbose:1
estimator__classifier__warm_start:False
estimator__memory:None
estimator__steps:[('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a22d1ccf8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=330,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False))]
estimator__vectorizer:CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
estimator__vectorizer__analyzer:word
estimator__vectorizer__binary:True
estimator__vectorizer__decode_error:strict
estimator__vectorizer__dtype:<type 'numpy.int64'>
estimator__vectorizer__encoding:utf-8
estimator__vectorizer__input:content
estimator__vectorizer__lowercase:False
estimator__vectorizer__max_df:0.75
estimator__vectorizer__max_features:None
estimator__vectorizer__min_df:0.02
estimator__vectorizer__ngram_range:(1, 2)
estimator__vectorizer__preprocessor:None
estimator__vectorizer__stop_words:english
estimator__vectorizer__strip_accents:None
estimator__vectorizer__token_pattern:(?u)\b\w\w+\b
estimator__vectorizer__tokenizer:None
estimator__vectorizer__vocabulary:None
fit_params:None
iid:True
n_jobs:1
param_grid:{'vectorizer__min_df': [0.01, 0.02], 'vectorizer__ngram_range': [(1, 1)], 'classifier__n_estimators': [5], 'vectorizer__max_df': [0.75]}
pre_dispatch:2*n_jobs
refit:True
return_train_score:warn
scoring:make_scorer(fbeta_score, beta=2, pos_label=1)
verbose:True

### Feature weights: highest 20
+0.6959	gene
+0.0887	suffici
+0.0717	metabol
+0.0581	puls
+0.0472	environment
+0.0384	versus
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-01-35. Total      1.08 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0560            0.01s
         2           0.9191            0.01s
         3           0.8052            0.00s
         4           0.7090            0.00s
         5           0.6267            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0560            0.01s
         2           0.9191            0.01s
         3           0.8052            0.00s
         4           0.7090            0.00s
         5           0.6267            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0825            0.01s
         2           0.9402            0.01s
         3           0.8224            0.00s
         4           0.7233            0.00s
         5           0.6388            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0560            0.01s
         2           0.9191            0.01s
         3           0.8052            0.00s
         4           0.7090            0.00s
         5           0.6267            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0529            0.01s
         2           0.9162            0.01s
         3           0.8026            0.00s
         4           0.7066            0.00s
         5           0.6246            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0529            0.01s
         2           0.9162            0.01s
         3           0.8026            0.00s
         4           0.7066            0.00s
         5           0.6246            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0829            0.01s
         2           0.9406            0.01s
         3           0.8228            0.00s
         4           0.7237            0.00s
         5           0.6392            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0529            0.01s
         2           0.9162            0.01s
         3           0.8026            0.00s
         4           0.7066            0.00s
         5           0.6246            0.00s
### Start Time 2019/11/06-11-15-48  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=1   randForSplit=361   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a19af4830>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Best Score: 1

### Feature weights: highest 20
+0.6959	gene
+0.1770	exampl
+0.0887	suffici
+0.0384	versus
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6
+0.0000	aav8

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-15-49. Total      1.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0374            0.01s
         2           0.9015            0.01s
         3           0.7891            0.00s
         4           0.6944            0.00s
         5           0.6136            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0374            0.01s
         2           0.9015            0.01s
         3           0.7891            0.00s
         4           0.6944            0.00s
         5           0.6136            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0767            0.01s
         2           0.9344            0.01s
         3           0.8170            0.01s
         4           0.7183            0.00s
         5           0.6342            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0374            0.01s
         2           0.9015            0.01s
         3           0.7891            0.00s
         4           0.6944            0.00s
         5           0.6136            0.00s
### Start Time 2019/11/06-11-16-21  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=106   randForSplit=822   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a17292830>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=106,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.1271	exampl
+0.0717	suffici
+0.0581	environment
+0.0472	metabol
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-16-22. Total      1.13 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0565            0.01s
         2           0.9195            0.01s
         3           0.8056            0.00s
         4           0.7093            0.00s
         5           0.6270            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0565            0.01s
         2           0.9195            0.01s
         3           0.8056            0.00s
         4           0.7093            0.00s
         5           0.6270            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0862            0.01s
         2           0.9439            0.01s
         3           0.8259            0.00s
         4           0.7265            0.00s
         5           0.6417            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0565            0.01s
         2           0.9195            0.01s
         3           0.8056            0.00s
         4           0.7093            0.00s
         5           0.6270            0.00s
   mean_fit_time  mean_score_time  ...  std_test_score  std_train_score
0       0.131005         0.007323  ...             0.0              0.0
1       0.131010         0.006803  ...             0.0              0.0

[2 rows x 16 columns]
### Start Time 2019/11/06-11-24-08  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=636   randForSplit=656   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a25080a70>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=636,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0886	govern
+0.0717	metabol
+0.0581	puls
+0.0472	insert
+0.0384	bm
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-24-09. Total      1.14 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0576            0.01s
         2           0.9206            0.01s
         3           0.8066            0.00s
         4           0.7102            0.00s
         5           0.6278            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0576            0.01s
         2           0.9206            0.01s
         3           0.8066            0.00s
         4           0.7102            0.00s
         5           0.6278            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0860            0.01s
         2           0.9436            0.01s
         3           0.8257            0.00s
         4           0.7263            0.00s
         5           0.6415            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0576            0.01s
         2           0.9206            0.01s
         3           0.8066            0.00s
         4           0.7102            0.00s
         5           0.6278            0.00s
   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_classifier__n_estimators param_vectorizer__max_df param_vectorizer__min_df param_vectorizer__ngram_range                                             params  rank_test_score  split0_test_score  split0_train_score  std_fit_time  std_score_time  std_test_score  std_train_score
0       0.133262         0.006921              1.0               1.0                              5                     0.75                     0.01                        (1, 1)  {u'vectorizer__min_df': 0.01, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
1       0.145061         0.006883              1.0               1.0                              5                     0.75                     0.02                        (1, 1)  {u'vectorizer__min_df': 0.02, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
### Start Time 2019/11/06-11-28-00  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=982   randForSplit=66   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1dec0a70>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=982,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0886	bm
+0.0717	suffici
+0.0581	helper
+0.0472	dendrit
+0.0384	predomin
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-28-01. Total      1.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0375            0.01s
         2           0.9017            0.01s
         3           0.7892            0.00s
         4           0.6945            0.00s
         5           0.6137            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0375            0.01s
         2           0.9017            0.01s
         3           0.7892            0.00s
         4           0.6945            0.00s
         5           0.6137            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0730            0.01s
         2           0.9306            0.01s
         3           0.8133            0.00s
         4           0.7148            0.00s
         5           0.6311            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0375            0.01s
         2           0.9017            0.01s
         3           0.7892            0.00s
         4           0.6945            0.00s
         5           0.6137            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0541            0.01s
         2           0.9173            0.01s
         3           0.8036            0.00s
         4           0.7075            0.00s
         5           0.6254            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0541            0.01s
         2           0.9173            0.01s
         3           0.8036            0.00s
         4           0.7075            0.00s
         5           0.6254            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0835            0.01s
         2           0.9412            0.01s
         3           0.8234            0.00s
         4           0.7242            0.00s
         5           0.6397            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0541            0.01s
         2           0.9173            0.01s
         3           0.8036            0.00s
         4           0.7075            0.00s
         5           0.6254            0.00s
### Start Time 2019/11/06-11-30-49  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=794   randForSplit=695   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a16bcaa70>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=794,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:
   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_classifier__n_estimators param_vectorizer__max_df param_vectorizer__min_df param_vectorizer__ngram_range                                             params  rank_test_score  split0_test_score  split0_train_score  std_fit_time  std_score_time  std_test_score  std_train_score
0       0.138284         0.006893              1.0               1.0                              5                     0.75                     0.01                        (1, 1)  {u'vectorizer__min_df': 0.01, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
1       0.133928         0.006748              1.0               1.0                              5                     0.75                     0.02                        (1, 1)  {u'vectorizer__min_df': 0.02, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.1359	exampl
+0.0717	govern
+0.0581	ventral
+0.0384	histogram
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-30-50. Total      1.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0515            0.01s
         2           0.9148            0.01s
         3           0.8013            0.00s
         4           0.7055            0.00s
         5           0.6236            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0515            0.01s
         2           0.9148            0.01s
         3           0.8013            0.00s
         4           0.7055            0.00s
         5           0.6236            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0815            0.01s
         2           0.9392            0.01s
         3           0.8215            0.00s
         4           0.7225            0.00s
         5           0.6380            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0515            0.01s
         2           0.9148            0.01s
         3           0.8013            0.00s
         4           0.7055            0.00s
         5           0.6236            0.00s
### Start Time 2019/11/06-11-35-50  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=414   randForSplit=579   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1b3f4a70>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=414,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Results:
{'std_train_score': array([0., 0.]), 'rank_test_score': array([1, 1], dtype=int32), 'std_test_score': array([0., 0.]), 'param_vectorizer__min_df': masked_array(data=[0.01, 0.02],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_vectorizer__max_df': masked_array(data=[0.75, 0.75],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_train_score': array([1., 1.]), 'split0_test_score': array([1., 1.]), 'mean_test_score': array([1., 1.]), 'std_score_time': array([0., 0.]), 'params': [{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}], 'std_fit_time': array([0., 0.]), 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1)],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_classifier__n_estimators': masked_array(data=[5, 5],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_score_time': array([0.00775886, 0.00715995]), 'mean_fit_time': array([0.13383102, 0.13206005]), 'split0_train_score': array([1., 1.])}
### Grid Search DataFrame:
   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_classifier__n_estimators param_vectorizer__max_df param_vectorizer__min_df param_vectorizer__ngram_range                                             params  rank_test_score  split0_test_score  split0_train_score  std_fit_time  std_score_time  std_test_score  std_train_score
0       0.133831         0.007759              1.0               1.0                              5                     0.75                     0.01                        (1, 1)  {u'vectorizer__min_df': 0.01, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
1       0.132060         0.007160              1.0               1.0                              5                     0.75                     0.02                        (1, 1)  {u'vectorizer__min_df': 0.02, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0887	predomin
+0.0717	cage
+0.0581	dendrit
+0.0472	bm
+0.0384	exampl
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-35-51. Total      1.10 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0244            0.01s
         2           0.8889            0.01s
         3           0.7774            0.00s
         4           0.6836            0.00s
         5           0.6038            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0244            0.01s
         2           0.8889            0.01s
         3           0.7774            0.00s
         4           0.6836            0.00s
         5           0.6038            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0700            0.01s
         2           0.9275            0.01s
         3           0.8103            0.00s
         4           0.7120            0.00s
         5           0.6285            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0244            0.01s
         2           0.8889            0.01s
         3           0.7774            0.00s
         4           0.6836            0.00s
         5           0.6038            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0484            0.01s
         2           0.9120            0.01s
         3           0.7987            0.00s
         4           0.7031            0.00s
         5           0.6215            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0484            0.01s
         2           0.9120            0.01s
         3           0.7987            0.00s
         4           0.7031            0.00s
         5           0.6215            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0842            0.01s
         2           0.9419            0.01s
         3           0.8240            0.00s
         4           0.7248            0.00s
         5           0.6402            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0484            0.01s
         2           0.9120            0.01s
         3           0.7987            0.00s
         4           0.7031            0.00s
         5           0.6215            0.00s
### Start Time 2019/11/06-11-48-47  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=553   randForSplit=369   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a23546518>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=553,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Results:
{'std_train_score': array([0., 0.]), 'rank_test_score': array([1, 1], dtype=int32), 'std_test_score': array([0., 0.]), 'param_vectorizer__min_df': masked_array(data=[0.01, 0.02],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_vectorizer__max_df': masked_array(data=[0.75, 0.75],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_train_score': array([1., 1.]), 'split0_test_score': array([1., 1.]), 'mean_test_score': array([1., 1.]), 'std_score_time': array([0., 0.]), 'params': [{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}], 'std_fit_time': array([0., 0.]), 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1)],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_classifier__n_estimators': masked_array(data=[5, 5],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_score_time': array([0.00750613, 0.00778913]), 'mean_fit_time': array([0.14886999, 0.14549804]), 'split0_train_score': array([1., 1.])}
### Grid Search DataFrame:
[{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}]
1.000000
[{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}]
1.000000


### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0887	versus
+0.0717	predomin
+0.0581	environment
+0.0472	restor
+0.0384	cage
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-48-48. Total      1.18 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0474            0.01s
         2           0.9110            0.01s
         3           0.7978            0.00s
         4           0.7023            0.00s
         5           0.6208            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0474            0.01s
         2           0.9110            0.01s
         3           0.7978            0.00s
         4           0.7023            0.00s
         5           0.6208            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0835            0.01s
         2           0.9411            0.01s
         3           0.8233            0.00s
         4           0.7242            0.00s
         5           0.6396            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0474            0.01s
         2           0.9110            0.01s
         3           0.7978            0.00s
         4           0.7023            0.00s
         5           0.6208            0.00s
### Start Time 2019/11/06-11-49-25  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=895   randForSplit=354   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a2365c2d8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=895,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Results:
{'std_train_score': array([0., 0.]), 'rank_test_score': array([1, 1], dtype=int32), 'std_test_score': array([0., 0.]), 'param_vectorizer__min_df': masked_array(data=[0.01, 0.02],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_vectorizer__max_df': masked_array(data=[0.75, 0.75],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_train_score': array([1., 1.]), 'split0_test_score': array([1., 1.]), 'mean_test_score': array([1., 1.]), 'std_score_time': array([0., 0.]), 'params': [{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}], 'std_fit_time': array([0., 0.]), 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1)],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_classifier__n_estimators': masked_array(data=[5, 5],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_score_time': array([0.00686097, 0.00700283]), 'mean_fit_time': array([0.1306951 , 0.13166404]), 'split0_train_score': array([1., 1.])}
### Grid Search DataFrame:
{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
1.000000


### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0887	faster
+0.0717	ventral
+0.0581	restor
+0.0472	dendrit
+0.0384	predomin
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-49-26. Total      1.09 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.04s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.03s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.05s
         3           0.8041            0.05s
         4           0.7079            0.05s
         5           0.6258            0.04s
         6           0.5550            0.04s
         7           0.4934            0.04s
         8           0.4397            0.04s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.04s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.03s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.04s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.04s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.04s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.03s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.04s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.05s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.04s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0876            0.04s
         2           0.9452            0.04s
         3           0.8272            0.03s
         4           0.7277            0.03s
         5           0.6428            0.03s
         6           0.5697            0.03s
         7           0.5063            0.02s
         8           0.4510            0.02s
         9           0.4024            0.02s
        10           0.3597            0.02s
        20           0.1237            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.04s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
### Start Time 2019/11/06-11-51-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=49   randForSplit=197   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a254e3cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=49,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 5, 8]
classifier__n_estimators:[20, 25]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Results:
{'std_train_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), 'mean_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'param_vectorizer__min_df': masked_array(data=[0.01, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01,
                   0.02, 0.01, 0.02],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'param_vectorizer__max_df': masked_array(data=[0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
                   0.75, 0.75, 0.75],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'mean_train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'split0_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'param_classifier__max_depth': masked_array(data=[3, 3, 3, 3, 5, 5, 5, 5, 8, 8, 8, 8],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'params': [{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}], 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1),
                   (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'param_classifier__n_estimators': masked_array(data=[20, 20, 25, 25, 20, 20, 25, 25, 20, 20, 25, 25],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'mean_score_time': array([0.00832605, 0.00974298, 0.0074501 , 0.00775909, 0.00936604,
       0.00731206, 0.00784302, 0.00718117, 0.00812101, 0.00848389,
       0.00746894, 0.00808001]), 'mean_fit_time': array([0.16530704, 0.16287589, 0.17560601, 0.1658411 , 0.19232106,
       0.164711  , 0.17110991, 0.17250395, 0.15873694, 0.16837406,
       0.16787601, 0.16779304]), 'split0_train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}
### Grid Search DataFrame:
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
1.000000


### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0578	histogram
+0.0508	restor
+0.0502	ventral
+0.0320	insert
+0.0251	cre
+0.0231	faster
+0.0166	dendrit
+0.0136	modul
+0.0112	bm
+0.0111	puls
+0.0093	exampl
+0.0018	versus
+0.0015	deliveri
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-51-17. Total      4.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.04s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.04s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.04s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0625            0.04s
         2           0.9193            0.04s
         3           0.8023            0.03s
         4           0.7045            0.03s
         5           0.6215            0.03s
         6           0.5502            0.03s
         7           0.4886            0.02s
         8           0.4350            0.02s
         9           0.3881            0.02s
        10           0.3468            0.02s
        20           0.1191            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.04s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.04s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.02s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.04s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0837            0.04s
         2           0.9414            0.04s
         3           0.8236            0.03s
         4           0.7244            0.03s
         5           0.6398            0.03s
         6           0.5670            0.03s
         7           0.5039            0.02s
         8           0.4488            0.02s
         9           0.4005            0.02s
        10           0.3580            0.02s
        20           0.1231            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
### Start Time 2019/11/06-11-56-26  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=423   randForSplit=829   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a253ddd40>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=423,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 5, 8]
classifier__n_estimators:[20, 25]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0601	densiti
+0.0468	insert
+0.0439	versus
+0.0432	ventral
+0.0419	faster
+0.0251	histogram
+0.0163	dendrit
+0.0136	metabol
+0.0074	modul
+0.0027	cage
+0.0018	remark
+0.0015	term
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-56-30. Total      4.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.04s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.04s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.04s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.04s
         7           0.4826            0.04s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.04s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.05s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.05s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.04s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.04s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0704            0.04s
         2           0.9278            0.04s
         3           0.8107            0.03s
         4           0.7124            0.03s
         5           0.6288            0.03s
         6           0.5570            0.03s
         7           0.4948            0.02s
         8           0.4406            0.02s
         9           0.3931            0.02s
        10           0.3513            0.02s
        20           0.1207            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
### Start Time 2019/11/06-11-59-03  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=409   randForSplit=786   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1f6f1f38>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=409,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 5, 8]
classifier__n_estimators:[20, 25]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.1125	densiti
+0.0579	suffici
+0.0308	histogram
+0.0294	exampl
+0.0166	helper
+0.0135	environment
+0.0111	ventral
+0.0074	versus
+0.0060	cre
+0.0049	remark
+0.0048	term
+0.0040	bm
+0.0022	predomin
+0.0018	govern
+0.0012	dendrit
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-59-07. Total      4.09 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0560            0.26s
         2           0.9191            0.23s
         3           0.8052            0.22s
         4           0.7090            0.21s
         5           0.6267            0.20s
         6           0.5558            0.18s
         7           0.4942            0.17s
         8           0.4404            0.16s
         9           0.3931            0.15s
        10           0.3515            0.13s
        20           0.1211            0.00s
### Start Time 2019/11/07-12-42-27  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=590   randForSplit=391   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f801c762ea8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=590,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Feature weights: highest 20
+0.6959	gene
+0.0776	restor
+0.0639	modul
+0.0379	dendrit
+0.0251	figur result
+0.0204	cell_lin indic
+0.0166	faster
+0.0136	cage
+0.0111	histogram
+0.0090	helper
+0.0082	environment
+0.0074	deliveri
+0.0052	bm
+0.0042	mice mice
+0.0022	densiti
+0.0018	mice treat
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/07-12-42-28. Total      1.31 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0658            0.24s
         2           0.9281            0.22s
         3           0.8134            0.21s
         4           0.7163            0.19s
         5           0.6333            0.18s
         6           0.5617            0.17s
         7           0.4994            0.16s
         8           0.4450            0.15s
         9           0.3973            0.14s
        10           0.3553            0.12s
        20           0.1224            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0658            0.24s
         2           0.9281            0.22s
         3           0.8134            0.21s
         4           0.7163            0.20s
         5           0.6333            0.18s
         6           0.5617            0.17s
         7           0.4994            0.16s
         8           0.4450            0.14s
         9           0.3973            0.13s
        10           0.3553            0.12s
        20           0.1224            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0884            0.27s
         2           0.9459            0.24s
         3           0.8278            0.22s
         4           0.7283            0.21s
         5           0.6433            0.19s
         6           0.5702            0.18s
         7           0.5067            0.17s
         8           0.4514            0.15s
         9           0.4028            0.14s
        10           0.3601            0.13s
        20           0.1238            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0658            0.24s
         2           0.9281            0.22s
         3           0.8134            0.20s
         4           0.7163            0.19s
         5           0.6333            0.18s
         6           0.5617            0.17s
         7           0.4994            0.16s
         8           0.4450            0.14s
         9           0.3973            0.13s
        10           0.3553            0.12s
        20           0.1224            0.00s
### Start Time 2019/11/07-12-44-22  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=770   randForSplit=152   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f66053f6248>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=770,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-12-44-25. Total      2.51 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0325            0.28s
         2           0.8968            0.26s
         3           0.7847            0.24s
         4           0.6904            0.22s
         5           0.6100            0.20s
         6           0.5408            0.18s
         7           0.4807            0.17s
         8           0.4283            0.15s
         9           0.3823            0.14s
        10           0.3418            0.13s
        20           0.1177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0325            0.24s
         2           0.8968            0.22s
         3           0.7847            0.21s
         4           0.6904            0.20s
         5           0.6100            0.18s
         6           0.5408            0.17s
         7           0.4807            0.16s
         8           0.4283            0.15s
         9           0.3823            0.13s
        10           0.3418            0.12s
        20           0.1177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0728            0.25s
         2           0.9303            0.23s
         3           0.8131            0.22s
         4           0.7146            0.20s
         5           0.6309            0.19s
         6           0.5589            0.18s
         7           0.4965            0.16s
         8           0.4421            0.15s
         9           0.3945            0.14s
        10           0.3526            0.13s
        20           0.1212            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0325            0.25s
         2           0.8968            0.23s
         3           0.7847            0.21s
         4           0.6904            0.20s
         5           0.6100            0.19s
         6           0.5408            0.17s
         7           0.4807            0.16s
         8           0.4283            0.15s
         9           0.3823            0.14s
        10           0.3418            0.12s
        20           0.1177            0.00s
### Start Time 2019/11/07-12-47-24  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=321   randForSplit=544   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f810da6d488>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=321,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-12-47-33. Total      9.01 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0507            0.25s
         2           0.9141            0.25s
         3           0.8007            0.23s
         4           0.7049            0.21s
         5           0.6231            0.19s
         6           0.5525            0.18s
         7           0.4913            0.17s
         8           0.4378            0.15s
         9           0.3908            0.14s
        10           0.3494            0.13s
        20           0.1204            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0507            0.36s
         2           0.9141            0.29s
         3           0.8007            0.26s
         4           0.7049            0.23s
         5           0.6231            0.21s
         6           0.5525            0.19s
         7           0.4913            0.18s
         8           0.4378            0.16s
         9           0.3908            0.15s
        10           0.3494            0.13s
        20           0.1204            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0819            0.32s
         2           0.9396            0.27s
         3           0.8219            0.24s
         4           0.7228            0.23s
         5           0.6383            0.21s
         6           0.5657            0.19s
         7           0.5027            0.18s
         8           0.4477            0.16s
         9           0.3995            0.15s
        10           0.3571            0.14s
        20           0.1228            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0507            0.31s
         2           0.9141            0.29s
         3           0.8007            0.27s
         4           0.7049            0.25s
         5           0.6231            0.23s
         6           0.5525            0.22s
         7           0.4913            0.22s
         8           0.4378            0.20s
         9           0.3908            0.18s
        10           0.3494            0.16s
        20           0.1204            0.00s
### Start Time 2019/11/07-12-50-50  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=185   randForSplit=613   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f66f05539e0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=185,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-12-51-00. Total     10.05 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0517            0.26s
         2           0.9150            0.24s
         3           0.8015            0.22s
         4           0.7056            0.20s
         5           0.6237            0.19s
         6           0.5531            0.17s
         7           0.4918            0.16s
         8           0.4382            0.15s
         9           0.3912            0.14s
        10           0.3498            0.12s
        20           0.1205            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0517            0.27s
         2           0.9150            0.24s
         3           0.8015            0.22s
         4           0.7056            0.21s
         5           0.6237            0.20s
         6           0.5531            0.18s
         7           0.4918            0.17s
         8           0.4382            0.15s
         9           0.3912            0.14s
        10           0.3498            0.13s
        20           0.1205            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0786            0.29s
         2           0.9363            0.27s
         3           0.8188            0.25s
         4           0.7200            0.23s
         5           0.6358            0.21s
         6           0.5633            0.19s
         7           0.5005            0.20s
         8           0.4458            0.18s
         9           0.3978            0.16s
        10           0.3556            0.15s
        20           0.1222            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0517            0.28s
         2           0.9150            0.24s
         3           0.8015            0.22s
         4           0.7056            0.21s
         5           0.6237            0.19s
         6           0.5531            0.18s
         7           0.4918            0.16s
         8           0.4382            0.15s
         9           0.3912            0.14s
        10           0.3498            0.13s
        20           0.1205            0.00s
### Start Time 2019/11/07-12-57-24  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=69   randForSplit=908   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f4d316d7dd0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=69,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-12-57-35. Total     10.31 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0338            0.26s
         2           0.8980            0.23s
         3           0.7859            0.21s
         4           0.6914            0.20s
         5           0.6109            0.18s
         6           0.5416            0.17s
         7           0.4814            0.16s
         8           0.4290            0.15s
         9           0.3829            0.13s
        10           0.3424            0.12s
        20           0.1179            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0338            0.25s
         2           0.8980            0.22s
         3           0.7859            0.21s
         4           0.6914            0.19s
         5           0.6109            0.18s
         6           0.5416            0.17s
         7           0.4814            0.16s
         8           0.4290            0.14s
         9           0.3829            0.13s
        10           0.3424            0.12s
        20           0.1179            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0755            0.28s
         2           0.9331            0.27s
         3           0.8158            0.24s
         4           0.7171            0.22s
         5           0.6332            0.21s
         6           0.5610            0.19s
         7           0.4984            0.18s
         8           0.4439            0.16s
         9           0.3961            0.15s
        10           0.3540            0.13s
        20           0.1217            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0338            0.26s
         2           0.8980            0.24s
         3           0.7859            0.22s
         4           0.6914            0.21s
         5           0.6109            0.19s
         6           0.5416            0.18s
         7           0.4814            0.16s
         8           0.4290            0.15s
         9           0.3829            0.14s
        10           0.3424            0.13s
        20           0.1179            0.00s
### Start Time 2019/11/07-12-58-10  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=448   randForSplit=901   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f4067c4a4d0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=448,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-12-58-17. Total      6.68 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0584            0.45s
         2           0.9213            0.41s
         3           0.8072            0.35s
         4           0.7108            0.34s
         5           0.6283            0.32s
         6           0.5572            0.29s
         7           0.4955            0.26s
         8           0.4415            0.23s
         9           0.3942            0.20s
        10           0.3525            0.18s
        20           0.1214            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0584            0.31s
         2           0.9213            0.27s
         3           0.8072            0.24s
         4           0.7108            0.22s
         5           0.6283            0.21s
         6           0.5572            0.19s
         7           0.4955            0.18s
         8           0.4415            0.16s
         9           0.3942            0.15s
        10           0.3525            0.13s
        20           0.1214            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0899            0.29s
         2           0.9474            0.27s
         3           0.8292            0.25s
         4           0.7295            0.23s
         5           0.6444            0.22s
         6           0.5712            0.20s
         7           0.5076            0.20s
         8           0.4522            0.19s
         9           0.4035            0.18s
        10           0.3607            0.17s
        20           0.1240            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0584            0.37s
         2           0.9213            0.29s
         3           0.8072            0.25s
         4           0.7108            0.23s
         5           0.6283            0.21s
         6           0.5572            0.19s
         7           0.4955            0.18s
         8           0.4415            0.16s
         9           0.3942            0.15s
        10           0.3525            0.13s
        20           0.1214            0.00s
### Start Time 2019/11/07-12-59-57  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=51   randForSplit=425   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f087b1537e8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=51,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-00-07. Total     10.82 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0501            0.29s
         2           0.9135            0.25s
         3           0.8001            0.23s
         4           0.7044            0.21s
         5           0.6226            0.20s
         6           0.5521            0.18s
         7           0.4909            0.17s
         8           0.4374            0.16s
         9           0.3905            0.15s
        10           0.3492            0.13s
        20           0.1203            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0501            0.36s
         2           0.9135            0.36s
         3           0.8001            0.35s
         4           0.7044            0.33s
         5           0.6226            0.32s
         6           0.5521            0.30s
         7           0.4909            0.28s
         8           0.4374            0.25s
         9           0.3905            0.23s
        10           0.3492            0.20s
        20           0.1203            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0873            0.30s
         2           0.9448            0.26s
         3           0.8268            0.24s
         4           0.7273            0.23s
         5           0.6425            0.21s
         6           0.5694            0.20s
         7           0.5060            0.18s
         8           0.4507            0.17s
         9           0.4023            0.15s
        10           0.3596            0.14s
        20           0.1236            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0501            0.28s
         2           0.9135            0.25s
         3           0.8001            0.23s
         4           0.7044            0.21s
         5           0.6226            0.20s
         6           0.5521            0.18s
         7           0.4909            0.17s
         8           0.4374            0.15s
         9           0.3905            0.14s
        10           0.3492            0.13s
        20           0.1203            0.00s
### Start Time 2019/11/07-13-01-10  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=625   randForSplit=532   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f1892e638c0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=625,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-01-20. Total      9.93 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0475            0.25s
         2           0.9111            0.23s
         3           0.7979            0.23s
         4           0.7024            0.21s
         5           0.6208            0.20s
         6           0.5505            0.18s
         7           0.4895            0.17s
         8           0.4361            0.16s
         9           0.3894            0.15s
        10           0.3482            0.13s
        20           0.1199            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0475            0.32s
         2           0.9111            0.27s
         3           0.7979            0.24s
         4           0.7024            0.22s
         5           0.6208            0.20s
         6           0.5505            0.20s
         7           0.4895            0.19s
         8           0.4361            0.18s
         9           0.3894            0.16s
        10           0.3482            0.15s
        20           0.1199            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0902            0.27s
         2           0.9477            0.25s
         3           0.8294            0.23s
         4           0.7297            0.21s
         5           0.6447            0.20s
         6           0.5714            0.19s
         7           0.5078            0.18s
         8           0.4523            0.16s
         9           0.4037            0.15s
        10           0.3609            0.13s
        20           0.1241            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0475            0.27s
         2           0.9111            0.24s
         3           0.7979            0.23s
         4           0.7024            0.21s
         5           0.6208            0.19s
         6           0.5505            0.18s
         7           0.4895            0.17s
         8           0.4361            0.15s
         9           0.3894            0.14s
        10           0.3482            0.13s
        20           0.1199            0.00s
### Start Time 2019/11/07-13-01-40  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=132   randForSplit=770   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7fdb6a7de908>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=132,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-01-45. Total      5.64 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0543            0.27s
         2           0.9175            0.24s
         3           0.8037            0.22s
         4           0.7077            0.20s
         5           0.6256            0.19s
         6           0.5547            0.17s
         7           0.4932            0.16s
         8           0.4395            0.15s
         9           0.3924            0.14s
        10           0.3509            0.12s
        20           0.1208            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0543            0.25s
         2           0.9175            0.23s
         3           0.8037            0.21s
         4           0.7077            0.20s
         5           0.6256            0.18s
         6           0.5547            0.17s
         7           0.4932            0.16s
         8           0.4395            0.15s
         9           0.3924            0.13s
        10           0.3509            0.12s
        20           0.1208            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0850            0.26s
         2           0.9427            0.24s
         3           0.8248            0.22s
         4           0.7255            0.21s
         5           0.6408            0.19s
         6           0.5679            0.18s
         7           0.5047            0.17s
         8           0.4495            0.16s
         9           0.4011            0.14s
        10           0.3586            0.13s
        20           0.1233            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0543            0.24s
         2           0.9175            0.28s
         3           0.8037            0.24s
         4           0.7077            0.22s
         5           0.6256            0.20s
         6           0.5547            0.18s
         7           0.4932            0.17s
         8           0.4395            0.15s
         9           0.3924            0.14s
        10           0.3509            0.13s
        20           0.1208            0.00s
### Start Time 2019/11/07-13-02-29  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=228   randForSplit=92   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f2414ad5248>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=228,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-02-31. Total      2.57 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0464            0.24s
         2           0.9100            0.23s
         3           0.7969            0.22s
         4           0.7015            0.20s
         5           0.6200            0.19s
         6           0.5498            0.18s
         7           0.4888            0.16s
         8           0.4356            0.15s
         9           0.3888            0.14s
        10           0.3477            0.12s
        20           0.1197            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0464            0.24s
         2           0.9100            0.22s
         3           0.7969            0.21s
         4           0.7015            0.19s
         5           0.6200            0.18s
         6           0.5498            0.17s
         7           0.4888            0.16s
         8           0.4356            0.14s
         9           0.3888            0.13s
        10           0.3477            0.12s
        20           0.1197            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0779            0.26s
         2           0.9355            0.24s
         3           0.8181            0.23s
         4           0.7193            0.21s
         5           0.6351            0.20s
         6           0.5628            0.19s
         7           0.5000            0.18s
         8           0.4453            0.17s
         9           0.3974            0.15s
        10           0.3552            0.14s
        20           0.1221            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0464            0.24s
         2           0.9100            0.22s
         3           0.7969            0.21s
         4           0.7015            0.20s
         5           0.6200            0.19s
         6           0.5498            0.18s
         7           0.4888            0.16s
         8           0.4356            0.15s
         9           0.3888            0.14s
        10           0.3477            0.13s
        20           0.1197            0.00s
### Start Time 2019/11/07-13-06-38  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=707   randForSplit=481   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f12f4e09d40>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=707,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-06-44. Total      5.38 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0375            0.25s
         2           0.9017            0.23s
         3           0.7892            0.22s
         4           0.6945            0.21s
         5           0.6137            0.20s
         6           0.5441            0.19s
         7           0.4837            0.17s
         8           0.4310            0.16s
         9           0.3847            0.15s
        10           0.3440            0.13s
        20           0.1185            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0375            0.27s
         2           0.9017            0.24s
         3           0.7892            0.22s
         4           0.6945            0.20s
         5           0.6137            0.19s
         6           0.5441            0.18s
         7           0.4837            0.17s
         8           0.4310            0.15s
         9           0.3847            0.14s
        10           0.3440            0.13s
        20           0.1185            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0738            0.27s
         2           0.9314            0.24s
         3           0.8141            0.23s
         4           0.7156            0.21s
         5           0.6318            0.20s
         6           0.5597            0.19s
         7           0.4973            0.17s
         8           0.4428            0.16s
         9           0.3951            0.15s
        10           0.3532            0.13s
        20           0.1214            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0375            0.26s
         2           0.9017            0.24s
         3           0.7892            0.22s
         4           0.6945            0.21s
         5           0.6137            0.20s
         6           0.5441            0.18s
         7           0.4837            0.17s
         8           0.4310            0.16s
         9           0.3847            0.15s
        10           0.3440            0.13s
        20           0.1185            0.00s
### Start Time 2019/11/07-13-07-48  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=208   randForSplit=228   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7faefa0c09e0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=208,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-07-58. Total      9.95 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0452            0.30s
         2           0.9090            0.26s
         3           0.7960            0.24s
         4           0.7007            0.22s
         5           0.6193            0.20s
         6           0.5491            0.23s
         7           0.4882            0.20s
         8           0.4350            0.18s
         9           0.3883            0.17s
        10           0.3472            0.15s
        20           0.1196            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0452            0.25s
         2           0.9090            0.22s
         3           0.7960            0.29s
         4           0.7007            0.25s
         5           0.6193            0.23s
         6           0.5491            0.20s
         7           0.4882            0.18s
         8           0.4350            0.17s
         9           0.3883            0.15s
        10           0.3472            0.13s
        20           0.1196            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0845            0.27s
         2           0.9421            0.27s
         3           0.8243            0.24s
         4           0.7250            0.22s
         5           0.6404            0.21s
         6           0.5675            0.19s
         7           0.5043            0.18s
         8           0.4492            0.16s
         9           0.4009            0.17s
        10           0.3583            0.15s
        20           0.1232            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0452            0.25s
         2           0.9090            0.23s
         3           0.7960            0.21s
         4           0.7007            0.20s
         5           0.6193            0.19s
         6           0.5491            0.18s
         7           0.4882            0.17s
         8           0.4350            0.15s
         9           0.3883            0.14s
        10           0.3472            0.13s
        20           0.1196            0.00s
### Start Time 2019/11/07-13-09-29  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=174   randForSplit=188   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f9e339a0758>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=174,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-09-38. Total      9.03 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0342            0.26s
         2           0.8984            0.23s
         3           0.7862            0.21s
         4           0.6918            0.20s
         5           0.6112            0.19s
         6           0.5419            0.17s
         7           0.4817            0.16s
         8           0.4292            0.15s
         9           0.3831            0.13s
        10           0.3425            0.12s
        20           0.1180            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0342            0.24s
         2           0.8984            0.22s
         3           0.7862            0.21s
         4           0.6918            0.19s
         5           0.6112            0.18s
         6           0.5419            0.17s
         7           0.4817            0.16s
         8           0.4292            0.15s
         9           0.3831            0.13s
        10           0.3425            0.12s
        20           0.1180            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0763            0.28s
         2           0.9340            0.27s
         3           0.8166            0.25s
         4           0.7179            0.23s
         5           0.6339            0.21s
         6           0.5616            0.19s
         7           0.4990            0.18s
         8           0.4444            0.17s
         9           0.3965            0.15s
        10           0.3544            0.14s
        20           0.1218            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0342            0.25s
         2           0.8984            0.23s
         3           0.7862            0.22s
         4           0.6918            0.21s
         5           0.6112            0.19s
         6           0.5419            0.18s
         7           0.4817            0.17s
         8           0.4292            0.16s
         9           0.3831            0.14s
        10           0.3425            0.14s
        20           0.1180            0.00s
### Start Time 2019/11/07-13-10-35  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=674   randForSplit=143   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7fdd03621320>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=674,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-10-38. Total      2.69 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0389            0.24s
         2           0.9029            0.22s
         3           0.7904            0.21s
         4           0.6956            0.19s
         5           0.6147            0.18s
         6           0.5450            0.17s
         7           0.4845            0.16s
         8           0.4317            0.14s
         9           0.3854            0.13s
        10           0.3446            0.12s
        20           0.1187            0.00s
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0389            0.24s
         2           0.9029            0.22s
         3           0.7904            0.20s
         4           0.6956            0.19s
         5           0.6147            0.18s
         6           0.5450            0.17s
         7           0.4845            0.16s
         8           0.4317            0.14s
         9           0.3854            0.13s
        10           0.3446            0.12s
        20           0.1187            0.00s
Not skipping fit 29
      Iter       Train Loss   Remaining Time 
         1           1.0769            0.25s
         2           0.9346            0.23s
         3           0.8172            0.22s
         4           0.7185            0.20s
         5           0.6344            0.19s
         6           0.5621            0.18s
         7           0.4994            0.16s
         8           0.4448            0.15s
         9           0.3969            0.14s
        10           0.3547            0.13s
        20           0.1219            0.00s
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0389            0.25s
         2           0.9029            0.23s
         3           0.7904            0.21s
         4           0.6956            0.20s
         5           0.6147            0.19s
         6           0.5450            0.18s
         7           0.4845            0.16s
         8           0.4317            0.15s
         9           0.3854            0.14s
        10           0.3446            0.12s
        20           0.1187            0.00s
### Start Time 2019/11/07-13-12-27  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=309   randForSplit=633   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f7a332f7170>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=309,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-12-29. Total      2.52 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0476            0.25s
         2           0.9112            0.23s
         3           0.7980            0.21s
         4           0.7025            0.20s
         5           0.6209            0.19s
         6           0.5506            0.17s
         7           0.4895            0.16s
         8           0.4362            0.15s
         9           0.3894            0.13s
        10           0.3482            0.12s
        20           0.1199            0.00s
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0476            0.25s
         2           0.9112            0.22s
         3           0.7980            0.21s
         4           0.7025            0.19s
         5           0.6209            0.18s
         6           0.5506            0.17s
         7           0.4895            0.16s
         8           0.4362            0.14s
         9           0.3894            0.13s
        10           0.3482            0.12s
        20           0.1199            0.00s
Not skipping fit 29
      Iter       Train Loss   Remaining Time 
         1           1.0836            0.27s
         2           0.9413            0.28s
         3           0.8235            0.25s
         4           0.7243            0.23s
         5           0.6397            0.21s
         6           0.5669            0.19s
         7           0.5038            0.18s
         8           0.4487            0.16s
         9           0.4004            0.15s
        10           0.3579            0.14s
        20           0.1231            0.00s
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0476            0.23s
         2           0.9112            0.22s
         3           0.7980            0.20s
         4           0.7025            0.19s
         5           0.6209            0.18s
         6           0.5506            0.17s
         7           0.4895            0.16s
         8           0.4362            0.15s
         9           0.3894            0.14s
        10           0.3482            0.12s
        20           0.1199            0.00s
### Start Time 2019/11/07-13-17-24  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=187   randForSplit=695   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f6ceaf09170>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=187,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[20]

### Grid Search Scores:
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-17-27. Total      2.54 seconds

Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0446            0.12s
         2           0.9084            0.10s
         3           0.7954            0.09s
         4           0.7002            0.07s
         5           0.6188            0.06s
         6           0.5487            0.05s
         7           0.4878            0.04s
         8           0.4347            0.02s
         9           0.3880            0.01s
        10           0.3470            0.00s
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0446            0.24s
         2           0.9084            0.22s
         3           0.7954            0.21s
         4           0.7002            0.19s
         5           0.6188            0.18s
         6           0.5487            0.17s
         7           0.4878            0.16s
         8           0.4347            0.14s
         9           0.3880            0.13s
        10           0.3470            0.12s
        20           0.1195            0.00s
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0446            0.11s
         2           0.9084            0.10s
         3           0.7954            0.09s
         4           0.7002            0.07s
         5           0.6188            0.06s
         6           0.5487            0.05s
         7           0.4878            0.04s
         8           0.4347            0.02s
         9           0.3880            0.01s
        10           0.3470            0.00s
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0446            0.25s
         2           0.9084            0.23s
         3           0.7954            0.21s
         4           0.7002            0.20s
         5           0.6188            0.18s
         6           0.5487            0.17s
         7           0.4878            0.16s
         8           0.4347            0.15s
         9           0.3880            0.13s
        10           0.3470            0.12s
        20           0.1195            0.00s
Not skipping fit 29
      Iter       Train Loss   Remaining Time 
         1           1.0817            0.12s
         2           0.9394            0.10s
         3           0.8217            0.09s
         4           0.7227            0.08s
         5           0.6382            0.06s
         6           0.5656            0.05s
         7           0.5026            0.04s
         8           0.4476            0.03s
         9           0.3994            0.01s
        10           0.3571            0.00s
Not skipping fit 26
      Iter       Train Loss   Remaining Time 
         1           1.0446            0.12s
         2           0.9084            0.10s
         3           0.7954            0.09s
         4           0.7002            0.07s
         5           0.6188            0.06s
         6           0.5487            0.05s
         7           0.4878            0.04s
         8           0.4347            0.02s
         9           0.3880            0.01s
        10           0.3470            0.00s
### Start Time 2019/11/07-13-18-21  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=496   randForSplit=881   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f7d59d378c0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=496,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-18-24. Total      3.32 seconds

Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0607            0.12s
         2           0.9234            0.10s
         3           0.8091            0.09s
         4           0.7125            0.07s
         5           0.6299            0.06s
         6           0.5586            0.05s
         7           0.4967            0.04s
         8           0.4426            0.02s
         9           0.3952            0.01s
        10           0.3534            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0607            0.26s
         2           0.9234            0.24s
         3           0.8091            0.22s
         4           0.7125            0.20s
         5           0.6299            0.19s
         6           0.5586            0.17s
         7           0.4967            0.16s
         8           0.4426            0.15s
         9           0.3952            0.14s
        10           0.3534            0.12s
        20           0.1217            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0607            0.11s
         2           0.9234            0.10s
         3           0.8091            0.09s
         4           0.7125            0.07s
         5           0.6299            0.06s
         6           0.5586            0.05s
         7           0.4967            0.04s
         8           0.4426            0.02s
         9           0.3952            0.01s
        10           0.3534            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0607            0.24s
         2           0.9234            0.22s
         3           0.8091            0.21s
         4           0.7125            0.19s
         5           0.6299            0.18s
         6           0.5586            0.17s
         7           0.4967            0.16s
         8           0.4426            0.14s
         9           0.3952            0.13s
        10           0.3534            0.12s
        20           0.1217            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0939            0.12s
         2           0.9512            0.10s
         3           0.8327            0.10s
         4           0.7327            0.08s
         5           0.6474            0.07s
         6           0.5738            0.05s
         7           0.5100            0.04s
         8           0.4543            0.03s
         9           0.4054            0.01s
        10           0.3624            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0607            0.12s
         2           0.9234            0.10s
         3           0.8091            0.09s
         4           0.7125            0.08s
         5           0.6299            0.06s
         6           0.5586            0.05s
         7           0.4967            0.04s
         8           0.4426            0.03s
         9           0.3952            0.01s
        10           0.3534            0.00s
### Start Time 2019/11/07-13-19-48  GB.py	index file: index.out
Training data path:   /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /home/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=251   randForSplit=569   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x7f360658b1b8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=251,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### End Time 2019/11/07-13-19-52. Total      3.43 seconds

Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0447            0.10s
         2           0.9085            0.09s
         3           0.7955            0.07s
         4           0.7002            0.06s
         5           0.6189            0.06s
         6           0.5487            0.04s
         7           0.4879            0.03s
         8           0.4347            0.02s
         9           0.3881            0.01s
        10           0.3470            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0447            0.22s
         2           0.9085            0.22s
         3           0.7955            0.20s
         4           0.7002            0.18s
         5           0.6189            0.17s
         6           0.5487            0.15s
         7           0.4879            0.14s
         8           0.4347            0.13s
         9           0.3881            0.12s
        10           0.3470            0.10s
        20           0.1195            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0447            0.09s
         2           0.9085            0.08s
         3           0.7955            0.07s
         4           0.7002            0.06s
         5           0.6189            0.05s
         6           0.5487            0.04s
         7           0.4879            0.03s
         8           0.4347            0.02s
         9           0.3881            0.01s
        10           0.3470            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0447            0.21s
         2           0.9085            0.20s
         3           0.7955            0.20s
         4           0.7002            0.19s
         5           0.6189            0.17s
         6           0.5487            0.16s
         7           0.4879            0.14s
         8           0.4347            0.13s
         9           0.3881            0.12s
        10           0.3470            0.11s
        20           0.1195            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0827            0.10s
         2           0.9404            0.09s
         3           0.8227            0.08s
         4           0.7235            0.07s
         5           0.6390            0.06s
         6           0.5663            0.04s
         7           0.5032            0.03s
         8           0.4482            0.02s
         9           0.4000            0.01s
        10           0.3575            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0447            0.10s
         2           0.9085            0.08s
         3           0.7955            0.07s
         4           0.7002            0.06s
         5           0.6189            0.05s
         6           0.5487            0.04s
         7           0.4879            0.03s
         8           0.4347            0.02s
         9           0.3881            0.01s
        10           0.3470            0.00s
Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0604            0.10s
         2           0.9231            0.08s
         3           0.8089            0.07s
         4           0.7123            0.06s
         5           0.6297            0.05s
         6           0.5584            0.04s
         7           0.4965            0.03s
         8           0.4425            0.02s
         9           0.3950            0.01s
        10           0.3532            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0604            0.20s
         2           0.9231            0.21s
         3           0.8089            0.19s
         4           0.7123            0.17s
         5           0.6297            0.16s
         6           0.5584            0.15s
         7           0.4965            0.13s
         8           0.4425            0.12s
         9           0.3950            0.11s
        10           0.3532            0.10s
        20           0.1217            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0604            0.10s
         2           0.9231            0.08s
         3           0.8089            0.07s
         4           0.7123            0.06s
         5           0.6297            0.05s
         6           0.5584            0.04s
         7           0.4965            0.03s
         8           0.4425            0.02s
         9           0.3950            0.01s
        10           0.3532            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0604            0.20s
         2           0.9231            0.19s
         3           0.8089            0.18s
         4           0.7123            0.17s
         5           0.6297            0.16s
         6           0.5584            0.15s
         7           0.4965            0.13s
         8           0.4425            0.12s
         9           0.3950            0.11s
        10           0.3532            0.10s
        20           0.1217            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0804            0.12s
         2           0.9381            0.10s
         3           0.8205            0.08s
         4           0.7216            0.07s
         5           0.6372            0.06s
         6           0.5646            0.05s
         7           0.5017            0.04s
         8           0.4469            0.02s
         9           0.3988            0.01s
        10           0.3564            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0604            0.10s
         2           0.9231            0.09s
         3           0.8089            0.08s
         4           0.7123            0.07s
         5           0.6297            0.06s
         6           0.5584            0.04s
         7           0.4965            0.03s
         8           0.4425            0.02s
         9           0.3950            0.01s
        10           0.3532            0.00s
Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.10s
         2           0.9271            0.09s
         3           0.8124            0.08s
         4           0.7155            0.07s
         5           0.6325            0.05s
         6           0.5610            0.04s
         7           0.4988            0.03s
         8           0.4445            0.02s
         9           0.3969            0.01s
        10           0.3549            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.20s
         2           0.9271            0.19s
         3           0.8124            0.17s
         4           0.7155            0.17s
         5           0.6325            0.16s
         6           0.5610            0.15s
         7           0.4988            0.14s
         8           0.4445            0.13s
         9           0.3969            0.12s
        10           0.3549            0.11s
        20           0.1222            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.09s
         2           0.9271            0.08s
         3           0.8124            0.07s
         4           0.7155            0.06s
         5           0.6325            0.05s
         6           0.5610            0.04s
         7           0.4988            0.03s
         8           0.4445            0.02s
         9           0.3969            0.01s
        10           0.3549            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.20s
         2           0.9271            0.18s
         3           0.8124            0.17s
         4           0.7155            0.17s
         5           0.6325            0.16s
         6           0.5610            0.15s
         7           0.4988            0.13s
         8           0.4445            0.13s
         9           0.3969            0.11s
        10           0.3549            0.10s
        20           0.1222            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0878            0.10s
         2           0.9453            0.09s
         3           0.8273            0.08s
         4           0.7278            0.07s
         5           0.6429            0.06s
         6           0.5698            0.05s
         7           0.5064            0.03s
         8           0.4510            0.02s
         9           0.4025            0.01s
        10           0.3598            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.09s
         2           0.9271            0.08s
         3           0.8124            0.07s
         4           0.7155            0.06s
         5           0.6325            0.05s
         6           0.5610            0.04s
         7           0.4988            0.03s
         8           0.4445            0.02s
         9           0.3969            0.01s
        10           0.3549            0.00s
### Start Time 2020/01/02-14-03-58  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=993   randForSplit=397   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1845a248>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=993,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0806	figur differ
+0.0554	term
+0.0528	figur result
+0.0348	mice treat
+0.0283	suffici
+0.0231	dendrit
+0.0188	faster
+0.0102	cell exampl
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427
+0.0000	a427 h358

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/01/02-14-04-04. Total      6.28 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Thu Jan  2 14:04:11 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0260            0.10s
         2           0.8905            0.10s
         3           0.7788            0.08s
         4           0.6850            0.07s
         5           0.6051            0.06s
         6           0.5363            0.05s
         7           0.4767            0.03s
         8           0.4247            0.02s
         9           0.3791            0.01s
        10           0.3389            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0260            0.35s
         2           0.8905            0.26s
         3           0.7788            0.23s
         4           0.6850            0.21s
         5           0.6051            0.18s
         6           0.5363            0.17s
         7           0.4767            0.15s
         8           0.4247            0.14s
         9           0.3791            0.12s
        10           0.3389            0.11s
        20           0.1167            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0260            0.09s
         2           0.8905            0.08s
         3           0.7788            0.07s
         4           0.6850            0.06s
         5           0.6051            0.05s
         6           0.5363            0.04s
         7           0.4767            0.03s
         8           0.4247            0.02s
         9           0.3791            0.01s
        10           0.3389            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0260            0.20s
         2           0.8905            0.19s
         3           0.7788            0.17s
         4           0.6850            0.17s
         5           0.6051            0.16s
         6           0.5363            0.15s
         7           0.4767            0.14s
         8           0.4247            0.12s
         9           0.3791            0.11s
        10           0.3389            0.10s
        20           0.1167            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0713            0.10s
         2           0.9288            0.09s
         3           0.8116            0.08s
         4           0.7133            0.07s
         5           0.6296            0.06s
         6           0.5577            0.05s
         7           0.4955            0.04s
         8           0.4412            0.02s
         9           0.3936            0.01s
        10           0.3518            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0260            0.10s
         2           0.8905            0.08s
         3           0.7788            0.07s
         4           0.6850            0.06s
         5           0.6051            0.05s
         6           0.5363            0.04s
         7           0.4767            0.03s
         8           0.4247            0.02s
         9           0.3791            0.01s
        10           0.3389            0.00s
### Start Time 2020/01/02-15-04-03  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=743   randForSplit=974   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a16fcd8c0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=743,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0938	dendrit
+0.0776	deliveri
+0.0529	signific differ
+0.0230	signific increas
+0.0188	densiti
+0.0153	mice mice
+0.0125	restor
+0.0102	remark
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427
+0.0000	a427 h358

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/01/02-15-04-09. Total      6.33 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Thu Jan  2 15:04:16 2020
Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Thu Jan  2 15:13:49 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0338            0.10s
         2           0.8981            0.08s
         3           0.7859            0.07s
         4           0.6915            0.06s
         5           0.6109            0.05s
         6           0.5416            0.04s
         7           0.4815            0.03s
         8           0.4290            0.02s
         9           0.3829            0.01s
        10           0.3424            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0338            0.22s
         2           0.8981            0.19s
         3           0.7859            0.18s
         4           0.6915            0.17s
         5           0.6109            0.16s
         6           0.5416            0.15s
         7           0.4815            0.14s
         8           0.4290            0.13s
         9           0.3829            0.12s
        10           0.3424            0.11s
        20           0.1179            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0338            0.09s
         2           0.8981            0.08s
         3           0.7859            0.07s
         4           0.6915            0.06s
         5           0.6109            0.05s
         6           0.5416            0.04s
         7           0.4815            0.03s
         8           0.4290            0.02s
         9           0.3829            0.01s
        10           0.3424            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0338            0.20s
         2           0.8981            0.19s
         3           0.7859            0.17s
         4           0.6915            0.16s
         5           0.6109            0.16s
         6           0.5416            0.15s
         7           0.4815            0.14s
         8           0.4290            0.13s
         9           0.3829            0.12s
        10           0.3424            0.10s
        20           0.1179            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0723            0.10s
         2           0.9298            0.09s
         3           0.8126            0.08s
         4           0.7142            0.07s
         5           0.6305            0.06s
         6           0.5585            0.04s
         7           0.4962            0.03s
         8           0.4418            0.02s
         9           0.3942            0.01s
        10           0.3523            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0338            0.09s
         2           0.8981            0.08s
         3           0.7859            0.07s
         4           0.6915            0.06s
         5           0.6109            0.05s
         6           0.5416            0.04s
         7           0.4815            0.03s
         8           0.4290            0.02s
         9           0.3829            0.01s
        10           0.3424            0.00s
### Start Time 2020/01/02-15-31-49  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=789   randForSplit=323   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a18ea27a0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=789,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0655	mice mice
+0.0529	metabol
+0.0429	modul
+0.0348	cell_lin indic
+0.0283	mice signific
+0.0255	densiti
+0.0230	figur result
+0.0188	repres imag
+0.0125	insert
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/01/02-15-31-56. Total      6.30 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Thu Jan  2 15:32:04 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0501            0.10s
         2           0.9135            0.09s
         3           0.8001            0.08s
         4           0.7044            0.07s
         5           0.6226            0.05s
         6           0.5521            0.04s
         7           0.4909            0.03s
         8           0.4374            0.02s
         9           0.3905            0.01s
        10           0.3492            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0501            0.21s
         2           0.9135            0.19s
         3           0.8001            0.20s
         4           0.7044            0.18s
         5           0.6226            0.17s
         6           0.5521            0.16s
         7           0.4909            0.15s
         8           0.4374            0.14s
         9           0.3905            0.12s
        10           0.3492            0.11s
        20           0.1203            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0501            0.10s
         2           0.9135            0.08s
         3           0.8001            0.07s
         4           0.7044            0.07s
         5           0.6226            0.06s
         6           0.5521            0.04s
         7           0.4909            0.03s
         8           0.4374            0.02s
         9           0.3905            0.01s
        10           0.3492            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0501            0.20s
         2           0.9135            0.19s
         3           0.8001            0.18s
         4           0.7044            0.18s
         5           0.6226            0.17s
         6           0.5521            0.16s
         7           0.4909            0.15s
         8           0.4374            0.13s
         9           0.3905            0.12s
        10           0.3492            0.11s
        20           0.1203            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0873            0.10s
         2           0.9448            0.09s
         3           0.8268            0.08s
         4           0.7273            0.07s
         5           0.6425            0.06s
         6           0.5694            0.05s
         7           0.5060            0.03s
         8           0.4507            0.02s
         9           0.4023            0.01s
        10           0.3596            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0501            0.10s
         2           0.9135            0.09s
         3           0.8001            0.08s
         4           0.7044            0.07s
         5           0.6226            0.06s
         6           0.5521            0.05s
         7           0.4909            0.03s
         8           0.4374            0.02s
         9           0.3905            0.01s
        10           0.3492            0.00s
### Start Time 2020/01/03-08-17-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=625   randForSplit=504   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a24b6fe18>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=625,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0654	govern
+0.0529	group level
+0.0436	term
+0.0429	densiti
+0.0348	suffici
+0.0231	cell_lin indic
+0.0188	dendrit
+0.0125	metabol
+0.0102	cage
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/01/03-08-17-10. Total      6.73 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Fri Jan  3 08:17:25 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0282            0.10s
         2           0.8926            0.09s
         3           0.7808            0.08s
         4           0.6868            0.07s
         5           0.6067            0.05s
         6           0.5378            0.04s
         7           0.4780            0.03s
         8           0.4259            0.02s
         9           0.3801            0.01s
        10           0.3399            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0282            0.21s
         2           0.8926            0.19s
         3           0.7808            0.19s
         4           0.6868            0.18s
         5           0.6067            0.17s
         6           0.5378            0.15s
         7           0.4780            0.14s
         8           0.4259            0.13s
         9           0.3801            0.12s
        10           0.3399            0.11s
        20           0.1170            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0282            0.10s
         2           0.8926            0.08s
         3           0.7808            0.07s
         4           0.6868            0.06s
         5           0.6067            0.05s
         6           0.5378            0.04s
         7           0.4780            0.03s
         8           0.4259            0.02s
         9           0.3801            0.01s
        10           0.3399            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0282            0.25s
         2           0.8926            0.21s
         3           0.7808            0.20s
         4           0.6868            0.19s
         5           0.6067            0.19s
         6           0.5378            0.17s
         7           0.4780            0.16s
         8           0.4259            0.14s
         9           0.3801            0.13s
        10           0.3399            0.12s
        20           0.1170            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0685            0.10s
         2           0.9259            0.09s
         3           0.8087            0.08s
         4           0.7106            0.07s
         5           0.6271            0.06s
         6           0.5554            0.05s
         7           0.4934            0.03s
         8           0.4393            0.02s
         9           0.3920            0.01s
        10           0.3503            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0282            0.10s
         2           0.8926            0.08s
         3           0.7808            0.07s
         4           0.6868            0.07s
         5           0.6067            0.05s
         6           0.5378            0.04s
         7           0.4780            0.03s
         8           0.4259            0.02s
         9           0.3801            0.01s
        10           0.3399            0.00s
### Start Time 2020/01/03-08-18-47  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=534   randForSplit=762   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a23ed4098>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=534,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0655	bm
+0.0529	govern
+0.0428	figur signific
+0.0408	group level
+0.0348	puls
+0.0230	metabol
+0.0188	figur indic
+0.0153	helper
+0.0102	environment
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/01/03-08-18-53. Total      6.56 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Fri Jan  3 08:19:03 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0512            0.10s
         2           0.9146            0.09s
         3           0.8011            0.08s
         4           0.7053            0.07s
         5           0.6234            0.06s
         6           0.5528            0.05s
         7           0.4915            0.03s
         8           0.4380            0.02s
         9           0.3910            0.01s
        10           0.3496            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0512            0.24s
         2           0.9146            0.22s
         3           0.8011            0.21s
         4           0.7053            0.20s
         5           0.6234            0.18s
         6           0.5528            0.17s
         7           0.4915            0.15s
         8           0.4380            0.14s
         9           0.3910            0.13s
        10           0.3496            0.12s
        20           0.1204            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0512            0.10s
         2           0.9146            0.09s
         3           0.8011            0.07s
         4           0.7053            0.07s
         5           0.6234            0.05s
         6           0.5528            0.04s
         7           0.4915            0.03s
         8           0.4380            0.02s
         9           0.3910            0.01s
        10           0.3496            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0512            0.20s
         2           0.9146            0.19s
         3           0.8011            0.18s
         4           0.7053            0.17s
         5           0.6234            0.16s
         6           0.5528            0.15s
         7           0.4915            0.14s
         8           0.4380            0.13s
         9           0.3910            0.12s
        10           0.3496            0.11s
        20           0.1204            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0819            0.10s
         2           0.9396            0.09s
         3           0.8219            0.08s
         4           0.7228            0.07s
         5           0.6384            0.06s
         6           0.5657            0.05s
         7           0.5027            0.03s
         8           0.4477            0.02s
         9           0.3996            0.01s
        10           0.3571            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0512            0.10s
         2           0.9146            0.08s
         3           0.8011            0.08s
         4           0.7053            0.07s
         5           0.6234            0.06s
         6           0.5528            0.04s
         7           0.4915            0.03s
         8           0.4380            0.02s
         9           0.3910            0.01s
        10           0.3496            0.00s
### Start Time 2020/01/03-08-30-36  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=573   randForSplit=864   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1a5cb758>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=573,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0857	suffici
+0.0712	exampl
+0.0654	signific differ
+0.0529	mice mice
+0.0188	faster
+0.0102	signific increas
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427
+0.0000	a427 h358
+0.0000	a68
+0.0000	a68 mm

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/01/03-08-30-43. Total      6.85 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Fri Jan  3 08:30:49 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0254            0.11s
         2           0.8899            0.09s
         3           0.7783            0.08s
         4           0.6845            0.07s
         5           0.6046            0.06s
         6           0.5359            0.05s
         7           0.4763            0.03s
         8           0.4243            0.02s
         9           0.3787            0.01s
        10           0.3386            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0254            0.22s
         2           0.8899            0.19s
         3           0.7783            0.19s
         4           0.6845            0.18s
         5           0.6046            0.17s
         6           0.5359            0.15s
         7           0.4763            0.14s
         8           0.4243            0.13s
         9           0.3787            0.12s
        10           0.3386            0.11s
        20           0.1166            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0254            0.11s
         2           0.8899            0.10s
         3           0.7783            0.08s
         4           0.6845            0.07s
         5           0.6046            0.06s
         6           0.5359            0.04s
         7           0.4763            0.03s
         8           0.4243            0.02s
         9           0.3787            0.01s
        10           0.3386            0.00s
Not skipping fit 0 26
      Iter       Train Loss   Remaining Time 
         1           1.0254            0.21s
         2           0.8899            0.19s
         3           0.7783            0.19s
         4           0.6845            0.18s
         5           0.6046            0.17s
         6           0.5359            0.16s
         7           0.4763            0.14s
         8           0.4243            0.13s
         9           0.3787            0.12s
        10           0.3386            0.11s
        20           0.1166            0.00s
Not skipping fit 0 29
      Iter       Train Loss   Remaining Time 
         1           1.0726            0.10s
         2           0.9302            0.09s
         3           0.8129            0.08s
         4           0.7145            0.07s
         5           0.6308            0.06s
         6           0.5588            0.05s
         7           0.4964            0.03s
         8           0.4421            0.02s
         9           0.3944            0.01s
        10           0.3525            0.00s
Not skipping fit 29 26
      Iter       Train Loss   Remaining Time 
         1           1.0254            0.10s
         2           0.8899            0.09s
         3           0.7783            0.07s
         4           0.6845            0.07s
         5           0.6046            0.05s
         6           0.5359            0.05s
         7           0.4763            0.03s
         8           0.4243            0.02s
         9           0.3787            0.01s
        10           0.3386            0.00s
### Start Time 2020/01/03-15-32-02  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=210   randForSplit=322   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['keep', 'discard']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['keep', 'discard']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['keep', 'discard']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1db4e3b0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=210,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0655	repres imag
+0.0529	increas level
+0.0428	term
+0.0348	ventral
+0.0283	cell_lin indic
+0.0230	mice mice
+0.0188	faster
+0.0153	signific differ
+0.0125	cell exampl
+0.0102	versus
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/01/03-15-32-09. Total      6.80 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Fri Jan  3 15:32:18 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.10s
         2           0.8350            0.08s
         3           0.7241            0.07s
         4           0.6331            0.06s
         5           0.5570            0.05s
         6           0.4921            0.04s
         7           0.4364            0.03s
         8           0.3881            0.02s
         9           0.3459            0.01s
        10           0.3089            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.20s
         2           0.8350            0.19s
         3           0.7241            0.18s
         4           0.6331            0.17s
         5           0.5570            0.15s
         6           0.4921            0.14s
         7           0.4364            0.13s
         8           0.3881            0.12s
         9           0.3459            0.11s
        10           0.3089            0.10s
        20           0.1059            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.10s
         2           0.8350            0.08s
         3           0.7241            0.07s
         4           0.6331            0.06s
         5           0.5570            0.05s
         6           0.4921            0.04s
         7           0.4364            0.03s
         8           0.3881            0.02s
         9           0.3459            0.01s
        10           0.3089            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.20s
         2           0.8350            0.19s
         3           0.7241            0.17s
         4           0.6331            0.16s
         5           0.5570            0.15s
         6           0.4921            0.14s
         7           0.4364            0.13s
         8           0.3881            0.12s
         9           0.3459            0.11s
        10           0.3089            0.10s
        20           0.1059            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.10s
         2           0.9030            0.09s
         3           0.7855            0.08s
         4           0.6882            0.06s
         5           0.6061            0.05s
         6           0.5360            0.04s
         7           0.4756            0.03s
         8           0.4231            0.02s
         9           0.3772            0.01s
        10           0.3369            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.11s
         2           0.8350            0.09s
         3           0.7241            0.08s
         4           0.6331            0.06s
         5           0.5570            0.05s
         6           0.4921            0.04s
         7           0.4364            0.03s
         8           0.3881            0.02s
         9           0.3459            0.01s
        10           0.3089            0.00s
### Start Time 2020/01/03-15-58-53  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=519   randForSplit=474   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['keep', 'discard']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['keep', 'discard']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['keep', 'discard']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=519,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0663	cell_lin indic
+0.0531	signific differ
+0.0498	figur differ
+0.0428	suffici
+0.0281	cre
+0.0229	metabol
+0.0186	versus
+0.0124	restor
+0.0101	control figur
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/01/03-15-58-59. Total      6.18 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Fri Jan  3 15:59:07 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.10s
         2           0.8350            0.09s
         3           0.7241            0.07s
         4           0.6331            0.06s
         5           0.5570            0.05s
         6           0.4921            0.04s
         7           0.4364            0.03s
         8           0.3881            0.02s
         9           0.3459            0.01s
        10           0.3089            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.21s
         2           0.8350            0.19s
         3           0.7241            0.17s
         4           0.6331            0.16s
         5           0.5570            0.15s
         6           0.4921            0.14s
         7           0.4364            0.13s
         8           0.3881            0.12s
         9           0.3459            0.11s
        10           0.3089            0.10s
        20           0.1059            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.10s
         2           0.8350            0.08s
         3           0.7241            0.07s
         4           0.6331            0.06s
         5           0.5570            0.05s
         6           0.4921            0.04s
         7           0.4364            0.03s
         8           0.3881            0.02s
         9           0.3459            0.01s
        10           0.3089            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.21s
         2           0.8350            0.19s
         3           0.7241            0.17s
         4           0.6331            0.16s
         5           0.5570            0.15s
         6           0.4921            0.14s
         7           0.4364            0.14s
         8           0.3881            0.13s
         9           0.3459            0.12s
        10           0.3089            0.11s
        20           0.1059            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.10s
         2           0.9030            0.09s
         3           0.7855            0.08s
         4           0.6882            0.07s
         5           0.6061            0.06s
         6           0.5360            0.05s
         7           0.4756            0.03s
         8           0.4231            0.02s
         9           0.3772            0.01s
        10           0.3369            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.10s
         2           0.8350            0.08s
         3           0.7241            0.07s
         4           0.6331            0.06s
         5           0.5570            0.05s
         6           0.4921            0.04s
         7           0.4364            0.03s
         8           0.3881            0.02s
         9           0.3459            0.01s
        10           0.3089            0.00s
### Start Time 2020/03/04-13-54-50  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=832   randForSplit=455   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['keep', 'discard']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['keep', 'discard']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['keep', 'discard']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=10,
              n_iter_no_change=None, presort='auto', random_state=832,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__n_estimators': 10, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'classifier__n_estimators': 10, 'classifier__max_depth': 4}
mean_test_score:  1.000000
{'classifier__n_estimators': 20, 'classifier__max_depth': 4}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0663	histogram
+0.0531	mice did
+0.0428	suffici
+0.0347	remark
+0.0281	environment
+0.0229	increas level
+0.0186	term
+0.0152	group level
+0.0124	govern
+0.0101	decreas number
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zpf57
+0.0000	zpf57 li
+0.0000	zygos
+0.0000	zygos crispr
+0.0000	zymo
+0.0000	zymo research

### Vectorizer:   Number of Features: 40115
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'knock_out microglia', u'knock_out mix', u'knock_out model', u'knock_out moi', u'knock_out mutant', u'knock_out mwt', u'knock_out nppa', u'knock_out onli', u'knock_out phenotyp', u'knock_out popul']

Last 10 features: [u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zpf57', u'zpf57 li', u'zygos', u'zygos crispr', u'zymo', u'zymo research']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2020/03/04-13-54-56. Total      5.98 seconds

Recall for papers selected by each curation group. 1 papers analyzed
ap             selected papers:     0 predicted keep:     0 recall: 0.000
gxd            selected papers:     0 predicted keep:     0 recall: 0.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     0 predicted keep:     0 recall: 0.000
Predictions from GB_test_pred.txt - Wed Mar  4 13:55:00 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.32s
         2           0.9527            0.25s
         3           0.8305            0.21s
         4           0.7287            0.17s
         5           0.6423            0.14s
         6           0.5684            0.11s
         7           0.5045            0.08s
         8           0.4489            0.05s
         9           0.4003            0.03s
        10           0.3576            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.52s
         2           0.9527            0.47s
         3           0.8305            0.44s
         4           0.7287            0.41s
         5           0.6423            0.38s
         6           0.5684            0.35s
         7           0.5045            0.33s
         8           0.4489            0.30s
         9           0.4003            0.28s
        10           0.3576            0.26s
        20           0.1226            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.24s
         2           0.9527            0.21s
         3           0.8305            0.18s
         4           0.7287            0.15s
         5           0.6423            0.13s
         6           0.5684            0.11s
         7           0.5045            0.08s
         8           0.4489            0.05s
         9           0.4003            0.03s
        10           0.3576            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.51s
         2           0.9527            0.46s
         3           0.8305            0.45s
         4           0.7287            0.41s
         5           0.6423            0.39s
         6           0.5684            0.36s
         7           0.5045            0.33s
         8           0.4489            0.31s
         9           0.4003            0.28s
        10           0.3576            0.25s
        20           0.1226            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.26s
         2           0.9507            0.22s
         3           0.8287            0.20s
         4           0.7270            0.17s
         5           0.6409            0.14s
         6           0.5671            0.11s
         7           0.5034            0.08s
         8           0.4479            0.06s
         9           0.3994            0.03s
        10           0.3568            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.24s
         2           0.9527            0.21s
         3           0.8305            0.18s
         4           0.7287            0.15s
         5           0.6423            0.13s
         6           0.5684            0.10s
         7           0.5045            0.08s
         8           0.4489            0.05s
         9           0.4003            0.03s
        10           0.3576            0.00s
Recall for papers selected by each curation group. 4 papers analyzed
ap             selected papers:     2 predicted keep:     2 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     1 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     3 predicted keep:     2 recall: 0.667
Predictions from GB_test_pred.txt - Mon Mar 23 10:36:05 2020
Fitting 1 folds for each of 4 candidates, totalling 4 fits
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.25s
         2           0.9527            0.21s
         3           0.8305            0.19s
         4           0.7287            0.16s
         5           0.6423            0.13s
         6           0.5684            0.10s
         7           0.5045            0.08s
         8           0.4489            0.05s
         9           0.4003            0.03s
        10           0.3576            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.52s
         2           0.9527            0.47s
         3           0.8305            0.44s
         4           0.7287            0.41s
         5           0.6423            0.38s
         6           0.5684            0.36s
         7           0.5045            0.33s
         8           0.4489            0.31s
         9           0.4003            0.28s
        10           0.3576            0.25s
        20           0.1226            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.26s
         2           0.9527            0.21s
         3           0.8305            0.18s
         4           0.7287            0.16s
         5           0.6423            0.13s
         6           0.5684            0.10s
         7           0.5045            0.08s
         8           0.4489            0.05s
         9           0.4003            0.03s
        10           0.3576            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.51s
         2           0.9527            0.46s
         3           0.8305            0.43s
         4           0.7287            0.41s
         5           0.6423            0.38s
         6           0.5684            0.36s
         7           0.5045            0.33s
         8           0.4489            0.31s
         9           0.4003            0.28s
        10           0.3576            0.25s
        20           0.1226            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.26s
         2           0.9507            0.22s
         3           0.8287            0.19s
         4           0.7270            0.16s
         5           0.6409            0.13s
         6           0.5671            0.11s
         7           0.5034            0.08s
         8           0.4479            0.06s
         9           0.3994            0.03s
        10           0.3568            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1023            0.24s
         2           0.9527            0.21s
         3           0.8305            0.18s
         4           0.7287            0.16s
         5           0.6423            0.13s
         6           0.5684            0.11s
         7           0.5045            0.08s
         8           0.4489            0.05s
         9           0.4003            0.03s
        10           0.3576            0.00s
### Start Time 2020/03/23-10-38-34  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage3/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage3/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage3/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=361   randForSplit=885   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        15
Train discard       1.00      1.00      1.00         8

     accuracy                           1.00        23
    macro avg       1.00      1.00      1.00        23
 weighted avg       1.00      1.00      1.00        23

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['keep', 'discard']
[[15  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.50      0.50      0.50         2
Valid discard       0.00      0.00      0.00         1

     accuracy                           0.33         3
    macro avg       0.25      0.25      0.25         3
 weighted avg       0.33      0.33      0.33         3

Valid (keep) F2: 0.5000    P: 0.5000    R: 0.5000    NPV: 0.0000

['keep', 'discard']
[[1 1]
 [1 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      0.67      0.80         3
Test  discard       0.50      1.00      0.67         1

     accuracy                           0.75         4
    macro avg       0.75      0.83      0.73         4
 weighted avg       0.88      0.75      0.77         4

Test  (keep) F2: 0.7143    P: 1.0000    R: 0.6667    NPV: 0.5000

['keep', 'discard']
[[2 1]
 [0 1]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 10

vectorizer:
CountVectorizer(analyzer='word', binary=True, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
                ngram_range=(1, 2), preprocessor=None, stop_words='english',
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)

classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=10,
                           n_iter_no_change=None, presort='auto',
                           random_state=361, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=1,
                           warm_start=False)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 4]
classifier__n_estimators:[10, 20]

### Grid Search Scores:
{'classifier__max_depth': 3, 'classifier__n_estimators': 10}
mean_test_score:  0.500000
{'classifier__max_depth': 3, 'classifier__n_estimators': 20}
mean_test_score:  0.500000
{'classifier__max_depth': 4, 'classifier__n_estimators': 10}
mean_test_score:  0.500000
{'classifier__max_depth': 4, 'classifier__n_estimators': 20}
mean_test_score:  0.500000

### Grid Search Best Score: 0.500000

### Feature weights: highest 20
+0.6618	bar
+0.0729	framework
+0.0588	spot form
+0.0477	cell_lin process
+0.0387	immunodefici
+0.0315	discoveri
+0.0256	temperatur plate
+0.0209	diphtheria
+0.0170	secondari structur
+0.0139	plate wash
+0.0113	overnight cell_lin
+0.0000	a004697
+0.0000	a004697 spiegel
+0.0000	a004754
+0.0000	a004754 yokoyama
+0.0000	a023473
+0.0000	a023473 maynard
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect

### Feature weights: lowest 20
+0.0000	zou smith
+0.0000	zou watanab
+0.0000	zou zhai
+0.0000	zoubouli
+0.0000	zoubouli alopecia
+0.0000	zoubouli cc
+0.0000	zoumpoulidou
+0.0000	zoumpoulidou sibyll
+0.0000	zqn
+0.0000	zqn natur
+0.0000	zw
+0.0000	zw jimenez
+0.0000	zwirner
+0.0000	zwirner poirier
+0.0000	zwirner toscano
+0.0000	zygos
+0.0000	zygos adamts13
+0.0000	zygos crispr
+0.0000	zymogen
+0.0000	zymogen form

### Vectorizer:   Number of Features: 89179
First 10 features: ['a004697', 'a004697 spiegel', 'a004754', 'a004754 yokoyama', 'a023473', 'a023473 maynard', 'a10', 'a10 relat', 'a10 respect', 'a10520']

Middle 10 features: ['lane cultur', 'lane dynll1', 'lane error', 'lane express', 'lane figur', 'lane howev', 'lane itch', 'lane ki', 'lane lane', 'lane lc']

Last 10 features: ['zw', 'zw jimenez', 'zwirner', 'zwirner poirier', 'zwirner toscano', 'zygos', 'zygos adamts13', 'zygos crispr', 'zymogen', 'zymogen form']

### False positives for Validation set: 1
30321399

### False negatives for Validation set: 1
25882312

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           23           15            8          65%
Validation Set      :            3            2            1          67%
Test Set            :            4            3            1          75%
ValidationSplit: 0.20
### End Time 2020/03/23-10-38-41. Total      6.84 seconds

Recall for papers selected by each curation group. 4 papers analyzed
ap             selected papers:     2 predicted keep:     2 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     1 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     3 predicted keep:     2 recall: 0.667
Predictions from GB_test_pred.txt - Mon Mar 23 10:39:02 2020
      Iter       Train Loss      OOB Improve   Remaining Time 
         1           1.3179          -0.0009           20.68s
         2           1.3177          -0.0009           12.47s
         3           1.3808          -0.0110            9.63s
         4           1.2530          -0.0024            8.43s
         5           1.3173          -0.0008            7.59s
         6           1.3794          -0.0107            7.04s
         7           1.3776          -0.0102            6.63s
         8           1.3760          -0.0097            6.26s
         9           1.3164          -0.0003            5.98s
        10           1.2560          -0.0028            5.73s
        20           1.2526          -0.0024            4.86s
        30           1.3180          -0.0009            4.40s
        40           1.3186          -0.0010            4.18s
        50           1.3170          -0.0007            4.01s
        60           1.3773          -0.0101            3.89s
        70           1.2547          -0.0027            3.80s
        80           1.2526          -0.0024            3.74s
        90           1.2514          -0.0022            3.68s
       100           1.3854          -0.0121            3.69s
       200           1.3205          -0.0013            3.34s
       300           1.3790          -0.0106            3.20s
       400           1.1802          -0.0146            2.99s
       500           1.3852          -0.0120            2.75s
       600           1.2504          -0.0019            2.46s
       700           1.2500          -0.0018            2.19s
       800           1.1069          -0.0389            1.93s
       900           1.1827          -0.0151            1.69s
      1000           1.2519          -0.0023            1.44s
      Iter       Train Loss      OOB Improve   Remaining Time 
         1           1.3122          -0.0007            7.95s
         2           1.3121          -0.0007            5.97s
         3           1.2545          -0.0020            5.44s
         4           1.3121          -0.0007            5.12s
         5           1.3120          -0.0007            4.89s
         6           1.2546          -0.0020            4.73s
         7           1.2542          -0.0019            4.64s
         8           1.3689          -0.0098            4.56s
         9           1.1957          -0.0134            4.50s
        10           1.3124          -0.0008            4.48s
        20           1.3132          -0.0010            4.40s
        30           1.3128          -0.0009            4.46s
        40           1.2551          -0.0021            4.32s
        50           1.2541          -0.0019            4.23s
        60           1.2526          -0.0014            4.18s
        70           1.2522          -0.0013            4.12s
        80           1.2516          -0.0009            4.08s
        90           1.2515          -0.0008            4.05s
       100           1.3731          -0.0109            4.05s
       200           1.1913          -0.0125            3.74s
       300           1.2564          -0.0023            3.50s
       400           1.1275          -0.0334            3.23s
       500           1.2047          -0.0151            2.94s
       600           1.3148          -0.0012            2.67s
       700           1.3117          -0.0006            2.40s
       800           1.2528          -0.0015            2.14s
       900           1.2535          -0.0017            1.86s
      1000           1.3114          -0.0005            1.60s
### Start Time 2020/04/17-11-20-59  GB.py	index file: index.out
Training data path:   ./data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: ./data/small/LegendsWords/Proc1/valSet.txt
Test data path:       ./data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=699   randForSplit=853   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       0.65      1.00      0.79        15
Train discard       0.00      0.00      0.00         8

     accuracy                           0.65        23
    macro avg       0.33      0.50      0.39        23
 weighted avg       0.43      0.65      0.51        23

Train (keep) F2: 0.9036    P: 0.6522    R: 1.0000    NPV: 0.0000

['keep', 'discard']
[[15  0]
 [ 8  0]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.67      1.00      0.80         2
Valid discard       0.00      0.00      0.00         1

     accuracy                           0.67         3
    macro avg       0.33      0.50      0.40         3
 weighted avg       0.44      0.67      0.53         3

Valid (keep) F2: 0.9091    P: 0.6667    R: 1.0000    NPV: 0.0000

['keep', 'discard']
[[2 0]
 [1 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.75      1.00      0.86         3
Test  discard       0.00      0.00      0.00         1

     accuracy                           0.75         4
    macro avg       0.38      0.50      0.43         4
 weighted avg       0.56      0.75      0.64         4

Test  (keep) F2: 0.9375    P: 0.7500    R: 1.0000    NPV: 0.0000

['keep', 'discard']
[[3 0]
 [1 0]]

### Note: blessed GB.

### Best Pipeline Parameters:

vectorizer:
CountVectorizer(analyzer='word', binary=True, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
                ngram_range=(1, 2), preprocessor=None, stop_words='english',
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)

classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='deviance', max_depth=3,
                           max_features=0.7, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=150, min_samples_split=600,
                           min_weight_fraction_leaf=0.0, n_estimators=1600,
                           n_iter_no_change=None, presort='auto',
                           random_state=699, subsample=0.85, tol=0.0001,
                           validation_fraction=0.1, verbose=1,
                           warm_start=False)


### Feature weights: highest 20
+0.0000	a004697
+0.0000	a004697 spiegel
+0.0000	a004754
+0.0000	a004754 yokoyama
+0.0000	a023473
+0.0000	a023473 maynard
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a10520
+0.0000	a10520 life
+0.0000	a11008
+0.0000	a11008 rabbit
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a11039
+0.0000	a11039 life
+0.0000	a1111111111
+0.0000	a1111111111 a1111111111
+0.0000	a1111111111 section

### Feature weights: lowest 20
+0.0000	zou smith
+0.0000	zou watanab
+0.0000	zou zhai
+0.0000	zoubouli
+0.0000	zoubouli alopecia
+0.0000	zoubouli cc
+0.0000	zoumpoulidou
+0.0000	zoumpoulidou sibyll
+0.0000	zqn
+0.0000	zqn natur
+0.0000	zw
+0.0000	zw jimenez
+0.0000	zwirner
+0.0000	zwirner poirier
+0.0000	zwirner toscano
+0.0000	zygos
+0.0000	zygos adamts13
+0.0000	zygos crispr
+0.0000	zymogen
+0.0000	zymogen form

### Vectorizer:   Number of Features: 89179
First 10 features: ['a004697', 'a004697 spiegel', 'a004754', 'a004754 yokoyama', 'a023473', 'a023473 maynard', 'a10', 'a10 relat', 'a10 respect', 'a10520']

Middle 10 features: ['lane cultur', 'lane dynll1', 'lane error', 'lane express', 'lane figur', 'lane howev', 'lane itch', 'lane ki', 'lane lane', 'lane lc']

Last 10 features: ['zw', 'zw jimenez', 'zwirner', 'zwirner poirier', 'zwirner toscano', 'zygos', 'zygos adamts13', 'zygos crispr', 'zymogen', 'zymogen form']

### False positives for Validation set: 1
30321399

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           23           15            8          65%
Validation Set      :            3            2            1          67%
Test Set            :            4            3            1          75%
ValidationSplit: 0.20
### End Time 2020/04/17-11-21-10. Total     11.21 seconds

Recall for papers selected by each curation group. 4 papers analyzed
ap             selected papers:     2 predicted keep:     2 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     1 predicted keep:     1 recall: 1.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     3 predicted keep:     3 recall: 1.000
Predictions from GB_test_pred.txt - Fri Apr 17 11:21:16 2020
      Iter       Train Loss      OOB Improve   Remaining Time 
         1           1.2515          -0.0022            7.60s
         2           1.3829          -0.0115            5.73s
         3           1.3176          -0.0008            4.98s
         4           1.3174          -0.0008            4.62s
         5           1.3173          -0.0007            4.38s
         6           1.3793          -0.0106            4.23s
         7           1.2538          -0.0025            4.13s
         8           1.3785          -0.0104            4.04s
         9           1.2543          -0.0026            3.98s
        10           1.3169          -0.0006            3.93s
        20           1.3170          -0.0006            3.70s
        30           1.3169          -0.0006            3.63s
        40           1.1951          -0.0170            3.63s
        50           1.1930          -0.0167            3.59s
        60           1.2525          -0.0023            3.59s
        70           1.3209          -0.0013            3.56s
        80           1.3215          -0.0014            3.52s
        90           1.3885          -0.0127            3.53s
       100           1.3184          -0.0010            3.49s
       200           1.2495          -0.0017            3.30s
       300           1.3195          -0.0012            3.03s
       400           1.1718          -0.0128            2.80s
       500           1.3747          -0.0093            2.55s
       600           1.2516          -0.0022            2.32s
       700           1.2497          -0.0017            2.10s
       800           1.3197          -0.0012            1.87s
       900           1.2539          -0.0026            1.64s
      1000           1.2501          -0.0019            1.40s
### Start Time 2020/04/17-11-30-05  GB.py	index file: index.out
Training data path:   ./data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: ./data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=392   randForSplit=384   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       0.65      1.00      0.79        15
Train discard       0.00      0.00      0.00         8

     accuracy                           0.65        23
    macro avg       0.33      0.50      0.39        23
 weighted avg       0.43      0.65      0.51        23

Train (keep) F2: 0.9036    P: 0.6522    R: 1.0000    NPV: 0.0000

['keep', 'discard']
[[15  0]
 [ 8  0]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.67      1.00      0.80         2
Valid discard       0.00      0.00      0.00         1

     accuracy                           0.67         3
    macro avg       0.33      0.50      0.40         3
 weighted avg       0.44      0.67      0.53         3

Valid (keep) F2: 0.9091    P: 0.6667    R: 1.0000    NPV: 0.0000

['keep', 'discard']
[[2 0]
 [1 0]]

### Note: blessed GB.

### Best Pipeline Parameters:

vectorizer:
CountVectorizer(analyzer='word', binary=True, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
                ngram_range=(1, 2), preprocessor=None, stop_words='english',
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)

classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='deviance', max_depth=3,
                           max_features=0.7, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=150, min_samples_split=600,
                           min_weight_fraction_leaf=0.0, n_estimators=1600,
                           n_iter_no_change=None, presort='auto',
                           random_state=392, subsample=0.85, tol=0.0001,
                           validation_fraction=0.1, verbose=1,
                           warm_start=False)


### End Time 2020/04/17-11-30-09. Total      4.68 seconds

Recall for papers selected by each curation group. 4 papers analyzed
ap             selected papers:     2 predicted keep:     2 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     1 predicted keep:     1 recall: 1.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     3 predicted keep:     3 recall: 1.000
Predictions from GB_test_pred.txt - Fri Apr 17 11:35:05 2020
      Iter       Train Loss      OOB Improve   Remaining Time 
         1           1.3179          -0.0009            7.49s
         2           1.3177          -0.0009            5.53s
         3           1.2520          -0.0023            5.04s
         4           1.3178          -0.0009            4.77s
         5           1.2518          -0.0022            4.58s
         6           1.2514          -0.0021            4.42s
         7           1.2510          -0.0021            4.31s
         8           1.2506          -0.0020            4.22s
         9           1.1794          -0.0145            4.15s
        10           1.2495          -0.0017            4.08s
        20           1.3805          -0.0109            3.86s
        30           1.2508          -0.0020            3.84s
        40           1.1787          -0.0143            3.75s
        50           1.3183          -0.0010            3.66s
        60           1.2502          -0.0019            3.63s
        70           1.1795          -0.0145            3.60s
        80           1.2505          -0.0020            3.58s
        90           1.3217          -0.0014            3.54s
       100           1.3192          -0.0011            3.56s
       200           1.2531          -0.0024            3.28s
       300           1.2512          -0.0021            2.97s
       400           1.3177          -0.0008            2.69s
       500           1.3805          -0.0109            2.45s
       600           1.1817          -0.0149            2.23s
       700           1.2535          -0.0025            2.01s
       800           1.2500          -0.0018            1.79s
       900           1.3168          -0.0006            1.57s
      1000           1.1796          -0.0145            1.35s
### Start Time 2020/04/17-11-35-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage3/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage3/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=599   randForSplit=703   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       0.65      1.00      0.79        15
Train discard       0.00      0.00      0.00         8

     accuracy                           0.65        23
    macro avg       0.33      0.50      0.39        23
 weighted avg       0.43      0.65      0.51        23

Train (keep) F2: 0.9036    P: 0.6522    R: 1.0000    NPV: 0.0000

['keep', 'discard']
[[15  0]
 [ 8  0]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.67      1.00      0.80         2
Valid discard       0.00      0.00      0.00         1

     accuracy                           0.67         3
    macro avg       0.33      0.50      0.40         3
 weighted avg       0.44      0.67      0.53         3

Valid (keep) F2: 0.9091    P: 0.6667    R: 1.0000    NPV: 0.0000

['keep', 'discard']
[[2 0]
 [1 0]]

### Note: blessed GB.

### Best Pipeline Parameters:

vectorizer:
CountVectorizer(analyzer='word', binary=True, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
                ngram_range=(1, 2), preprocessor=None, stop_words='english',
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)

classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='deviance', max_depth=3,
                           max_features=0.7, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=150, min_samples_split=600,
                           min_weight_fraction_leaf=0.0, n_estimators=1600,
                           n_iter_no_change=None, presort='auto',
                           random_state=599, subsample=0.85, tol=0.0001,
                           validation_fraction=0.1, verbose=1,
                           warm_start=False)


### End Time 2020/04/17-11-35-17. Total      4.56 seconds

Recall for papers selected by each curation group. 4 papers analyzed
ap             selected papers:     2 predicted keep:     2 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     1 predicted keep:     1 recall: 1.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     3 predicted keep:     3 recall: 1.000
Predictions from GB_test_pred.txt - Fri Apr 17 11:35:50 2020
