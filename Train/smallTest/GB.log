Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.06s
         2           0.9507            0.04s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.08s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-08-26-42  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=388   randForSplit=497   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=388,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	faster
+0.0618	cre
+0.0500	metabol
+0.0453	ani chang
+0.0406	cell_lin indic
+0.0365	cyclin bound
+0.0330	bm
+0.0295	analys shown
+0.0239	strong accumul
+0.0195	panel figur
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-08-26-45. Total      2.85 seconds

Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-08-50-26  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=904   randForSplit=555   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=904,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	histogram
+0.0618	govern
+0.0500	deliveri
+0.0453	analyz ponceau
+0.0406	puls
+0.0365	t487 figur
+0.0330	cell exampl
+0.0295	use synchron
+0.0239	roscovitin confirm
+0.0195	lysat untreat
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-08-50-29. Total      2.85 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 08:50:36 2019
Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:00:17 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.06s
         2           0.9507            0.04s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-09-01-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=926   randForSplit=869   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=926,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	cre
+0.0618	cage
+0.0500	figur signific
+0.0453	ad short
+0.0406	decreas number
+0.0365	myc plk1
+0.0330	restor
+0.0295	phosphoryl endogen
+0.0239	abolish plk1
+0.0195	endogen bmyb
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-09-01-14. Total      1.64 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:06:31 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-09-18-40  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=816   randForSplit=48   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=816,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	histogram
+0.0618	deliveri
+0.0500	restor
+0.0453	experi gain
+0.0406	exampl
+0.0365	t487 control
+0.0330	mice signific
+0.0295	wb cdk2
+0.0239	advantag substrat
+0.0195	proteolysi experi
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-09-18-42. Total      1.87 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:19:09 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-09-19-12  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=351   randForSplit=247   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=351,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	control figur
+0.0618	signific differ
+0.0500	figur differ
+0.0453	phosphoryl surpris
+0.0406	metabol
+0.0365	transfect myb
+0.0330	figur indic
+0.0295	resist permit
+0.0239	protein complex
+0.0195	terminus figur
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-09-19-15. Total      3.44 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:19:59 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.04s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
### Start Time 2019/10/09-09-20-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=392   randForSplit=744   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=392,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]

### Feature weights: highest 20
+0.5834	gene
+0.0767	mice mice
+0.0618	cre
+0.0500	dendrit
+0.0453	abil pin1
+0.0406	environment
+0.0365	extrem terminus
+0.0330	mice did
+0.0295	experi antibodi
+0.0239	tad follow
+0.0195	subdivid activ
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a11037
+0.0000	a11037 describ
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 42133
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a11037', u'a11037 describ', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427']

Middle 10 features: [u'l552e mutat', u'l552e similar', u'l552e singl', u'l552e substitut', u'l552e t18', u'l552e unfold', u'l552e variant', u'l552e wherea', u'l697', u'l697 alanin']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Test set: 0

### False negatives for Test set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Validation Set      :            2            2            0         100%
Training Set        :           26           17            9          65%
Test Set            :            2            1            1          50%
TestSplit: 0.20
### End Time 2019/10/09-09-20-05. Total      1.78 seconds

Recall for papers selected by each curation group. 2 papers analyzed
ap             selected papers:     1 predicted keep:     1 recall: 1.000
gxd            selected papers:     1 predicted keep:     1 recall: 1.000
go             selected papers:     0 predicted keep:     0 recall: 0.000
tumor          selected papers:     0 predicted keep:     0 recall: 0.000
qtl            selected papers:     0 predicted keep:     0 recall: 0.000
Totals         keep     papers:     1 predicted keep:     1 recall: 1.000
Predictions from GB_test_pred.txt - Wed Oct  9 09:20:23 2019
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.07s
         2           0.9189            0.05s
         3           0.7999            0.04s
         4           0.7011            0.02s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.05s
         2           0.9507            0.04s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-09-22  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=357   randForSplit=443   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=357,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]

### Feature weights: highest 20
+0.6176	mice figur
+0.3231	yellow
+0.0593	figur indic
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427
+0.0000	a427 h358
+0.0000	a68
+0.0000	a68 mm
+0.0000	a7c11
+0.0000	a7c11 breast
+0.0000	a7c11 brpkp110
+0.0000	a7c11 cell
+0.0000	a7c11 cell_lin
+0.0000	a7c11 growth

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 39600
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427', u'a427 h358', u'a68']

Middle 10 features: [u'lack plk1', u'lack secondari', u'lack t476', u'lack termin', u'lack therefor', u'lactat', u'lactat bnip3', u'lactat output', u'lactat raav', u'lactat triglycerid']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/16-15-09-24. Total      2.10 seconds

Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.06s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.06s
         2           0.9189            0.05s
         3           0.7999            0.03s
         4           0.7011            0.02s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.04s
         2           0.9507            0.03s
         3           0.8287            0.02s
         4           0.7270            0.01s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-14-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=780   randForSplit=902   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=780,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]

### Feature weights: highest 20
+0.6176	mice figur
+0.2224	yellow
+0.1599	figur indic
+0.0000	a10
+0.0000	a10 relat
+0.0000	a10 respect
+0.0000	a1978
+0.0000	a1978 sigma
+0.0000	a1express
+0.0000	a1express basement
+0.0000	a427
+0.0000	a427 h358
+0.0000	a68
+0.0000	a68 mm
+0.0000	a7c11
+0.0000	a7c11 breast
+0.0000	a7c11 brpkp110
+0.0000	a7c11 cell
+0.0000	a7c11 cell_lin
+0.0000	a7c11 growth

### Feature weights: lowest 20
+0.0000	zhang
+0.0000	zhang et
+0.0000	zhang help
+0.0000	zip
+0.0000	zip domain
+0.0000	zip flag
+0.0000	zn
+0.0000	zn finger
+0.0000	znf822
+0.0000	znf822 synerg
+0.0000	znpp
+0.0000	znpp addit
+0.0000	znpp cell
+0.0000	znpp decreas
+0.0000	znpp impair
+0.0000	znpp prevent
+0.0000	zone
+0.0000	zone cell_lin
+0.0000	zygos
+0.0000	zygos crispr

### Vectorizer:   Number of Features: 39600
First 10 features: [u'a10', u'a10 relat', u'a10 respect', u'a1978', u'a1978 sigma', u'a1express', u'a1express basement', u'a427', u'a427 h358', u'a68']

Middle 10 features: [u'lack plk1', u'lack secondari', u'lack t476', u'lack termin', u'lack therefor', u'lactat', u'lactat bnip3', u'lactat output', u'lactat raav', u'lactat triglycerid']

Last 10 features: [u'znpp', u'znpp addit', u'znpp cell', u'znpp decreas', u'znpp impair', u'znpp prevent', u'zone', u'zone cell_lin', u'zygos', u'zygos crispr']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/16-15-14-15. Total      1.99 seconds

Fitting 1 folds for each of 1 candidates, totalling 1 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-16-37  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=423   randForSplit=500   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=423,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	exampl
+0.0641	faster
+0.0519	densiti
+0.0466	pretreat
+0.0421	bm
+0.0376	employ
+0.0343	term
+0.0304	wce
+0.0247	besid
+0.0201	coprecipit
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/16-15-16-38. Total      0.77 seconds

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-18-10  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=209   randForSplit=178   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=209,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	restor
+0.0770	ro
+0.0641	modul
+0.0519	densiti
+0.0421	metabol
+0.0376	star
+0.0343	insert
+0.0247	aliquot
+0.0201	coloni
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/16-15-18-11. Total      0.94 seconds

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.02s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/16-15-21-46  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=303   randForSplit=564   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=303,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.1137	govern
+0.0641	bm
+0.0519	dendrit
+0.0466	wb
+0.0421	helper
+0.0376	prove
+0.0304	pbd
+0.0247	e2f5
+0.0201	glutathionesepharos
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/16-15-21-47. Total      0.86 seconds

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.02s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/17-11-43-31  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=409   randForSplit=265   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=409, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	govern
+0.0641	faster
+0.0519	insert
+0.0466	crystal
+0.0421	environment
+0.0376	alkalin
+0.0343	bm
+0.0304	pretreat
+0.0247	t476
+0.0201	judg
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/17-11-43-32. Total      0.87 seconds

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
*** After GS Fit

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
*** After trainset Fit

### Start Time 2019/10/17-11-49-03  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=886   randForSplit=889   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=886, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.1137	term
+0.0641	modul
+0.0519	dendrit
+0.0466	mimosin
+0.0421	densiti
+0.0376	nonmitot
+0.0304	r695
+0.0247	preferenti
+0.0201	multiploid
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/17-11-49-04. Total      0.82 seconds

in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.00s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.0524            0.00s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.00s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.0320            0.01s
         2           0.8873            0.00s
         3           0.7712            0.00s
         4           0.6754            0.00s
         5           0.5947            0.00s
*** After GS Fit

in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.1918            0.02s
         2           1.0365            0.01s
         3           0.9071            0.01s
         4           0.7977            0.00s
         5           0.7044            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1928            0.01s
         2           1.0374            0.00s
         3           0.9079            0.00s
         4           0.7985            0.00s
         5           0.7051            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1863            0.01s
         2           1.0313            0.01s
         3           0.9023            0.00s
         4           0.7934            0.00s
         5           0.7005            0.00s
*** After GS Fit

()
<type 'numpy.ndarray'>
<type 'numpy.ndarray'>
()
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.01s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
*** After GS Fit


<type 'numpy.ndarray'>
1
<type 'numpy.ndarray'>
1

in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
*** After GS Fit


<type 'numpy.ndarray'>
1
<type 'numpy.ndarray'>
1

y
<type 'numpy.ndarray'>
1
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-08-19-03  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=872   randForSplit=356   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.75      0.86         4
Valid discard       0.67      1.00      0.80         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.83      0.88      0.83         6
 weighted avg       0.89      0.83      0.84         6

Valid F2: 0.78947 (keep)

['yes', 'no']
[[3 1]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=872, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6087	gene
+0.0593	clinic
+0.0550	cre
+0.0478	gstplk1
+0.0444	repeat
+0.0387	pin1transfect
+0.0360	fos
+0.0315	pin1medi
+0.0292	dysfunct
+0.0256	trityl
+0.0237	infus
+0.0000	a10
+0.0000	a1978
+0.0000	a427
+0.0000	a68
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xl
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4054
First 10 features: [u'a10', u'a1978', u'a427', u'a68', u'aa', u'aad', u'aav', u'aav5', u'aav6', u'aav8']

Middle 10 features: [u'lack', u'lactat', u'ladder', u'laden', u'laid', u'lamp1', u'lamp2', u'land', u'lane', u'langdon']

Last 10 features: [u'yield', u'young', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 1
25533336

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-08-19-05. Total      1.33 seconds

{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=225, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=225, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 225, 'classifier__criterion': 'friedman_mse'}
{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=233, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=233, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 233, 'classifier__criterion': 'friedman_mse'}
{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=131, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=131, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 131, 'classifier__criterion': 'friedman_mse'}
******
{'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
******
{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=527, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=527, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 527, 'classifier__criterion': 'friedman_mse'}
******
{'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
******
{'vectorizer__ngram_range': (1, 2), 'vectorizer__max_features': None, 'vectorizer__max_df': 0.75, 'classifier__n_estimators': 100, 'classifier__validation_fraction': 0.1, 'classifier__presort': 'auto', 'vectorizer__encoding': u'utf-8', 'classifier__verbose': 1, 'classifier__max_leaf_nodes': None, 'vectorizer__input': u'content', 'classifier__warm_start': False, 'memory': None, 'vectorizer__preprocessor': None, 'vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'classifier__min_impurity_split': None, 'classifier__subsample': 1.0, 'classifier__max_features': None, 'vectorizer__min_df': 0.02, 'classifier__learning_rate': 0.1, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__min_impurity_decrease': 0.0, 'vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'vectorizer__analyzer': u'word', 'vectorizer__binary': True, 'vectorizer__lowercase': False, 'vectorizer__tokenizer': None, 'vectorizer__stop_words': 'english', 'classifier__min_samples_leaf': 1, 'vectorizer__vocabulary': None, 'classifier__max_depth': 3, 'classifier__init': None, 'vectorizer__dtype': <type 'numpy.int64'>, 'classifier__min_samples_split': 2, 'classifier__n_iter_no_change': None, 'classifier__loss': 'deviance', 'classifier__tol': 0.0001, 'vectorizer__decode_error': 'strict', 'steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=517, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False))], 'vectorizer__strip_accents': None, 'classifier': GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=517, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False), 'classifier__random_state': 517, 'classifier__criterion': 'friedman_mse'}
******
{'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
******
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-11-32-57  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=517   randForSplit=113   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.80      1.00      0.89         4
Valid discard       1.00      0.50      0.67         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.90      0.75      0.78         6
 weighted avg       0.87      0.83      0.81         6

Valid F2: 0.95238 (keep)

['yes', 'no']
[[4 0]
 [1 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=517, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.1697	section
+0.1622	gene
+0.1370	ratio
+0.0963	week
+0.0777	recipi
+0.0630	modul
+0.0589	idu
+0.0511	anim
+0.0475	cvd
+0.0416	recapitul
+0.0385	mimosin
+0.0312	cytokinesi
+0.0254	oxygenas
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4058
First 10 features: [u'a10', u'a1express', u'a427', u'a7c11', u'aa', u'aad', u'aav', u'aav5', u'aav9', u'ab']

Middle 10 features: [u'laden', u'lambda', u'lambert', u'laminin', u'lamp2', u'land', u'lane', u'langdon', u'larg', u'larger']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 1
29247188

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-11-32-58. Total      0.55 seconds

in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.00s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-11-34-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=169   randForSplit=464   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.67      0.50      0.57         4
Valid discard       0.33      0.50      0.40         2

    micro avg       0.50      0.50      0.50         6
    macro avg       0.50      0.50      0.49         6
 weighted avg       0.56      0.50      0.51         6

Valid F2: 0.52632 (keep)

['yes', 'no']
[[2 2]
 [1 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=169, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+1.0000	gene
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav8
+0.0000	aav8hntcp
+0.0000	aavscr
+0.0000	aavshano2
+0.0000	abbrevi
+0.0000	abca1
+0.0000	abd
+0.0000	abdomin
+0.0000	aberr
+0.0000	abil
+0.0000	abl
+0.0000	ablat

### Feature weights: lowest 20
+0.0000	xf96
+0.0000	xmg1
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 3820
First 10 features: [u'a1978', u'a1express', u'a427', u'a68', u'aa', u'aad', u'aav', u'aav8', u'aav8hntcp', u'aavscr']

Middle 10 features: [u'kb', u'kd', u'kda', u'keap', u'keap1', u'keap1medi', u'kelch', u'kept', u'keratin', u'keratinocyt']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 1
30321399

### False negatives for Validation set: 2
25533336
27411738

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-11-34-04. Total      0.53 seconds

in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-11-35-22  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=598   randForSplit=524   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.75      0.86         4
Valid discard       0.67      1.00      0.80         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.83      0.88      0.83         6
 weighted avg       0.89      0.83      0.84         6

Valid F2: 0.78947 (keep)

['yes', 'no']
[[3 1]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=598, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6087	gene
+0.0593	boundari
+0.0550	latent
+0.0478	readout
+0.0444	pad
+0.0387	wb
+0.0360	ly6g
+0.0315	fastap
+0.0292	supernat
+0.0256	myb
+0.0237	recombinas
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	wwp2
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone

### Vectorizer:   Number of Features: 3992
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aavscr']

Middle 10 features: [u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e', u'l697']

Last 10 features: [u'yellow', u'yield', u'young', u'youngest', u'zaytseva', u'zero', u'zn', u'znf822', u'znpp', u'zone']

### False positives for Validation set: 0

### False negatives for Validation set: 1
27411738

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-11-35-23. Total      0.55 seconds

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
### Start Time 2019/10/18-11-37-24  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=180   randForSplit=428   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=180, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	cage
+0.0641	ventral
+0.0519	cre
+0.0466	stlc
+0.0421	term
+0.0376	mimosin
+0.0343	modul
+0.0304	tp
+0.0247	autoradiographi
+0.0201	pbm
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-11-37-25. Total      0.59 seconds

in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.03s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
### Start Time 2019/10/18-12-40-46  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=23   randForSplit=943   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=23, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0941	insert
+0.0794	faster
+0.0641	modul
+0.0466	glutathionesepharos
+0.0376	wce
+0.0343	deliveri
+0.0304	dbd
+0.0247	dic
+0.0201	poorer
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-12-40-47. Total      0.65 seconds

in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-12-42-42  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=84   randForSplit=820   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        12
Train discard       1.00      1.00      1.00         8

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[12  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.60      0.75         5
Valid discard       0.33      1.00      0.50         1

    micro avg       0.67      0.67      0.67         6
    macro avg       0.67      0.80      0.62         6
 weighted avg       0.89      0.67      0.71         6

Valid F2: 0.65217 (keep)

['yes', 'no']
[[3 2]
 [0 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.02
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=5,
   n_iter_no_change=None, presort='auto', random_state=84, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.4095	reduct
+0.2172	ratio
+0.0556	nonlinear
+0.0531	leupeptin
+0.0450	intraven
+0.0430	autophag
+0.0365	authent
+0.0349	male
+0.0297	proteolysi
+0.0283	firm
+0.0241	adjuv
+0.0230	glycogen
+0.0000	a10
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	wwp2
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xl
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	zaytseva
+0.0000	zero
+0.0000	zn
+0.0000	znf822
+0.0000	znpp

### Vectorizer:   Number of Features: 4060
First 10 features: [u'a10', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5', u'aav6', u'aav8']

Middle 10 features: [u'laid', u'lamp1', u'lamp2', u'land', u'lane', u'langdon', u'larg', u'larger', u'largest', u'laser']

Last 10 features: [u'year', u'yeast', u'yellow', u'yield', u'young', u'zaytseva', u'zero', u'zn', u'znf822', u'znpp']

### False positives for Validation set: 0

### False negatives for Validation set: 2
28732055
25882312

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           12            8          60%
Validation Set      :            6            5            1          83%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-12-42-43. Total      0.56 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.01s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 15

      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
in GB fit, length 16

      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 17

      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
in GB fit, length 20

      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-12-50-54  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=815   randForSplit=679   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.80      1.00      0.89         4
Valid discard       1.00      0.50      0.67         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.90      0.75      0.78         6
 weighted avg       0.87      0.83      0.81         6

Valid F2: 0.95238 (keep)

['yes', 'no']
[[4 0]
 [1 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=815, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.3583	gene
+0.1106	scale
+0.0963	term
+0.0777	s2c
+0.0630	defin
+0.0589	pretreat
+0.0511	inject
+0.0475	thymidin
+0.0416	faster
+0.0385	kinesin
+0.0312	cy
+0.0254	coprecipit
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aav

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xl
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4119
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aav', u'aav5', u'aav6']

Middle 10 features: [u'ldlr', u'lead', u'learn', u'learnt', u'lectin', u'led', u'left', u'legend', u'length', u'lentivir']

Last 10 features: [u'yellow', u'yield', u'young', u'youngest', u'zero', u'zhang', u'zip', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 1
29247188

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-12-50-56. Total      2.25 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
in GB fit, length 28

      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
in GB fit, length 26

      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-12-55-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=764   randForSplit=180   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GBjak(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',
   max_depth=3, max_features=None, max_leaf_nodes=None,
   min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1,
   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,
   n_iter_no_change=None, presort='auto', random_state=764, subsample=1.0,
   tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	environment
+0.0641	cage
+0.0519	helper
+0.0466	eearly_embryo
+0.0421	puls
+0.0376	mobil
+0.0343	faster
+0.0304	unphosphoryl
+0.0247	bmyb
+0.0201	alanin
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-12-55-14. Total      1.07 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-15-29  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=692   randForSplit=167   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=692,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	bm
+0.0641	predomin
+0.0519	helper
+0.0505	coomassi
+0.0466	exemplarili
+0.0421	remark
+0.0376	cytokinesi
+0.0343	govern
+0.0247	readout
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-13-15-30. Total      1.06 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-52-24  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=738   randForSplit=163   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=738,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	densiti
+0.0641	ventral
+0.0519	faster
+0.0466	mybspecif
+0.0421	metabol
+0.0376	electrophoret
+0.0343	govern
+0.0304	l697
+0.0247	assum
+0.0201	foxm1
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-13-52-25. Total      1.06 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-54-22  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=886   randForSplit=618   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=886,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.1137	term
+0.0641	modul
+0.0519	dendrit
+0.0466	mimosin
+0.0421	densiti
+0.0376	nonmitot
+0.0304	r695
+0.0247	preferenti
+0.0201	multiploid
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-13-54-23. Total      1.06 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0661            0.01s
         2           0.9189            0.01s
         3           0.7999            0.01s
         4           0.7011            0.00s
         5           0.6177            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-56-41  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=451   randForSplit=165   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        17
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train F2: 1.00000 (keep)

['yes', 'no']
[[17  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         2
Valid discard       0.00      0.00      0.00         0

    micro avg       1.00      1.00      1.00         2
    macro avg       0.50      0.50      0.50         2
 weighted avg       1.00      1.00      1.00         2

Valid F2: 1.00000 (keep)

['yes', 'no']
[[2 0]
 [0 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=451,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.5688	gene
+0.0794	modul
+0.0641	versus
+0.0519	metabol
+0.0466	polyploidi
+0.0421	insert
+0.0376	binucl
+0.0343	faster
+0.0304	replat
+0.0247	gstplk1
+0.0201	spectral
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4760
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kodak', u'kras', u'krasg12d', u'kruskal', u'kwon', u'l2', u'l211a', u'l3', u'l477h', u'l552e']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           17            9          65%
Validation Set      :            2            2            0         100%
ValidationSplit: 0.20
### End Time 2019/10/18-13-56-42. Total      1.06 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.00s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.01s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.01s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.01s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.00s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.01s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.01s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.01s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0320            0.01s
         2           0.8873            0.01s
         3           0.7712            0.00s
         4           0.6754            0.00s
         5           0.5947            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-13-58-45  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=38   randForSplit=547   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        14
Train discard       1.00      1.00      1.00         6

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[14  0]
 [ 0  6]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         3
Valid discard       1.00      1.00      1.00         3

    micro avg       1.00      1.00      1.00         6
    macro avg       1.00      1.00      1.00         6
 weighted avg       1.00      1.00      1.00         6

Valid F2: 1.00000 (keep)

['yes', 'no']
[[3 0]
 [0 3]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=38,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.3721	gene
+0.1451	histogram
+0.0998	dendrit
+0.0680	ratio
+0.0645	violet
+0.0526	despit
+0.0518	consensus
+0.0427	faster
+0.0419	aliquot
+0.0340	address
+0.0276	foxm1
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav

### Feature weights: lowest 20
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone

### Vectorizer:   Number of Features: 4184
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'kinet', u'kit', u'kl', u'klh', u'klrg', u'klrg1', u'klrg1hi', u'klrg1lo', u'knock', u'knock_in']

Last 10 features: [u'yellow', u'yield', u'young', u'youngest', u'zaytseva', u'zero', u'zn', u'znf822', u'znpp', u'zone']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           14            6          70%
Validation Set      :            6            3            3          50%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-13-58-48. Total      2.26 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.01s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.00s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9351            0.01s
         2           0.7981            0.01s
         3           0.6909            0.00s
         4           0.6035            0.00s
         5           0.5306            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-00-25  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=933   randForSplit=150   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        15
Train discard       1.00      1.00      1.00         5

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[15  0]
 [ 0  5]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.50      1.00      0.67         2
Valid discard       1.00      0.50      0.67         4

    micro avg       0.67      0.67      0.67         6
    macro avg       0.75      0.75      0.67         6
 weighted avg       0.83      0.67      0.67         6

Valid F2: 0.83333 (keep)

['yes', 'no']
[[2 0]
 [2 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=933,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6190	ratio
+0.1310	comparison
+0.1125	fraction
+0.0899	bone
+0.0475	despit
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5
+0.0000	aav6
+0.0000	aav8
+0.0000	aav8hntcp
+0.0000	aav9
+0.0000	aavscr

### Feature weights: lowest 20
+0.0000	xl
+0.0000	xmg1
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4196
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5']

Middle 10 features: [u'ki', u'ki67', u'kidney', u'kill', u'kinas', u'kinesin', u'kinet', u'kit', u'kl', u'klh']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 2
30683694
28515318

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           15            5          75%
Validation Set      :            6            2            4          33%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-00-27. Total      2.23 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.00s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.00s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-01-11  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=99   randForSplit=467   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.75      0.75      0.75         4
Valid discard       0.50      0.50      0.50         2

    micro avg       0.67      0.67      0.67         6
    macro avg       0.62      0.62      0.62         6
 weighted avg       0.67      0.67      0.67         6

Valid F2: 0.75000 (keep)

['yes', 'no']
[[3 1]
 [1 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=99,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.8077	gene
+0.0562	govern
+0.0453	gh
+0.0367	s7a
+0.0298	vehicl
+0.0242	marrow
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav8
+0.0000	aav8hntcp
+0.0000	ab
+0.0000	abbrevi
+0.0000	abca1
+0.0000	abd

### Feature weights: lowest 20
+0.0000	xf96
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 3888
First 10 features: [u'a10', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav8', u'aav8hntcp']

Middle 10 features: [u'kl', u'klh', u'klrg', u'klrg1', u'klrg1hi', u'klrg1lo', u'knock_in', u'knock_out', u'knockdown', u'knowledg']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 1
30321399

### False negatives for Validation set: 1
27411738

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-01-13. Total      2.03 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.00s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.00s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.01s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.00s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0320            0.01s
         2           0.8873            0.01s
         3           0.7712            0.00s
         4           0.6754            0.00s
         5           0.5947            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0832            0.01s
         2           0.9348            0.00s
         3           0.8143            0.00s
         4           0.7141            0.00s
         5           0.6293            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.01s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1086            0.01s
         2           0.9585            0.00s
         3           0.8359            0.00s
         4           0.7335            0.00s
         5           0.6467            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1050            0.01s
         2           0.9552            0.01s
         3           0.8328            0.00s
         4           0.7307            0.00s
         5           0.6442            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-07-11  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=619   randForSplit=73   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        13
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.00000 (keep)	NPV: 1.00000

['yes', 'no']
[[13  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.75      0.86         4
Valid discard       0.67      1.00      0.80         2

    micro avg       0.83      0.83      0.83         6
    macro avg       0.83      0.88      0.83         6
 weighted avg       0.89      0.83      0.84         6

Valid F2: 0.78947 (keep)	NPV: 0.66667

['yes', 'no']
[[3 1]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.00000 (keep)	NPV: 1.00000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=619,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6087	gene
+0.0593	eg5
+0.0550	memory27
+0.0478	spectral
+0.0444	ampa
+0.0387	nonmitot
+0.0360	fepsp
+0.0315	pretreat
+0.0292	anticamkii
+0.0256	thymidin
+0.0237	camkiia
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5
+0.0000	aav8

### Feature weights: lowest 20
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4080
First 10 features: [u'a1978', u'a1express', u'a427', u'a68', u'aa', u'aad', u'aav', u'aav5', u'aav8', u'aav8hntcp']

Middle 10 features: [u'klrg1hi', u'klrg1lo', u'knock', u'knock_in', u'knock_out', u'knockdown', u'knowledg', u'known', u'kodak', u'kras']

Last 10 features: [u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 1
25533336

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           13            7          65%
Validation Set      :            6            4            2          67%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-07-13. Total      2.07 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.1918            0.01s
         2           1.0365            0.00s
         3           0.9071            0.00s
         4           0.7977            0.00s
         5           0.7044            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.01s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.00s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1928            0.01s
         2           1.0374            0.00s
         3           0.9079            0.00s
         4           0.7985            0.00s
         5           0.7051            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1918            0.01s
         2           1.0365            0.00s
         3           0.9071            0.00s
         4           0.7977            0.00s
         5           0.7044            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.00s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.00s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1806            0.01s
         2           1.0260            0.00s
         3           0.8974            0.00s
         4           0.7890            0.00s
         5           0.6965            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1928            0.01s
         2           1.0374            0.00s
         3           0.9079            0.00s
         4           0.7985            0.00s
         5           0.7051            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1863            0.01s
         2           1.0313            0.01s
         3           0.9023            0.00s
         4           0.7934            0.00s
         5           0.7005            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-11-19  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=153   randForSplit=831   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        11
Train discard       1.00      1.00      1.00         9

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.0000 (keep)    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[11  0]
 [ 0  9]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.67      0.80         6
Valid discard       0.00      0.00      0.00         0

    micro avg       0.67      0.67      0.67         6
    macro avg       0.50      0.33      0.40         6
 weighted avg       1.00      0.67      0.80         6

Valid F2: 0.7143 (keep)    P: 1.0000    R: 0.6667    NPV: 0.0000

['yes', 'no']
[[4 2]
 [0 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.0000 (keep)    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=153,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6368	ratio
+0.0665	furthermor
+0.0534	span
+0.0522	santa
+0.0423	necrosi
+0.0351	share
+0.0344	apci
+0.0286	xds
+0.0279	posttranscript
+0.0227	chemotact
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	writh
+0.0000	wrote
+0.0000	ww
+0.0000	wwp1
+0.0000	wwp2
+0.0000	x710
+0.0000	xf96
+0.0000	xl
+0.0000	xscale
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zn
+0.0000	znf822
+0.0000	znpp

### Vectorizer:   Number of Features: 4114
First 10 features: [u'a10', u'a1978', u'a1express', u'a68', u'a7c11', u'aa', u'aad', u'aav', u'aav5', u'aav6']

Middle 10 features: [u'l211a', u'l3', u'l477h', u'l552e', u'l697', u'la', u'label', u'laboratori', u'lack', u'lactat']

Last 10 features: [u'yeast', u'yellow', u'yield', u'young', u'youngest', u'zaytseva', u'zero', u'zn', u'znf822', u'znpp']

### False positives for Validation set: 0

### False negatives for Validation set: 2
30097518
28732055

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           11            9          55%
Validation Set      :            6            6            0         100%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-11-21. Total      2.18 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-12-40  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=234   randForSplit=950   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        12
Train discard       1.00      1.00      1.00         8

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train F2: 1.0000 (keep)    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[12  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.40      0.57         5
Valid discard       0.25      1.00      0.40         1

    micro avg       0.50      0.50      0.50         6
    macro avg       0.62      0.70      0.49         6
 weighted avg       0.88      0.50      0.54         6

Valid F2: 0.4545 (keep)    P: 1.0000    R: 0.4000    NPV: 0.2500

['yes', 'no']
[[2 3]
 [0 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  F2: 1.0000 (keep)    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=234,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6429	imag
+0.1882	scheme
+0.1689	caspas
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	aa
+0.0000	aav
+0.0000	aav5
+0.0000	aav6
+0.0000	aav8
+0.0000	aav8hntcp
+0.0000	aav9
+0.0000	aavscr
+0.0000	aavshano2
+0.0000	abbrevi
+0.0000	abca1
+0.0000	abd
+0.0000	abdomin

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xl
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4074
First 10 features: [u'a10', u'a1express', u'a427', u'a68', u'aa', u'aav', u'aav5', u'aav6', u'aav8', u'aav8hntcp']

Middle 10 features: [u'late', u'latenc', u'later', u'layer', u'lc', u'lc3', u'lc3b', u'lc3posit', u'ldl', u'ldlr']

Last 10 features: [u'yellow', u'yield', u'young', u'youngest', u'zero', u'zhang', u'zip', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 3
27298446
25882312
28558013

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           12            8          60%
Validation Set      :            6            5            1          83%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-12-42. Total      2.10 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.00s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-17-16  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=694   randForSplit=743   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        12
Train discard       1.00      1.00      1.00         8

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train (keep) P: 1.0000    R: 1.0000    f2: 1.0000    NPV: 1.0000

['yes', 'no']
[[12  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.80      0.80      0.80         5
Valid discard       0.00      0.00      0.00         1

    micro avg       0.67      0.67      0.67         6
    macro avg       0.40      0.40      0.40         6
 weighted avg       0.67      0.67      0.67         6

Valid (keep) P: 0.8000    R: 0.8000    f2: 0.8000    NPV: 0.0000

['yes', 'no']
[[4 1]
 [1 0]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  (keep) P: 1.0000    R: 1.0000    f2: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=694,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.3592	imag
+0.2676	ratio
+0.0556	mice
+0.0531	fusion
+0.0450	r192g
+0.0430	femor
+0.0365	sigmoid
+0.0349	callus
+0.0297	impli
+0.0283	suppress
+0.0241	ip
+0.0230	recombinas
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	xds
+0.0000	xf96
+0.0000	xl
+0.0000	xscale
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4151
First 10 features: [u'a10', u'a1express', u'a427', u'a7c11', u'aa', u'aad', u'aav', u'aav5', u'aav6', u'aav8']

Middle 10 features: [u'laboratori', u'lack', u'lactat', u'lactobacillus', u'ladder', u'laden', u'laminin', u'land', u'lane', u'langdon']

Last 10 features: [u'yield', u'young', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'zone', u'zygos']

### False positives for Validation set: 1
28274624

### False negatives for Validation set: 1
25882312

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           12            8          60%
Validation Set      :            6            5            1          83%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-17-18. Total      2.14 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.01s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.01s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9702            0.01s
         2           0.8303            0.01s
         3           0.7198            0.00s
         4           0.6293            0.00s
         5           0.5536            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9062            0.00s
         3           0.7884            0.00s
         4           0.6907            0.00s
         5           0.6084            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0218            0.01s
         2           0.8779            0.01s
         3           0.7628            0.00s
         4           0.6678            0.00s
         5           0.5879            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0320            0.01s
         2           0.8873            0.01s
         3           0.7712            0.00s
         4           0.6754            0.00s
         5           0.5947            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.00s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-21-36  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=63   randForSplit=979   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        14
Train discard       1.00      1.00      1.00         6

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train (keep) P: 1.0000    R: 1.0000    F2: 1.0000    NPV: 1.0000

['yes', 'no']
[[14  0]
 [ 0  6]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         3
Valid discard       1.00      1.00      1.00         3

    micro avg       1.00      1.00      1.00         6
    macro avg       1.00      1.00      1.00         6
 weighted avg       1.00      1.00      1.00         6

Valid (keep) P: 1.0000    R: 1.0000    F2: 1.0000    NPV: 1.0000

['yes', 'no']
[[3 0]
 [0 3]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  (keep) P: 1.0000    R: 1.0000    F2: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=63,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.4401	gene
+0.0998	helper
+0.0802	restor
+0.0648	govern
+0.0645	conform
+0.0526	predomin
+0.0518	l697
+0.0427	cage
+0.0419	glutathionesepharos
+0.0340	pretreat
+0.0276	roscovitin
+0.0000	a10
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	aa
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	wzl
+0.0000	x710
+0.0000	xds
+0.0000	xf96
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	znpp
+0.0000	zone
+0.0000	zygos

### Vectorizer:   Number of Features: 4082
First 10 features: [u'a10', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'aa', u'aav', u'aav5', u'aav9']

Middle 10 features: [u'la', u'label', u'lack', u'lactat', u'lactobacillus', u'laden', u'laid', u'laminin', u'lamp1', u'lamp2']

Last 10 features: [u'yeast', u'yellow', u'yield', u'young', u'zero', u'zhang', u'zip', u'znpp', u'zone', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           14            6          70%
Validation Set      :            6            3            3          50%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-21-38. Total      2.18 seconds

Fitting 5 folds for each of 2 candidates, totalling 10 fits
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.00s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.01s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.00s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1332            0.01s
         2           0.9815            0.01s
         3           0.8568            0.00s
         4           0.7523            0.00s
         5           0.6636            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1650            0.01s
         2           1.0113            0.00s
         3           0.8840            0.00s
         4           0.7769            0.00s
         5           0.6856            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1561            0.01s
         2           1.0029            0.01s
         3           0.8764            0.00s
         4           0.7700            0.00s
         5           0.6794            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.1002            0.01s
         2           0.9507            0.01s
         3           0.8287            0.01s
         4           0.7270            0.00s
         5           0.6409            0.00s
### Start Time 2019/10/18-14-23-14  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: None
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=556   randForSplit=667   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        12
Train discard       1.00      1.00      1.00         8

    micro avg       1.00      1.00      1.00        20
    macro avg       1.00      1.00      1.00        20
 weighted avg       1.00      1.00      1.00        20

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[12  0]
 [ 0  8]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      0.80      0.89         5
Valid discard       0.50      1.00      0.67         1

    micro avg       0.83      0.83      0.83         6
    macro avg       0.75      0.90      0.78         6
 weighted avg       0.92      0.83      0.85         6

Valid (keep) F2: 0.8333    P: 1.0000    R: 0.8000    NPV: 0.5000

['yes', 'no']
[[4 1]
 [0 1]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       1.00      1.00      1.00         1
Test  discard       1.00      1.00      1.00         1

    micro avg       1.00      1.00      1.00         2
    macro avg       1.00      1.00      1.00         2
 weighted avg       1.00      1.00      1.00         2

Test  (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=556,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.4277	gene
+0.1991	ratio
+0.0853	ask
+0.0531	innat
+0.0450	altogeth
+0.0430	chimera
+0.0365	deimmun
+0.0349	halt
+0.0283	pge2
+0.0241	splenocyt
+0.0230	nk1
+0.0000	a10
+0.0000	a1express
+0.0000	a427
+0.0000	a7c11
+0.0000	aa
+0.0000	aad
+0.0000	aav
+0.0000	aav6
+0.0000	aav8

### Feature weights: lowest 20
+0.0000	xl
+0.0000	xmg1
+0.0000	xscale
+0.0000	xt22
+0.0000	y1767a
+0.0000	y2154l
+0.0000	year
+0.0000	yeast
+0.0000	yellow
+0.0000	yield
+0.0000	young
+0.0000	youngest
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zygos

### Vectorizer:   Number of Features: 3939
First 10 features: [u'a10', u'a1express', u'a427', u'a7c11', u'aa', u'aad', u'aav', u'aav6', u'aav8', u'aav8hntcp']

Middle 10 features: [u'l697', u'label', u'laboratori', u'lack', u'lactat', u'lactobacillus', u'ladder', u'lambda', u'lambert', u'laminin']

Last 10 features: [u'young', u'youngest', u'zaytseva', u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zygos']

### False positives for Validation set: 0

### False negatives for Validation set: 1
28732055

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           20           12            8          60%
Validation Set      :            6            5            1          83%
Test Set            :            2            1            1          50%
ValidationSplit: 0.20
### End Time 2019/10/18-14-23-16. Total      2.11 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.02s
         2           0.8350            0.01s
         3           0.7241            0.01s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/24-14-22-17  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=961   randForSplit=408   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=961,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	ventral
+0.0717	predomin
+0.0579	deliveri
+0.0469	metabol
+0.0380	densiti
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/24-14-22-18. Total      1.17 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.02s
         2           0.8350            0.01s
         3           0.7241            0.01s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-15-21-22  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=490   randForSplit=979   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=490,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	deliveri
+0.0717	histogram
+0.0579	modul
+0.0469	environment
+0.0380	govern
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/28-15-21-24. Total      1.23 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-15-28-18  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=594   randForSplit=944   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=594,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	dendrit
+0.0717	exampl
+0.0579	densiti
+0.0469	remark
+0.0380	modul
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2019/10/28-15-28-19. Total      1.18 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-15-43-08  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=204   randForSplit=168   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=204,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	cage
+0.0717	deliveri
+0.0579	govern
+0.0469	faster
+0.0380	densiti
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2019/10/28-15-43-09. Total      1.18 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-15-50-15  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=19   randForSplit=813   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=19,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	bm
+0.0717	term
+0.0579	insert
+0.0469	govern
+0.0380	helper
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2019/10/28-15-50-16. Total      1.18 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/28-16-18-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/testSet.txt
Random Seeds:	randForClassifier=875   randForSplit=344   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Metrics: Test Set
               precision    recall  f1-score   support

   Test  keep       0.00      0.00      0.00         0
Test  discard       0.00      0.00      0.00         1

    micro avg       0.00      0.00      0.00         1
    macro avg       0.00      0.00      0.00         1
 weighted avg       0.00      0.00      0.00         1

Test  (keep) F2: 0.0000    P: 0.0000    R: 0.0000    NPV: 0.0000

['yes', 'no']
[[0 0]
 [1 0]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=875,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	restor
+0.0849	insert
+0.0717	govern
+0.0579	cre
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
Test Set            :            1            0            1           0%
ValidationSplit: 0.20
### End Time 2019/10/28-16-18-05. Total      1.25 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.03s
         2           0.8350            0.01s
         3           0.7241            0.01s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0490            0.01s
         2           0.9030            0.01s
         3           0.7855            0.00s
         4           0.6882            0.00s
         5           0.6061            0.00s
      Iter       Train Loss   Remaining Time 
         1           0.9753            0.01s
         2           0.8350            0.01s
         3           0.7241            0.00s
         4           0.6331            0.00s
         5           0.5570            0.00s
### Start Time 2019/10/29-09-01-55  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=498   randForSplit=762   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=498,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0896	dendrit
+0.0717	restor
+0.0579	histogram
+0.0469	faster
+0.0380	densiti
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/29-09-01-56. Total      1.01 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0550            0.01s
         2           0.9182            0.01s
         3           0.8044            0.00s
         4           0.7082            0.00s
         5           0.6261            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0550            0.01s
         2           0.9182            0.01s
         3           0.8044            0.00s
         4           0.7082            0.00s
         5           0.6261            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0817            0.01s
         2           0.9394            0.01s
         3           0.8217            0.00s
         4           0.7226            0.00s
         5           0.6382            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0550            0.01s
         2           0.9182            0.01s
         3           0.8044            0.00s
         4           0.7082            0.00s
         5           0.6261            0.00s
### Start Time 2019/10/29-09-08-36  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=407   randForSplit=405   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Clf instance at 0x1a1cf41908>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=407,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.1053	bm
+0.0886	term
+0.0717	histogram
+0.0384	metabol
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/29-09-08-37. Total      1.06 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0597            0.01s
         2           0.9224            0.01s
         3           0.8083            0.00s
         4           0.7117            0.00s
         5           0.6292            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0597            0.01s
         2           0.9224            0.01s
         3           0.8083            0.00s
         4           0.7117            0.00s
         5           0.6292            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0857            0.01s
         2           0.9434            0.01s
         3           0.8254            0.00s
         4           0.7261            0.00s
         5           0.6413            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0597            0.01s
         2           0.9224            0.01s
         3           0.8083            0.00s
         4           0.7117            0.00s
         5           0.6292            0.00s
### Start Time 2019/10/29-17-00-39  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=454   randForSplit=429   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Clf instance at 0x1a14935d40>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=454,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.1054	remark
+0.0886	ventral
+0.0717	densiti
+0.0384	term
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/29-17-00-40. Total      0.96 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0593            0.01s
         2           0.9221            0.01s
         3           0.8080            0.00s
         4           0.7115            0.00s
         5           0.6290            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0593            0.01s
         2           0.9221            0.01s
         3           0.8080            0.00s
         4           0.7115            0.00s
         5           0.6290            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0893            0.01s
         2           0.9468            0.01s
         3           0.8286            0.00s
         4           0.7290            0.00s
         5           0.6440            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0593            0.01s
         2           0.9221            0.01s
         3           0.8080            0.00s
         4           0.7115            0.00s
         5           0.6290            0.00s
Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9157            0.01s
         3           0.8021            0.00s
         4           0.7062            0.00s
         5           0.6242            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9157            0.01s
         3           0.8021            0.00s
         4           0.7062            0.00s
         5           0.6242            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0847            0.01s
         2           0.9423            0.01s
         3           0.8244            0.00s
         4           0.7252            0.00s
         5           0.6405            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0524            0.01s
         2           0.9157            0.01s
         3           0.8021            0.00s
         4           0.7062            0.00s
         5           0.6242            0.00s
### Start Time 2019/10/30-10-49-04  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=56   randForSplit=547   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Clf instance at 0x1a22c02cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=56,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0887	predomin
+0.0717	histogram
+0.0581	govern
+0.0472	puls
+0.0384	term
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/30-10-49-05. Total      0.95 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.01s
         2           0.9271            0.01s
         3           0.8124            0.00s
         4           0.7155            0.00s
         5           0.6325            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.01s
         2           0.9271            0.01s
         3           0.8124            0.00s
         4           0.7155            0.00s
         5           0.6325            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0878            0.01s
         2           0.9453            0.01s
         3           0.8273            0.00s
         4           0.7278            0.00s
         5           0.6429            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0647            0.01s
         2           0.9271            0.01s
         3           0.8124            0.00s
         4           0.7155            0.00s
         5           0.6325            0.00s
### Start Time 2019/10/30-10-50-06  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=993   randForSplit=289   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.50      1.00      0.67         1
Valid discard       1.00      0.50      0.67         2

    micro avg       0.67      0.67      0.67         3
    macro avg       0.75      0.75      0.67         3
 weighted avg       0.83      0.67      0.67         3

Valid (keep) F2: 0.8333    P: 0.5000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [1 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Clf instance at 0x1a1da70908>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=993,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.1298	suffici
+0.0886	cage
+0.0472	cre
+0.0384	remark
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 1
29378950

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/10/30-10-50-07. Total      1.05 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0248            0.01s
         2           0.8893            0.01s
         3           0.7777            0.00s
         4           0.6840            0.00s
         5           0.6041            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0248            0.01s
         2           0.8893            0.01s
         3           0.7777            0.00s
         4           0.6840            0.00s
         5           0.6041            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0720            0.01s
         2           0.9296            0.01s
         3           0.8124            0.00s
         4           0.7140            0.00s
         5           0.6303            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0248            0.01s
         2           0.8893            0.01s
         3           0.7777            0.00s
         4           0.6840            0.00s
         5           0.6041            0.00s
### Start Time 2019/11/04-12-17-38  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=329   randForSplit=410   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Note: this is a note
### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a15b59cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=329,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.0888	restor
+0.0855	term
+0.0717	deliveri
+0.0581	exampl
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/04-12-17-39. Total      1.08 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0496            0.01s
         2           0.9131            0.01s
         3           0.7997            0.00s
         4           0.7041            0.00s
         5           0.6223            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0496            0.01s
         2           0.9131            0.01s
         3           0.7997            0.00s
         4           0.7041            0.00s
         5           0.6223            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0835            0.01s
         2           0.9412            0.01s
         3           0.8234            0.00s
         4           0.7242            0.00s
         5           0.6396            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0496            0.01s
         2           0.9131            0.01s
         3           0.7997            0.00s
         4           0.7041            0.00s
         5           0.6223            0.00s
### Start Time 2019/11/04-12-18-14  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=431   randForSplit=393   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a13edd908>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=431,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Feature weights: highest 20
+0.6959	gene
+0.1468	histogram
+0.0717	modul
+0.0472	insert
+0.0384	deliveri
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/04-12-18-15. Total      1.14 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0611            0.01s
         2           0.9238            0.01s
         3           0.8095            0.00s
         4           0.7128            0.00s
         5           0.6302            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0611            0.01s
         2           0.9238            0.01s
         3           0.8095            0.00s
         4           0.7128            0.00s
         5           0.6302            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0903            0.01s
         2           0.9478            0.01s
         3           0.8295            0.00s
         4           0.7298            0.00s
         5           0.6448            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0611            0.01s
         2           0.9238            0.01s
         3           0.8095            0.00s
         4           0.7128            0.00s
         5           0.6302            0.00s
### Start Time 2019/11/06-10-53-21  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=55   randForSplit=364   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       0.50      1.00      0.67         1
Valid discard       1.00      0.50      0.67         2

    micro avg       0.67      0.67      0.67         3
    macro avg       0.75      0.75      0.67         3
 weighted avg       0.83      0.67      0.67         3

Valid (keep) F2: 0.8333    P: 0.5000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [1 1]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

Pipeline:
classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a18ab4c68>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=55,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Feature weights: highest 20
+0.6959	gene
+0.0966	suffici
+0.0886	exampl
+0.0717	remark
+0.0472	deliveri
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 1
29378950

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-10-53-22. Total      1.13 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0440            0.01s
         2           0.9078            0.01s
         3           0.7949            0.00s
         4           0.6997            0.00s
         5           0.6184            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0440            0.01s
         2           0.9078            0.01s
         3           0.7949            0.00s
         4           0.6997            0.00s
         5           0.6184            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0801            0.01s
         2           0.9378            0.01s
         3           0.8202            0.00s
         4           0.7213            0.00s
         5           0.6370            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0440            0.01s
         2           0.9078            0.01s
         3           0.7949            0.00s
         4           0.6997            0.00s
         5           0.6184            0.00s
### Start Time 2019/11/06-10-56-33  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=395   randForSplit=954   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a15dcbb00>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=395,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
{'n_jobs': 1, 'verbose': True, 'estimator__vectorizer__dtype': <type 'numpy.int64'>, 'estimator__vectorizer__ngram_range': (1, 2), 'estimator__vectorizer__min_df': 0.02, 'estimator__memory': None, 'estimator__classifier__min_samples_leaf': 1, 'estimator__classifier__validation_fraction': 0.1, 'estimator__steps': [('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a15c04cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=395,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False))], 'estimator__vectorizer__input': u'content', 'estimator__classifier__loss': 'deviance', 'param_grid': {'vectorizer__min_df': [0.01, 0.02], 'vectorizer__ngram_range': [(1, 1)], 'classifier__n_estimators': [5], 'vectorizer__max_df': [0.75]}, 'cv': [([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 27, 28])], 'estimator__classifier__warm_start': False, 'scoring': make_scorer(fbeta_score, beta=2, pos_label=1), 'estimator__classifier__min_impurity_decrease': 0.0, 'estimator__vectorizer__preprocessor': None, 'estimator__vectorizer': CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'fit_params': None, 'estimator__vectorizer__vocabulary': None, 'estimator': Pipeline(memory=None,
     steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='englis...    subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False))]), 'estimator__vectorizer__lowercase': False, 'estimator__classifier__max_features': None, 'pre_dispatch': '2*n_jobs', 'estimator__vectorizer__tokenizer': None, 'estimator__classifier__tol': 0.0001, 'estimator__classifier__min_weight_fraction_leaf': 0.0, 'estimator__classifier__verbose': 1, 'estimator__classifier__init': <__main__.Working_Init_Classifier instance at 0x1a15c04cb0>, 'estimator__vectorizer__decode_error': 'strict', 'estimator__classifier': GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a15c04cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=395,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False), 'refit': True, 'estimator__vectorizer__max_features': None, 'estimator__classifier__presort': 'auto', 'estimator__classifier__criterion': 'friedman_mse', 'estimator__vectorizer__max_df': 0.75, 'estimator__vectorizer__strip_accents': None, 'estimator__vectorizer__encoding': u'utf-8', 'estimator__vectorizer__stop_words': 'english', 'estimator__classifier__min_samples_split': 2, 'iid': True, 'estimator__vectorizer__analyzer': u'word', 'estimator__classifier__n_estimators': 100, 'return_train_score': 'warn', 'estimator__classifier__subsample': 1.0, 'estimator__classifier__min_impurity_split': None, 'estimator__classifier__n_iter_no_change': None, 'estimator__vectorizer__binary': True, 'error_score': 'raise-deprecating', 'estimator__classifier__learning_rate': 0.1, 'estimator__classifier__random_state': 395, 'estimator__classifier__max_leaf_nodes': None, 'estimator__vectorizer__token_pattern': u'(?u)\\b\\w\\w+\\b', 'estimator__classifier__max_depth': 3}
### Feature weights: highest 20
+0.6959	gene
+0.0887	versus
+0.0717	environment
+0.0581	densiti
+0.0472	suffici
+0.0384	restor
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-10-56-34. Total      1.10 seconds

Fitting 1 folds for each of 2 candidates, totalling 2 fits
      Iter       Train Loss   Remaining Time 
         1           1.0492            0.01s
         2           0.9128            0.01s
         3           0.7994            0.00s
         4           0.7038            0.00s
         5           0.6221            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0492            0.01s
         2           0.9128            0.01s
         3           0.7994            0.00s
         4           0.7038            0.00s
         5           0.6221            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0834            0.01s
         2           0.9410            0.01s
         3           0.8233            0.00s
         4           0.7241            0.00s
         5           0.6395            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0492            0.01s
         2           0.9128            0.01s
         3           0.7994            0.00s
         4           0.7038            0.00s
         5           0.6221            0.00s
### Start Time 2019/11/06-11-01-34  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=330   randForSplit=264   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a22f2bc68>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=330,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]
cv:[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 27, 28])]
error_score:raise-deprecating
estimator:Pipeline(memory=None,
     steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='englis...    subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False))])
estimator__classifier:GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a22d1ccf8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=330,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)
estimator__classifier__criterion:friedman_mse
estimator__classifier__init:<__main__.Working_Init_Classifier instance at 0x1a22d1ccf8>
estimator__classifier__learning_rate:0.1
estimator__classifier__loss:deviance
estimator__classifier__max_depth:3
estimator__classifier__max_features:None
estimator__classifier__max_leaf_nodes:None
estimator__classifier__min_impurity_decrease:0.0
estimator__classifier__min_impurity_split:None
estimator__classifier__min_samples_leaf:1
estimator__classifier__min_samples_split:2
estimator__classifier__min_weight_fraction_leaf:0.0
estimator__classifier__n_estimators:100
estimator__classifier__n_iter_no_change:None
estimator__classifier__presort:auto
estimator__classifier__random_state:330
estimator__classifier__subsample:1.0
estimator__classifier__tol:0.0001
estimator__classifier__validation_fraction:0.1
estimator__classifier__verbose:1
estimator__classifier__warm_start:False
estimator__memory:None
estimator__steps:[('vectorizer', CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('classifier', GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a22d1ccf8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=330,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False))]
estimator__vectorizer:CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.02,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
estimator__vectorizer__analyzer:word
estimator__vectorizer__binary:True
estimator__vectorizer__decode_error:strict
estimator__vectorizer__dtype:<type 'numpy.int64'>
estimator__vectorizer__encoding:utf-8
estimator__vectorizer__input:content
estimator__vectorizer__lowercase:False
estimator__vectorizer__max_df:0.75
estimator__vectorizer__max_features:None
estimator__vectorizer__min_df:0.02
estimator__vectorizer__ngram_range:(1, 2)
estimator__vectorizer__preprocessor:None
estimator__vectorizer__stop_words:english
estimator__vectorizer__strip_accents:None
estimator__vectorizer__token_pattern:(?u)\b\w\w+\b
estimator__vectorizer__tokenizer:None
estimator__vectorizer__vocabulary:None
fit_params:None
iid:True
n_jobs:1
param_grid:{'vectorizer__min_df': [0.01, 0.02], 'vectorizer__ngram_range': [(1, 1)], 'classifier__n_estimators': [5], 'vectorizer__max_df': [0.75]}
pre_dispatch:2*n_jobs
refit:True
return_train_score:warn
scoring:make_scorer(fbeta_score, beta=2, pos_label=1)
verbose:True

### Feature weights: highest 20
+0.6959	gene
+0.0887	suffici
+0.0717	metabol
+0.0581	puls
+0.0472	environment
+0.0384	versus
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-01-35. Total      1.08 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0560            0.01s
         2           0.9191            0.01s
         3           0.8052            0.00s
         4           0.7090            0.00s
         5           0.6267            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0560            0.01s
         2           0.9191            0.01s
         3           0.8052            0.00s
         4           0.7090            0.00s
         5           0.6267            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0825            0.01s
         2           0.9402            0.01s
         3           0.8224            0.00s
         4           0.7233            0.00s
         5           0.6388            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0560            0.01s
         2           0.9191            0.01s
         3           0.8052            0.00s
         4           0.7090            0.00s
         5           0.6267            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0529            0.01s
         2           0.9162            0.01s
         3           0.8026            0.00s
         4           0.7066            0.00s
         5           0.6246            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0529            0.01s
         2           0.9162            0.01s
         3           0.8026            0.00s
         4           0.7066            0.00s
         5           0.6246            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0829            0.01s
         2           0.9406            0.01s
         3           0.8228            0.00s
         4           0.7237            0.00s
         5           0.6392            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0529            0.01s
         2           0.9162            0.01s
         3           0.8026            0.00s
         4           0.7066            0.00s
         5           0.6246            0.00s
### Start Time 2019/11/06-11-15-48  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=1   randForSplit=361   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a19af4830>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Best Score: 1

### Feature weights: highest 20
+0.6959	gene
+0.1770	exampl
+0.0887	suffici
+0.0384	versus
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6
+0.0000	aav8

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-15-49. Total      1.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0374            0.01s
         2           0.9015            0.01s
         3           0.7891            0.00s
         4           0.6944            0.00s
         5           0.6136            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0374            0.01s
         2           0.9015            0.01s
         3           0.7891            0.00s
         4           0.6944            0.00s
         5           0.6136            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0767            0.01s
         2           0.9344            0.01s
         3           0.8170            0.01s
         4           0.7183            0.00s
         5           0.6342            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0374            0.01s
         2           0.9015            0.01s
         3           0.7891            0.00s
         4           0.6944            0.00s
         5           0.6136            0.00s
### Start Time 2019/11/06-11-16-21  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=106   randForSplit=822   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a17292830>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=106,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.1271	exampl
+0.0717	suffici
+0.0581	environment
+0.0472	metabol
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-16-22. Total      1.13 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0565            0.01s
         2           0.9195            0.01s
         3           0.8056            0.00s
         4           0.7093            0.00s
         5           0.6270            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0565            0.01s
         2           0.9195            0.01s
         3           0.8056            0.00s
         4           0.7093            0.00s
         5           0.6270            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0862            0.01s
         2           0.9439            0.01s
         3           0.8259            0.00s
         4           0.7265            0.00s
         5           0.6417            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0565            0.01s
         2           0.9195            0.01s
         3           0.8056            0.00s
         4           0.7093            0.00s
         5           0.6270            0.00s
   mean_fit_time  mean_score_time  ...  std_test_score  std_train_score
0       0.131005         0.007323  ...             0.0              0.0
1       0.131010         0.006803  ...             0.0              0.0

[2 rows x 16 columns]
### Start Time 2019/11/06-11-24-08  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=636   randForSplit=656   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a25080a70>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=636,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0886	govern
+0.0717	metabol
+0.0581	puls
+0.0472	insert
+0.0384	bm
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-24-09. Total      1.14 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0576            0.01s
         2           0.9206            0.01s
         3           0.8066            0.00s
         4           0.7102            0.00s
         5           0.6278            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0576            0.01s
         2           0.9206            0.01s
         3           0.8066            0.00s
         4           0.7102            0.00s
         5           0.6278            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0860            0.01s
         2           0.9436            0.01s
         3           0.8257            0.00s
         4           0.7263            0.00s
         5           0.6415            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0576            0.01s
         2           0.9206            0.01s
         3           0.8066            0.00s
         4           0.7102            0.00s
         5           0.6278            0.00s
   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_classifier__n_estimators param_vectorizer__max_df param_vectorizer__min_df param_vectorizer__ngram_range                                             params  rank_test_score  split0_test_score  split0_train_score  std_fit_time  std_score_time  std_test_score  std_train_score
0       0.133262         0.006921              1.0               1.0                              5                     0.75                     0.01                        (1, 1)  {u'vectorizer__min_df': 0.01, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
1       0.145061         0.006883              1.0               1.0                              5                     0.75                     0.02                        (1, 1)  {u'vectorizer__min_df': 0.02, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
### Start Time 2019/11/06-11-28-00  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=982   randForSplit=66   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1dec0a70>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=982,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0886	bm
+0.0717	suffici
+0.0581	helper
+0.0472	dendrit
+0.0384	predomin
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-28-01. Total      1.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0375            0.01s
         2           0.9017            0.01s
         3           0.7892            0.00s
         4           0.6945            0.00s
         5           0.6137            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0375            0.01s
         2           0.9017            0.01s
         3           0.7892            0.00s
         4           0.6945            0.00s
         5           0.6137            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0730            0.01s
         2           0.9306            0.01s
         3           0.8133            0.00s
         4           0.7148            0.00s
         5           0.6311            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0375            0.01s
         2           0.9017            0.01s
         3           0.7892            0.00s
         4           0.6945            0.00s
         5           0.6137            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0541            0.01s
         2           0.9173            0.01s
         3           0.8036            0.00s
         4           0.7075            0.00s
         5           0.6254            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0541            0.01s
         2           0.9173            0.01s
         3           0.8036            0.00s
         4           0.7075            0.00s
         5           0.6254            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0835            0.01s
         2           0.9412            0.01s
         3           0.8234            0.00s
         4           0.7242            0.00s
         5           0.6397            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0541            0.01s
         2           0.9173            0.01s
         3           0.8036            0.00s
         4           0.7075            0.00s
         5           0.6254            0.00s
### Start Time 2019/11/06-11-30-49  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=794   randForSplit=695   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a16bcaa70>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=794,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:
   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_classifier__n_estimators param_vectorizer__max_df param_vectorizer__min_df param_vectorizer__ngram_range                                             params  rank_test_score  split0_test_score  split0_train_score  std_fit_time  std_score_time  std_test_score  std_train_score
0       0.138284         0.006893              1.0               1.0                              5                     0.75                     0.01                        (1, 1)  {u'vectorizer__min_df': 0.01, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
1       0.133928         0.006748              1.0               1.0                              5                     0.75                     0.02                        (1, 1)  {u'vectorizer__min_df': 0.02, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.1359	exampl
+0.0717	govern
+0.0581	ventral
+0.0384	histogram
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5
+0.0000	aav6

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-30-50. Total      1.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0515            0.01s
         2           0.9148            0.01s
         3           0.8013            0.00s
         4           0.7055            0.00s
         5           0.6236            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0515            0.01s
         2           0.9148            0.01s
         3           0.8013            0.00s
         4           0.7055            0.00s
         5           0.6236            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0815            0.01s
         2           0.9392            0.01s
         3           0.8215            0.00s
         4           0.7225            0.00s
         5           0.6380            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0515            0.01s
         2           0.9148            0.01s
         3           0.8013            0.00s
         4           0.7055            0.00s
         5           0.6236            0.00s
### Start Time 2019/11/06-11-35-50  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=414   randForSplit=579   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1b3f4a70>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=414,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Results:
{'std_train_score': array([0., 0.]), 'rank_test_score': array([1, 1], dtype=int32), 'std_test_score': array([0., 0.]), 'param_vectorizer__min_df': masked_array(data=[0.01, 0.02],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_vectorizer__max_df': masked_array(data=[0.75, 0.75],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_train_score': array([1., 1.]), 'split0_test_score': array([1., 1.]), 'mean_test_score': array([1., 1.]), 'std_score_time': array([0., 0.]), 'params': [{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}], 'std_fit_time': array([0., 0.]), 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1)],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_classifier__n_estimators': masked_array(data=[5, 5],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_score_time': array([0.00775886, 0.00715995]), 'mean_fit_time': array([0.13383102, 0.13206005]), 'split0_train_score': array([1., 1.])}
### Grid Search DataFrame:
   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_classifier__n_estimators param_vectorizer__max_df param_vectorizer__min_df param_vectorizer__ngram_range                                             params  rank_test_score  split0_test_score  split0_train_score  std_fit_time  std_score_time  std_test_score  std_train_score
0       0.133831         0.007759              1.0               1.0                              5                     0.75                     0.01                        (1, 1)  {u'vectorizer__min_df': 0.01, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0
1       0.132060         0.007160              1.0               1.0                              5                     0.75                     0.02                        (1, 1)  {u'vectorizer__min_df': 0.02, u'vectorizer__ng...                1                1.0                 1.0           0.0             0.0             0.0              0.0

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0887	predomin
+0.0717	cage
+0.0581	dendrit
+0.0472	bm
+0.0384	exampl
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-35-51. Total      1.10 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0244            0.01s
         2           0.8889            0.01s
         3           0.7774            0.00s
         4           0.6836            0.00s
         5           0.6038            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0244            0.01s
         2           0.8889            0.01s
         3           0.7774            0.00s
         4           0.6836            0.00s
         5           0.6038            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0700            0.01s
         2           0.9275            0.01s
         3           0.8103            0.00s
         4           0.7120            0.00s
         5           0.6285            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0244            0.01s
         2           0.8889            0.01s
         3           0.7774            0.00s
         4           0.6836            0.00s
         5           0.6038            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0484            0.01s
         2           0.9120            0.01s
         3           0.7987            0.00s
         4           0.7031            0.00s
         5           0.6215            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0484            0.01s
         2           0.9120            0.01s
         3           0.7987            0.00s
         4           0.7031            0.00s
         5           0.6215            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0842            0.01s
         2           0.9419            0.01s
         3           0.8240            0.00s
         4           0.7248            0.00s
         5           0.6402            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0484            0.01s
         2           0.9120            0.01s
         3           0.7987            0.00s
         4           0.7031            0.00s
         5           0.6215            0.00s
### Start Time 2019/11/06-11-48-47  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=553   randForSplit=369   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a23546518>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=553,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Results:
{'std_train_score': array([0., 0.]), 'rank_test_score': array([1, 1], dtype=int32), 'std_test_score': array([0., 0.]), 'param_vectorizer__min_df': masked_array(data=[0.01, 0.02],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_vectorizer__max_df': masked_array(data=[0.75, 0.75],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_train_score': array([1., 1.]), 'split0_test_score': array([1., 1.]), 'mean_test_score': array([1., 1.]), 'std_score_time': array([0., 0.]), 'params': [{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}], 'std_fit_time': array([0., 0.]), 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1)],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_classifier__n_estimators': masked_array(data=[5, 5],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_score_time': array([0.00750613, 0.00778913]), 'mean_fit_time': array([0.14886999, 0.14549804]), 'split0_train_score': array([1., 1.])}
### Grid Search DataFrame:
[{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}]
1.000000
[{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}]
1.000000


### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0887	versus
+0.0717	predomin
+0.0581	environment
+0.0472	restor
+0.0384	cage
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-48-48. Total      1.18 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0474            0.01s
         2           0.9110            0.01s
         3           0.7978            0.00s
         4           0.7023            0.00s
         5           0.6208            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0474            0.01s
         2           0.9110            0.01s
         3           0.7978            0.00s
         4           0.7023            0.00s
         5           0.6208            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0835            0.01s
         2           0.9411            0.01s
         3           0.8233            0.00s
         4           0.7242            0.00s
         5           0.6396            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0474            0.01s
         2           0.9110            0.01s
         3           0.7978            0.00s
         4           0.7023            0.00s
         5           0.6208            0.00s
### Start Time 2019/11/06-11-49-25  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=895   randForSplit=354   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__n_estimators: 5
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a2365c2d8>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=5,
              n_iter_no_change=None, presort='auto', random_state=895,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__n_estimators:[5]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Results:
{'std_train_score': array([0., 0.]), 'rank_test_score': array([1, 1], dtype=int32), 'std_test_score': array([0., 0.]), 'param_vectorizer__min_df': masked_array(data=[0.01, 0.02],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_vectorizer__max_df': masked_array(data=[0.75, 0.75],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_train_score': array([1., 1.]), 'split0_test_score': array([1., 1.]), 'mean_test_score': array([1., 1.]), 'std_score_time': array([0., 0.]), 'params': [{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}, {'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}], 'std_fit_time': array([0., 0.]), 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1)],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'param_classifier__n_estimators': masked_array(data=[5, 5],
             mask=[False, False],
       fill_value='?',
            dtype=object), 'mean_score_time': array([0.00686097, 0.00700283]), 'mean_fit_time': array([0.1306951 , 0.13166404]), 'split0_train_score': array([1., 1.])}
### Grid Search DataFrame:
{'vectorizer__min_df': 0.01, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 5, 'vectorizer__max_df': 0.75}
1.000000


### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0887	faster
+0.0717	ventral
+0.0581	restor
+0.0472	dendrit
+0.0384	predomin
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11
+0.0000	a9231
+0.0000	aa
+0.0000	aaap
+0.0000	aad
+0.0000	aap
+0.0000	aav
+0.0000	aav5

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-49-26. Total      1.09 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.04s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.03s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.05s
         3           0.8041            0.05s
         4           0.7079            0.05s
         5           0.6258            0.04s
         6           0.5550            0.04s
         7           0.4934            0.04s
         8           0.4397            0.04s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.04s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.03s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.04s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.04s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.04s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.03s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.04s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.05s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.05s
         2           0.9178            0.04s
         3           0.8041            0.04s
         4           0.7079            0.04s
         5           0.6258            0.04s
         6           0.5550            0.03s
         7           0.4934            0.03s
         8           0.4397            0.03s
         9           0.3926            0.03s
        10           0.3510            0.03s
        20           0.1209            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0876            0.04s
         2           0.9452            0.04s
         3           0.8272            0.03s
         4           0.7277            0.03s
         5           0.6428            0.03s
         6           0.5697            0.03s
         7           0.5063            0.02s
         8           0.4510            0.02s
         9           0.4024            0.02s
        10           0.3597            0.02s
        20           0.1237            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0547            0.04s
         2           0.9178            0.04s
         3           0.8041            0.03s
         4           0.7079            0.03s
         5           0.6258            0.03s
         6           0.5550            0.03s
         7           0.4934            0.02s
         8           0.4397            0.02s
         9           0.3926            0.02s
        10           0.3510            0.02s
        20           0.1209            0.00s
### Start Time 2019/11/06-11-51-13  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=49   randForSplit=197   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a254e3cb0>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=49,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 5, 8]
classifier__n_estimators:[20, 25]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Results:
{'std_train_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), 'mean_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'param_vectorizer__min_df': masked_array(data=[0.01, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01,
                   0.02, 0.01, 0.02],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'param_vectorizer__max_df': masked_array(data=[0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
                   0.75, 0.75, 0.75],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'mean_train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'split0_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'param_classifier__max_depth': masked_array(data=[3, 3, 3, 3, 5, 5, 5, 5, 8, 8, 8, 8],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'params': [{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}, {'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}, {'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}], 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1),
                   (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'param_classifier__n_estimators': masked_array(data=[20, 20, 25, 25, 20, 20, 25, 25, 20, 20, 25, 25],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False],
       fill_value='?',
            dtype=object), 'mean_score_time': array([0.00832605, 0.00974298, 0.0074501 , 0.00775909, 0.00936604,
       0.00731206, 0.00784302, 0.00718117, 0.00812101, 0.00848389,
       0.00746894, 0.00808001]), 'mean_fit_time': array([0.16530704, 0.16287589, 0.17560601, 0.1658411 , 0.19232106,
       0.164711  , 0.17110991, 0.17250395, 0.15873694, 0.16837406,
       0.16787601, 0.16779304]), 'split0_train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}
### Grid Search DataFrame:
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
1.000000


### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0578	histogram
+0.0508	restor
+0.0502	ventral
+0.0320	insert
+0.0251	cre
+0.0231	faster
+0.0166	dendrit
+0.0136	modul
+0.0112	bm
+0.0111	puls
+0.0093	exampl
+0.0018	versus
+0.0015	deliveri
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-51-17. Total      4.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.04s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.04s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.03s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.05s
         2           0.8863            0.04s
         3           0.7748            0.04s
         4           0.6813            0.04s
         5           0.6017            0.04s
         6           0.5333            0.04s
         7           0.4739            0.03s
         8           0.4222            0.03s
         9           0.3768            0.03s
        10           0.3369            0.03s
        20           0.1160            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0625            0.04s
         2           0.9193            0.04s
         3           0.8023            0.03s
         4           0.7045            0.03s
         5           0.6215            0.03s
         6           0.5502            0.03s
         7           0.4886            0.02s
         8           0.4350            0.02s
         9           0.3881            0.02s
        10           0.3468            0.02s
        20           0.1191            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0217            0.04s
         2           0.8863            0.03s
         3           0.7748            0.03s
         4           0.6813            0.03s
         5           0.6017            0.03s
         6           0.5333            0.03s
         7           0.4739            0.02s
         8           0.4222            0.02s
         9           0.3768            0.02s
        10           0.3369            0.02s
        20           0.1160            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.04s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.04s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.02s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.04s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.05s
         2           0.9119            0.04s
         3           0.7986            0.04s
         4           0.7031            0.04s
         5           0.6214            0.04s
         6           0.5510            0.03s
         7           0.4899            0.03s
         8           0.4365            0.03s
         9           0.3897            0.03s
        10           0.3485            0.03s
        20           0.1200            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0837            0.04s
         2           0.9414            0.04s
         3           0.8236            0.03s
         4           0.7244            0.03s
         5           0.6398            0.03s
         6           0.5670            0.03s
         7           0.5039            0.02s
         8           0.4488            0.02s
         9           0.4005            0.02s
        10           0.3580            0.02s
        20           0.1231            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0483            0.04s
         2           0.9119            0.03s
         3           0.7986            0.03s
         4           0.7031            0.03s
         5           0.6214            0.03s
         6           0.5510            0.03s
         7           0.4899            0.02s
         8           0.4365            0.02s
         9           0.3897            0.02s
        10           0.3485            0.02s
        20           0.1200            0.00s
### Start Time 2019/11/06-11-56-26  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=423   randForSplit=829   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a253ddd40>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=423,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 5, 8]
classifier__n_estimators:[20, 25]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.0601	densiti
+0.0468	insert
+0.0439	versus
+0.0432	ventral
+0.0419	faster
+0.0251	histogram
+0.0163	dendrit
+0.0136	metabol
+0.0074	modul
+0.0027	cage
+0.0018	remark
+0.0015	term
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express
+0.0000	a427
+0.0000	a68
+0.0000	a7c11

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-56-30. Total      4.12 seconds

      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.04s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.04s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.04s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.04s
         7           0.4826            0.04s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.04s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.05s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.05s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.04s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.05s
         2           0.8999            0.04s
         3           0.7876            0.04s
         4           0.6930            0.04s
         5           0.6124            0.04s
         6           0.5429            0.03s
         7           0.4826            0.03s
         8           0.4300            0.03s
         9           0.3838            0.03s
        10           0.3432            0.03s
        20           0.1182            0.01s
      Iter       Train Loss   Remaining Time 
         1           1.0704            0.04s
         2           0.9278            0.04s
         3           0.8107            0.03s
         4           0.7124            0.03s
         5           0.6288            0.03s
         6           0.5570            0.03s
         7           0.4948            0.02s
         8           0.4406            0.02s
         9           0.3931            0.02s
        10           0.3513            0.02s
        20           0.1207            0.00s
      Iter       Train Loss   Remaining Time 
         1           1.0357            0.04s
         2           0.8999            0.03s
         3           0.7876            0.03s
         4           0.6930            0.03s
         5           0.6124            0.03s
         6           0.5429            0.03s
         7           0.4826            0.02s
         8           0.4300            0.02s
         9           0.3838            0.02s
        10           0.3432            0.02s
        20           0.1182            0.00s
### Start Time 2019/11/06-11-59-03  GB.py	index file: index.out
Training data path:   /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/trainSet.txt	GridSearch Beta: 2
Validation data path: /Users/jak/work/autolittriage/Train/smallTest/data/small/LegendsWords/Proc1/valSet.txt
Test data path:       None
Random Seeds:	randForClassifier=409   randForSplit=786   
### Metrics: Training Set
               precision    recall  f1-score   support

   Train keep       1.00      1.00      1.00        19
Train discard       1.00      1.00      1.00         7

    micro avg       1.00      1.00      1.00        26
    macro avg       1.00      1.00      1.00        26
 weighted avg       1.00      1.00      1.00        26

Train (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[19  0]
 [ 0  7]]

### Metrics: Validation Set
               precision    recall  f1-score   support

   Valid keep       1.00      1.00      1.00         1
Valid discard       1.00      1.00      1.00         2

    micro avg       1.00      1.00      1.00         3
    macro avg       1.00      1.00      1.00         3
 weighted avg       1.00      1.00      1.00         3

Valid (keep) F2: 1.0000    P: 1.0000    R: 1.0000    NPV: 1.0000

['yes', 'no']
[[1 0]
 [0 2]]

### Best Pipeline Parameters:
classifier__max_depth: 3
classifier__n_estimators: 20
vectorizer__max_df: 0.75
vectorizer__min_df: 0.01
vectorizer__ngram_range: (1, 1)

classifier:
GradientBoostingClassifier(criterion='friedman_mse',
              init=<__main__.Working_Init_Classifier instance at 0x1a1f6f1f38>,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=20,
              n_iter_no_change=None, presort='auto', random_state=409,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=False, max_df=0.75, max_features=None, min_df=0.01,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)


### Grid Search Parameter Options Tried:
classifier__max_depth:[3, 5, 8]
classifier__n_estimators:[20, 25]
vectorizer__max_df:[0.75]
vectorizer__min_df:[0.01, 0.02]
vectorizer__ngram_range:[(1, 1)]

### Grid Search Scores:
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 3}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 5}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 20, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.01, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
mean_test_score:  1.000000
{'vectorizer__min_df': 0.02, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1), 'classifier__n_estimators': 25, 'classifier__max_depth': 8}
mean_test_score:  1.000000

### Grid Search Best Score: 1.000000

### Feature weights: highest 20
+0.6959	gene
+0.1125	densiti
+0.0579	suffici
+0.0308	histogram
+0.0294	exampl
+0.0166	helper
+0.0135	environment
+0.0111	ventral
+0.0074	versus
+0.0060	cre
+0.0049	remark
+0.0048	term
+0.0040	bm
+0.0022	predomin
+0.0018	govern
+0.0012	dendrit
+0.0000	a10
+0.0000	a11037
+0.0000	a1978
+0.0000	a1express

### Feature weights: lowest 20
+0.0000	yield
+0.0000	yj11
+0.0000	yj11xsd7
+0.0000	yj69
+0.0000	yj69xsd7
+0.0000	young
+0.0000	youngest
+0.0000	yvad
+0.0000	zac1
+0.0000	zaytseva
+0.0000	zero
+0.0000	zhang
+0.0000	zip
+0.0000	zn
+0.0000	znf822
+0.0000	znpp
+0.0000	zone
+0.0000	zpf57
+0.0000	zygos
+0.0000	zymo

### Vectorizer:   Number of Features: 4870
First 10 features: [u'a10', u'a11037', u'a1978', u'a1express', u'a427', u'a68', u'a7c11', u'a9231', u'aa', u'aaap']

Middle 10 features: [u'kodak', u'korostowski', u'kras', u'krasg12d', u'kruskal', u'kvdmr', u'kvdmr1', u'kvdmr1mice', u'kvdmr1was', u'kwon']

Last 10 features: [u'zero', u'zhang', u'zip', u'zn', u'znf822', u'znpp', u'zone', u'zpf57', u'zygos', u'zymo']

### False positives for Validation set: 0

### False negatives for Validation set: 0

### Sample set sizes
                    :      Samples     Positive     Negative   % Positive
Training Set        :           26           19            7          73%
Validation Set      :            3            1            2          33%
ValidationSplit: 0.20
### End Time 2019/11/06-11-59-07. Total      4.09 seconds

